[
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#introdução",
    "href": "presentations/xgboost_catboost/trabalho.html#introdução",
    "title": "XGBoost/Catboost",
    "section": "Introdução",
    "text": "Introdução\nXGBoost (eXtreme Gradient Boosting) (Tianqi Chen, 2016) é uma das implementações de three ensembling baseada em gradient boosting mais populares, por ser otimizado para desempenho e eficiência.\nA ideia do algoritmo gradient boosting é realizar ensemble de M classificadores fracos para obter um modelo robusto que faça boas previsões. Estes classificadores serão árvores de decisão. Porém, diferentemente do AdaBoost estudado anteriormente, a regularização é feita a partir da minimização de uma função de perda diferenciável, e não pela atribuição de maior peso a instâncias classificadas erroneamente."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#introdução-1",
    "href": "presentations/xgboost_catboost/trabalho.html#introdução-1",
    "title": "XGBoost/Catboost",
    "section": "Introdução",
    "text": "Introdução\nPor se tratar de um modelo sequencial, em que a árvore \\(i+2\\) necessitará da informação dos resíduos da árvore \\(i+1\\), também herda a restrição de paralelismo que outros modelos de three ensembling também contém. Porém, a implementação do algoritmo XGBoost busca paralelizar todas as rotinas necessárias para construção do modelo, tornando sequencial apenas o início da construção de cada árvore.\n\n\n\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#gradient-boosting",
    "href": "presentations/xgboost_catboost/trabalho.html#gradient-boosting",
    "title": "XGBoost/Catboost",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting\nO algoritmo base do XGBoost é o Gradient Boosting, no qual busca-se minimizar uma função de perda utilizando gradiente descendente. Esta otimização não será numérica, mas sim fornecida pela combinação das árvores geradas (boosting), que irão conduzir o gradiente para o mínimo global da função.\nCada um desses aprendizes fracos sequenciais irão focar nos resíduos da árvore anterior, buscando a divisão que minimize a função de perda definida (podendo ser default ou escolhida pelo usuário)."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#gradient-boosting-1",
    "href": "presentations/xgboost_catboost/trabalho.html#gradient-boosting-1",
    "title": "XGBoost/Catboost",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting\nSeja M o número de árvores fixadas para construção do modelo, o modelo final será dado pela fórmula \\[F_M(x) = F_0(x) +\\sum^M_{m=1}F_m(x)\\] Em que \\(F_M(x)\\) será o modelo final para predições, \\(F_0(x)\\) será um modelo inicial de árvore, \\(F_1(x)\\) será o modelo que minimiza os resíduos do modelo \\(F_0(x)\\); \\(F_2(x)\\) será o modelo que minimiza os resíduos do modelo \\(F_1(x)\\), e assim por diante até o modelo \\(F_{m}(x)\\). Portanto, a classificação da i-ésima instância será dada por:\\[\\hat{y_i} = F_M(x_i)\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#construção-das-árvores",
    "href": "presentations/xgboost_catboost/trabalho.html#construção-das-árvores",
    "title": "XGBoost/Catboost",
    "section": "Construção das árvores",
    "text": "Construção das árvores\nSalvo a primeira árvore, que será um chute inicial, iremos definir uma função objetivo com sendo\\[obj = \\sum^n_{i=1}l(y_i,\\hat{y}_i^{(t)})+\\sum^t_{i=1}\\Omega(f_i),\\] em que o termo \\(l(y_i,\\hat{y}_i^{(t)})\\) irá computar a perda relativa no t-ésimo passo, e \\(\\Omega(f_i)\\) será o t-ésimo termo de regularização, e \\(obj\\) será baseado nos resíduos da árvore anterior.\nO algoritmo escolhe a cada passo a árvore que minimiza \\(obj\\) com base nas features selecionadas automaticamente pelo modelo."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#under-the-hood",
    "href": "presentations/xgboost_catboost/trabalho.html#under-the-hood",
    "title": "XGBoost/Catboost",
    "section": "Under the hood",
    "text": "Under the hood\nPara a seleção de features que irão minimizar a função objetivo, o algoritmo irá avaliar diversas possíveis divisões em diferentes features, construindo entre outras coisas histogramas para as features que ajudem a encontrar o melhor ponto de divisão. Para isso, o algoritmo utiliza do paralelismo, executando diversas divisões e encontrando a melhor possível de forma iterativa e rápida.\nDesta forma, apesar de se tratar de um modelo sequencial, o algoritmo implementado utiliza em todas as etapas que for possível da computação paralelizada, afim de aproveitar ao máximo o desempenho computacional e retornar um modelo que seja robusto e rápido\nPara maior aprofundamento teórico sobre o funcionamento do modelo, consulte XGBoost tutorial, seção 1.3"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo",
    "href": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo",
    "title": "XGBoost/Catboost",
    "section": "Flow do modelo",
    "text": "Flow do modelo\nUma forma simplificada de mostrar o funcionamento do XGBoost pode ser representada pelo seguinte grafo:\n\n\n\n\n\n\ngraph TD\n\n    A[Entrada de dados] --&gt; B\n    B[Preparação dos Dados] --&gt; B1 & B2 & B3\n    B1[Limpeza dos Dados]\n    B2[Normalização]\n    B3[Divisão Treino/Teste]\n    B1 & B2 & B3 --&gt; B_Final[Dados Preparados]\n\n    B_Final --&gt; C[Entrada de Dados no Modelo]\n\n    C --&gt; D[Construção da Primeira Árvore]\n    D --&gt; D1 & D2 & D3\n    D1[Avaliação das Divisões]\n    D2[Construção dos Nós]\n    D3[Combinação das Previsões]\n    D1 & D2 & D3 --&gt; D_Final[Árvore Construída]\n    \n    D_Final --&gt; E[Entrada de Dados Residual para a Segunda Árvore]\n\n    E --&gt; F[Construção da Segunda Árvore]\n    F --&gt; F1 & F2 & F3\n    F1[Avaliação das Divisões]\n    F2[Construção dos Nós]\n    F3[Combinação das Previsões]\n    F1 & F2 & F3 --&gt; F_Final[Segunda Árvore Construída]\n\n    F_Final --&gt; G[Repetir o processo anterior m vezes]\n\n    G --&gt; H[Avaliação do Modelo]\n    \n    H --&gt; I[Ensembling das Árvores com Pesos Relativos]\n\n    I --&gt; J[Modelo final]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#implementação-xgboost",
    "href": "presentations/xgboost_catboost/trabalho.html#implementação-xgboost",
    "title": "XGBoost/Catboost",
    "section": "Implementação XGBoost",
    "text": "Implementação XGBoost\nVamos à prática!"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#exemplo-prático-xgboost-catboost-outros",
    "href": "presentations/xgboost_catboost/trabalho.html#exemplo-prático-xgboost-catboost-outros",
    "title": "XGBoost/Catboost",
    "section": "Exemplo prático: XGBoost, CatBoost & outros",
    "text": "Exemplo prático: XGBoost, CatBoost & outros\nExemplo prático"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#categorical-boost---catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#categorical-boost---catboost",
    "title": "XGBoost/Catboost",
    "section": "Categorical Boost - CatBoost",
    "text": "Categorical Boost - CatBoost\n\nAlgoritmo de aprendizado de máquina\nModelo Ensemble\nÁrvores de decisão\nNão é necessário pré-processamento dos dados categóricos\nCada árvore reduz a perda comparada com a anterior\nRandom Permutations"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#target-encoding",
    "href": "presentations/xgboost_catboost/trabalho.html#target-encoding",
    "title": "XGBoost/Catboost",
    "section": "Target Encoding",
    "text": "Target Encoding\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {OptionCount+Prior}{n+1}\\]\nPrior: Palpite inicial arbitrário \nOption Count: número de vezes que alguém da mesma cor gostou de estatística anteriormente \nn: número de repetições da mesma cor já vistas"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {OptionCount+Prior}{n+1}\\]\nTarget Encoding \\[\\frac {0+0.05}{0+1} = 0.05\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-1",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-1",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {0+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {0.05}{2} = 0.025\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-2",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-2",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {1.05}{2} = 0.525\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-3",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-3",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {1.05}{2} = 0.525\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-4",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-4",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{2+1}\\]\nTarget Encoding \\[\\frac {1.05}{3} = 0.35\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#árvores-de-decisão-simétricas",
    "href": "presentations/xgboost_catboost/trabalho.html#árvores-de-decisão-simétricas",
    "title": "XGBoost/Catboost",
    "section": "Árvores de decisão Simétricas",
    "text": "Árvores de decisão Simétricas\n\nYoutube: CatBoost Parte 2: Construindo e Usando Árvores\nGera modelos mais fracos, porém mais simples\nGera previsões mais rapidamente"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo-catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo-catboost",
    "title": "XGBoost/Catboost",
    "section": "Flow do modelo CatBoost",
    "text": "Flow do modelo CatBoost\nUma forma simplificada de mostrar o funcionamento do Catboost pode ser representada pelo seguinte grafo:\n\n\n\n\n\n\ngraph TD\n\n    A[Entrada de dados] --&gt; B\n    B[Preparação dos Dados] --&gt; B1 & B3\n    B1[Limpeza dos Dados]\n    B3[Divisão Treino/Teste]\n    B1 & B3 --&gt; B_Final[Dados Preparados]\n\n    B_Final --&gt; C[Entrada de Dados no Modelo]\n    \n    C[Entrada de Dados no Modelo] --&gt; K1[Random Permutations]\n    \n    K1[Random Permutations] --&gt; K2[Target Encoding]\n    \n    K2[Target Encoding] --&gt; D[Construção da Primeira Árvore]\n    D --&gt; D1 & D2 & D3\n    D1[Avaliação das Divisões]\n    D2[Construção dos Nós]\n    D3[Combinação das Previsões]\n    D1 & D2 & D3 --&gt; D_Final[Árvore Construída]\n    \n    D_Final --&gt; E[Entrada de Dados Residual para a Segunda Árvore]\n\n    E --&gt; F[Construção da Segunda Árvore]\n    F --&gt; F1 & F2 & F3\n    F1[Avaliação das Divisões]\n    F2[Construção dos Nós]\n    F3[Combinação das Previsões]\n    F1 & F2 & F3 --&gt; F_Final[Segunda Árvore Construída]\n\n    F_Final --&gt; G[Repetir o processo anterior m vezes]\n\n    G --&gt; H[Avaliação do Modelo]\n    \n    H --&gt; I[Ensembling das Árvores com Pesos Relativos]\n\n    I --&gt; J[Modelo final]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#implementação-catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#implementação-catboost",
    "title": "XGBoost/Catboost",
    "section": "Implementação CatBoost",
    "text": "Implementação CatBoost\nVamos à prática!"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#referências",
    "href": "presentations/xgboost_catboost/trabalho.html#referências",
    "title": "XGBoost/Catboost",
    "section": "Referências",
    "text": "Referências\nDocumentação XGBoost.\nPinheiro, João Manoel Herrera. Um estudo sobre Algoritmos de Boosting e a Otimização de Hiperparâmetros Utilizando Optuna. São Carlos, SP. 2023.\nDocumetação CatBoost\nChepenko, Introduction to gradient boosting on decision trees with Catboost\nCatBoost Parte 1: Codificação de destino ordenada\nCatBoost Parte 2: Construindo e Usando Árvores\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#introdução",
    "href": "presentations/random_forest/apresentacao.html#introdução",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Introdução",
    "text": "Introdução\n\nO que é floresta aleatória?\n\nFloresta aleatória é um algoritmo de Machine learning, utilizado para realizar predições, que combina o resultado de múltiplas árvores de decisão criadas aleatoriamente com o objetivo de diminuir a variância e o viés no contexto dos métodos de árvores."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#introdução-1",
    "href": "presentations/random_forest/apresentacao.html#introdução-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Introdução",
    "text": "Introdução\n\nÁrvore de decisão\n\nUma árvore de decisão é a forma mais básica entre os métodos baseados em árvore, muito utilizada em regressão e classificação, uma árvore de decisão consiste em segmentar o espaço composto pelas variáveis preditoras em regiões mais simples, nos quais a média (se a variável resposta for quantitativa) é utilizada como valor predito ou (caso categorizado) a classe da variável resposta com maior frequência, os modelos de árvores de decisão são atraentes pela simplicidade e fácil interpretação, contudo não possuem a precisão que outros métodos de classificação ou regressão alcançam."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens",
    "href": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Vantagens e desvantagens",
    "text": "Vantagens e desvantagens\n\nsobre árvore de decisão:\n\n\n\nVantagens\n\n\nSão facilmente explicaveis;\nPodem ser representadas graficamente com fácil interpretação;\nPodem manupular preditores qualitativos sem a necessidade de variáveis dummy.\n\n\n\nDesvantagens\n\n\nNão possuem o mesmo nível de precisão preditiva como outros modelos de regressão e classificação;\nNão são robustas, ou seja, uma pequena mudança nos dados pode gerar uma grande mudança na árvore de estimação final."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#motivação",
    "href": "presentations/random_forest/apresentacao.html#motivação",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Motivação",
    "text": "Motivação\nUma forma de superar a baixa precisão das árvores de decisão é a utilização de métodos agregadoes que ajustam modelos poderosos, como floresta aleatória, que consegue uma grande melhoria no poder de predição mesmo comparando com outros modelos de classificação."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#procedimento",
    "href": "presentations/random_forest/apresentacao.html#procedimento",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Procedimento",
    "text": "Procedimento\n\n\nComo na técnica bagging, construímos várias árvores a partir das amostras bootstrap do conjunto de testes.\nNa construção de cada árvore, cada vez que uma divisão (ou corte) é considerada para alguma árvore, uma seleção aleatória dos preditores é escolhida como candidatos dos cortes ao invés de todos os preditores como normalmente é feito.\nEssa abordagem tem como propósito reduzir a correlação entre as árvores, reduzindo a variância quando tiramos a média das árvores, já que a média tende a ser menor quando temos menor correlação entre as árvores.\nA escolha do número de preditores que serão selecionados para cada corte, é tipicamente escolhida pela raiz quadrada do número total de preditores.\nFlorestas aleatórias fornecem uma melhoria em relação ao método bagging, já que a correlação entre as árvores diminui."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#considerações",
    "href": "presentations/random_forest/apresentacao.html#considerações",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Considerações",
    "text": "Considerações\n\nDe forma geral, pode-se dizer que o procedimento introduz mais aleatoriedade e diversidade no processo de construção em relação ao método bagging.\nIntuitivamente, a utilização de florestas aleatórias para tomada de decisão corresponde à síntese da opinião de indivíduos com diferentes fontes de informação.\nEm geral, florestas aleatórias produzem resultados menos variáveis em relação ao método bagging, já que nesse método as árvores geradas podem ser muito semelhantes, dependendo de preditores fortes, o que não contribui para redução de variabilidade das predições, o que não acontece com florestas aleatórias."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#rotina",
    "href": "presentations/random_forest/apresentacao.html#rotina",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Rotina",
    "text": "Rotina\nUma rotina minimalista de classificação via Random Forest em R pode ser executada da seguinte forma:\n\n\n\nlibrary(randomForest)\n\ndata &lt;- iris\n\ndata$Species &lt;- as.factor(data$Species)\n\nset.seed(150167636)\nind &lt;- sample(2, nrow(data), replace = TRUE,\n              prob = c(0.7, 0.3))\ntrain &lt;- data[ind==1,]\ntest &lt;- data[ind==2,]\n\nrf &lt;- randomForest(Species~., data=train, proximity=TRUE)\n\n\n\n\n\nCall:\n randomForest(formula = Species ~ ., data = train, proximity = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6.36%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         33          0         0  0.00000000\nversicolor      0         36         3  0.07692308\nvirginica       0          4        34  0.10526316"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#resultados",
    "href": "presentations/random_forest/apresentacao.html#resultados",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Resultados",
    "text": "Resultados\nPodemos acessar os resultados do modelo no objeto rf\n\n\nTeste do modelo no conjunto de treino:\n\nlibrary(caret)\n\np1 &lt;- predict(rf, train)\nconfusionMatrix(p1, train$ Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         33          0         0\n  versicolor      0         39         0\n  virginica       0          0        38\n\nOverall Statistics\n                                    \n               Accuracy : 1         \n                 95% CI : (0.967, 1)\n    No Information Rate : 0.3545    \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                    1.0            1.0000           1.0000\nSpecificity                    1.0            1.0000           1.0000\nPos Pred Value                 1.0            1.0000           1.0000\nNeg Pred Value                 1.0            1.0000           1.0000\nPrevalence                     0.3            0.3545           0.3455\nDetection Rate                 0.3            0.3545           0.3455\nDetection Prevalence           0.3            0.3545           0.3455\nBalanced Accuracy              1.0            1.0000           1.0000\n\n\n\nValidação do modelo nos dados de teste:\n\np2 &lt;- predict(rf, test)\nconfusionMatrix(p2, test$ Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         2\n  virginica       0          1        10\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8854          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.8333\nSpecificity                  1.000            0.9310           0.9643\nPos Pred Value               1.000            0.8333           0.9091\nNeg Pred Value               1.000            0.9643           0.9310\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2500\nDetection Prevalence         0.425            0.3000           0.2750\nBalanced Accuracy            1.000            0.9201           0.8988"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#parâmetros",
    "href": "presentations/random_forest/apresentacao.html#parâmetros",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Parâmetros",
    "text": "Parâmetros\n\nA função randomForest do pacote homônimo tem uma série de parâmetros opcionais além do mínimo obrigatório, que seria o modelo e os dados. O mais importante destes parâmetros é o ntree, que por default é 500 e em geral deve-se utilizar o máximo possível tal que execute em um tempo aceitável. Em geral, o restante dos parâmetros deve ser deixado em default.\nNeste caso, o modelo foi extremamente eficiente mesmo na versão minimalista"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#fine-tuning",
    "href": "presentations/random_forest/apresentacao.html#fine-tuning",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nSe for o caso, também podemos fazer o fine-tuning dos parâmetros do modelo\n\n\n\n\nt &lt;- tuneRF(train[,-5], train[,5],\n       stepFactor = 0.5,\n       plot = TRUE,\n       ntreeTry = 150,\n       trace = TRUE,\n       improve = 0.05)\n\nmtry = 2  OOB error = 5.45% \nSearching left ...\nmtry = 4    OOB error = 5.45% \n0 0.05 \nSearching right ...\nmtry = 1    OOB error = 5.45% \n0 0.05 \n\n\n\n\n\n\n\n\n\n\n\nhist(treesize(rf),\n     main = \"No. of Nodes for the Trees\",\n     col = \"green\")"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#importâncias",
    "href": "presentations/random_forest/apresentacao.html#importâncias",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Importâncias",
    "text": "Importâncias\n\nPodemos verificar a importância de cada variável para o modelo.\n\n\n\n\nvarImpPlot(rf,\n           sort = T,\n           n.var = 10,\n           main = \"Top 10 - Variable Importance\")\n\n\n\n\n\n\n\nimportance(rf)\n\n             MeanDecreaseGini\nSepal.Length         7.299621\nSepal.Width          2.006132\nPetal.Length        35.799244\nPetal.Width         27.318058"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#implementações",
    "href": "presentations/random_forest/apresentacao.html#implementações",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Implementações",
    "text": "Implementações\n\nUma rotina de classificação via Random forest poderia ser executada de forma análoga em python da seguinte forma:\n\n\n\n\nlibrary(reticulate)\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\n\niris = datasets.load_iris()\ndados = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n\nX = dados\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\ny_pred = model.predict(X_test)\nconfusion_matrix(y_test, y_pred)\n\narray([[20,  0,  0],\n       [ 0, 14,  2],\n       [ 0,  1,  8]], dtype=int64)"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições",
    "href": "presentations/random_forest/apresentacao.html#definições",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\nAs árvores podem ser representadas como \\(h_1(\\boldsymbol x),\\;  h_2(\\boldsymbol x),\\; . . . ,\\; h_K( \\boldsymbol x)\\) na forma\n\\[ \\{ h \\left( \\boldsymbol x, \\Theta_k \\right), \\; \\; k = 1, \\dots \\}  \\]\nonde \\(\\Theta_k \\; \\text{i.i.d}\\) são vetores aleatórios representando a escolha de \\(p\\) entre os \\(m\\) atributos de \\(\\boldsymbol X\\).\nNormalmente, \\(p \\approx \\sqrt m\\)"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições-1",
    "href": "presentations/random_forest/apresentacao.html#definições-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\n1) A medida em que o número de árvores cresce, a média de acertos se estabiliza e a chance de cometer uma predição errada pode ser quantificada:\n\\[  P_{\\boldsymbol X, Y} \\left(   P_{\\Theta}( h(\\boldsymbol X, \\Theta) = Y)   - P_{\\Theta} (h(\\boldsymbol X, \\Theta)  \\neq Y) &lt; 0\\right) \\]\n\n\\(P_{\\Theta}( h(\\boldsymbol X, \\Theta) = Y)\\) representa a probabilidade de que uma árvore acerte a predição de Y"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições-2",
    "href": "presentations/random_forest/apresentacao.html#definições-2",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\n2) A acurácia da random forest vai depender do “poder” de cada um dos classificadores individuais e da dependência entre eles.\nUm limite superior para o erro de generalização é dado por\n\\[PE^* \\le -\\bar \\rho(1 − s^2)/s^2 \\]\nonde:\n\n\\(\\boldsymbol s = E_{\\boldsymbol X, Y} mr({\\boldsymbol X, Y} )\\) é o “poder” das árvores \\(h(\\boldsymbol x, \\Theta)\\)\n\\(\\bar \\rho\\) pode ser entendido como a média entre as correlações das árvores."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#exemplo-alzheimer",
    "href": "presentations/random_forest/apresentacao.html#exemplo-alzheimer",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Exemplo Alzheimer",
    "text": "Exemplo Alzheimer\nO dataset DARWIN (https://archive.ics.uci.edu/dataset/732/darwin) contém dados sobre a escrita a mão de pessoas afetadas pelo Alzheimer e de um grupo de controle, totalizando 174 observações. São 450 variáveis e o objetivo é distinguir pessoas afetadas (P) de pessoas saudáveis (H).\n\n\n\nset.seed(123)\n\nrf_model &lt;- randomForest(\n  class ~ ., data = train_data, \n  ntree = 500, \n  importance = TRUE\n  )\n\n\nNo. of variables tried at each split: 21\nOOB estimate of  error rate: 13.57%\n\nConfusion matrix:\n   H  P class.error\nH 58 10   0.1470588\nP  9 63   0.1250000\n\n\n   H  P class.error\nH 58 10   0.1470588\nP  8 64   0.1111111\n\n\n[1] \"Test error:8.82%\""
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#section",
    "href": "presentations/random_forest/apresentacao.html#section",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "",
    "text": "No gráfico abaixo é possível perceber como a escolha do número de variáveis em cada split faz diferença para o resultado final do modelo.\n\n\n\n\n\n\n\n\n\n\nAinda, podemos verficiar a acurácia do modelo pelo número de árvores:"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\nUma das vantagens das florestas aleatórias é sua robustez a pontos atípicos, ou outliers. O exemplo a seguir demonstra a robustez desses modelos a contaminações, além de compará-los a outros métodos de classificação:\n\npacman::p_load(randomForest)\npacman::p_load(caret,e1071,VGAM)\n\niris &lt;- iris %&gt;%\n  mutate(cor = ifelse(Species == \"setosa\",1,ifelse(Species == \"versicolor\",2,3)))\n\nset.seed(150167636)\nind &lt;- sample(2, nrow(iris), replace = TRUE,\n              prob = c(0.7, 0.3))\ntrain &lt;- iris[ind==1,]\ntest &lt;- iris[ind==2,]\n\n\n\ni=4 #Número de pontos contaminados\ndadosPoluidos1 &lt;- train[train$cor==1,]\ndadosPoluidos1 &lt;- dadosPoluidos1[sample(1:nrow(dadosPoluidos1),i,replace = F),]\n\ndadosPoluidos2 &lt;- train[train$cor==3,]\ndadosPoluidos2 &lt;- dadosPoluidos2[sample(1:nrow(dadosPoluidos2),i,replace = F),]\n\ndadosPoluidos1$Petal.Length &lt;- dadosPoluidos1$Petal.Length + 5\ndadosPoluidos1$Petal.Width &lt;- dadosPoluidos1$Petal.Width + 1.7\n\ndadosPoluidos2$Petal.Length &lt;- dadosPoluidos2$Petal.Length - 4\ndadosPoluidos2$Petal.Width &lt;- dadosPoluidos2$Petal.Width - 1.2\n\nDadosExempOutTreino &lt;- rbind(train,dadosPoluidos1,dadosPoluidos2)\n#DadosExempOutTreino &lt;- DadosExempOutTreino[,-6]"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-1",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-2",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-2",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n\nRandom Forest\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         2\n  virginica       0          1        10\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8854          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.8333\nSpecificity                  1.000            0.9310           0.9643\nPos Pred Value               1.000            0.8333           0.9091\nNeg Pred Value               1.000            0.9643           0.9310\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2500\nDetection Prevalence         0.425            0.3000           0.2750\nBalanced Accuracy            1.000            0.9201           0.8988\n\n\n\n\nRegressão logística\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0          6         1\n  virginica       0          5        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.85            \n                 95% CI : (0.7016, 0.9429)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.669e-08       \n                                          \n                  Kappa : 0.7697          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.5455           0.9167\nSpecificity                  1.000            0.9655           0.8214\nPos Pred Value               1.000            0.8571           0.6875\nNeg Pred Value               1.000            0.8485           0.9583\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.1500           0.2750\nDetection Prevalence         0.425            0.1750           0.4000\nBalanced Accuracy            1.000            0.7555           0.8690"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-3",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-3",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n\nSVM linear\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0          9         1\n  virginica       0          2        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8852          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.8182           0.9167\nSpecificity                  1.000            0.9655           0.9286\nPos Pred Value               1.000            0.9000           0.8462\nNeg Pred Value               1.000            0.9333           0.9630\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2250           0.2750\nDetection Prevalence         0.425            0.2500           0.3250\nBalanced Accuracy            1.000            0.8918           0.9226\n\n\n\n\nSVM radial\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         1\n  virginica       0          1        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.95            \n                 95% CI : (0.8308, 0.9939)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 2.026e-12       \n                                          \n                  Kappa : 0.9235          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.9167\nSpecificity                  1.000            0.9655           0.9643\nPos Pred Value               1.000            0.9091           0.9167\nNeg Pred Value               1.000            0.9655           0.9643\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2750\nDetection Prevalence         0.425            0.2750           0.3000\nBalanced Accuracy            1.000            0.9373           0.9405"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-4",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-4",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\nComparando robustez de modelos com observações contaminadas\nSem contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-5",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-5",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n2 contaminações\n\n\n\n\n\n\n\n\n\n\n4 contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-6",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-6",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n6 contaminações\n\n\n\n\n\n\n\n\n\n\n8 contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens-1",
    "href": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Vantagens e desvantagens",
    "text": "Vantagens e desvantagens\n\nsobre classificação por florestas aleatórias:\n\n\n\nVantagens\n\n\nRobusto contra overfitting;\nTrabalha bem com dados de alta dimesão;\nConsegue captar relações não-lineares nos dados;\nFornece uma medida de importância;\nRobusto contra outliers e ruídos;\nConsegue lidar com dados faltantes.\n\n\n\nDesvantagens\n\n\nDificil interpretação;\nNão adequado para dados escassos;\nDemora para fazer predições;\nRequer ajuste de hiper-parâmetros."
  },
  {
    "objectID": "presentations/npa/lista1.html",
    "href": "presentations/npa/lista1.html",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "",
    "text": "Show the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(MASS)"
  },
  {
    "objectID": "presentations/npa/lista1.html#solução",
    "href": "presentations/npa/lista1.html#solução",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "Solução",
    "text": "Solução\nPrecisamos calcular a inversa da distribuição, tal que se \\(\\mathbf{f(x)=\\frac{1}{2}e^{-|x|}},\\) temos \\(\\mathbf{x=F^{-1}(u) \\rightarrow u = \\frac{1}{2}e^{-|x|}}\\), pois este é o inverso da acumulada. Como desejamos isolar \\(\\mathbf{x}\\), devemos manipular a equação tal que \\(\\mathbf{2u=e^{-|z|} \\rightarrow ln(2u)=-|x| \\rightarrow -ln(2u)=|x| \\therefore x = ln(2u) \\cup-ln(2u)}.\\) Se tomarmos os limites de \\(\\mathbf{e^{-|x|}}\\), temos que \\(\\mathbf{\\lim_{x\\to\\infty} e^{-|x|}=0}\\), e \\(\\mathbf{\\lim_{x\\to0}e^{-|x|}=1}\\). Como temos a relação \\(\\mathbf{2u=e^{-|x|}}\\), sabemos que \\(\\mathbf{0 \\leq 2u \\leq 1 \\rightarrow 0 \\leq u \\leq \\frac{1}{2}}\\). Desta forma, para gerar valores da distribuição Laplace padrão, iremos gerar valores uniforme para u tal que \\(\\mathbf{u\\sim U[0,\\frac{1}{2}]}\\). Além disso, geraremos uma segunda uniforme \\(U_s\\sim U[0,1]\\) como suporte para garantir a geração de valores positivos e negativos, dado que \\(\\mathbf{x = \\pm ln(2u)}\\), tal que: \\[\nx=\n\\begin{cases}\n0 \\leq U_s &lt; 0.5 \\rightarrow x = -ln(2u)\\\\\n0.5 \\leq U_s \\leq 1 \\rightarrow x = ln(2u)\n\\end{cases}\n\\] E, desta forma, garantimos gerar valores em todo o suporte de x.\n\n\nShow the code\nn &lt;- 1000\nu &lt;- runif(n, min = 0, max = .5)\nx &lt;- ifelse(runif(n) &lt; 0.5, -log(2*u), log(2*u))\nt &lt;- seq(-10,10,.01)\nhist(x,probability = TRUE,main = \"\")\nlines(t,(1/2)*exp(-abs(t)),col=c(\"red\"))"
  },
  {
    "objectID": "presentations/npa/lista1.html#solução-1",
    "href": "presentations/npa/lista1.html#solução-1",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "Solução",
    "text": "Solução\nPara utilizar o método de aceitação-rejeição, desejamos gerar um valor X de uma distribuição alvo. Para isso, primeiramente devemos derivar a densidade alvo. No caso, a posteriori será a convolução da verossimilhança com a priori. Neste caso, tomaremos uma priori Cauchy(0,1) e a verossimilhança fornecida por dados gerados a partir de uma Normal, de parâmetros média 3 e variância unitária, isto é, \\(\\mathbf{D} = (d_1,...,d_n) \\sim N(3,1)\\). Utilizaremos ainda uma densidade candidata/instrumental Cauchy(0,1), isto é, geraremos um valor Y desta distribuição, e iremos avaliar se Y é tal que \\(u \\leq \\frac{1}{M}\\frac{f(y)}{g(y)},\\) onde \\(u\\sim U[0,1], f(y)\\) é a posteriori avaliada em Y e \\(g(y)\\) é a densidade da Cauchy(0,1) no ponto Y. Se de fato u for menor ou igual a esta quantidade, então tomaremos X = Y. Irei fixar o número de simulações Nsim = 10.000.\n\nPara n = 10\n\n\nShow the code\nn = 10\ntheta0 = 3\nset.seed(251106723)\nx = rnorm(n,3)\nM = prod(dnorm(x,mean(x)))\n\naceito = 0\nNsim = 10000\namostras = numeric(Nsim)\n\nfor (i in 1:Nsim){\n  theta = rcauchy(1,0,1)\n  posteriori = prod(dnorm(x, mean = theta, sd = 1)) * dcauchy(theta, 0, 1)\n  candidata = dcauchy(theta,0,1)\n  \n  if (runif(1) &lt;= (1/M)*(posteriori/candidata)){\n      aceito = aceito + 1\n      amostras[i] &lt;- theta\n  }\n}\namostras &lt;- amostras[amostras != 0] \nmain = paste0(\"n = \", n)\nsub = paste0(\"Amostras não rejeitadas = \", aceito)\nhist(amostras,main=main,sub = sub)\n\n\n\n\n\n\n\n\n\n\n\nPara n = 10,25,50,100\n\n\nShow the code\npar(mfrow=c(2,2))\nfor (n in c(10,25,50,100)){\n  theta0 = 3\n  set.seed(251106723)\n  x = rnorm(n,3)\n  M = prod(dnorm(x,mean(x)))\n  \n  aceito = 0\n  Nsim = 10000\n  amostras = numeric(Nsim)\n  \n  for (i in 1:Nsim){\n    theta = rcauchy(1,0,1)\n    posteriori = prod(dnorm(x, mean = theta, sd = 1)) * dcauchy(theta, 0, 1)\n    candidata = dcauchy(theta,0,1)\n    \n    if (runif(1) &lt;= (1/M)*(posteriori/candidata)){\n      aceito = aceito + 1\n      amostras[i] &lt;- theta\n    }\n  }\n  amostras &lt;- amostras[amostras != 0] \n  main = paste0(\"n = \", n)\n  sub = paste0(\"Amostras não rejeitadas = \", aceito)\n  hist(amostras,main=main,sub = sub)\n}\n\n\n\n\n\n\n\n\n\nShow the code\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "presentations/npa/lista1.html#conclusões",
    "href": "presentations/npa/lista1.html#conclusões",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "Conclusões",
    "text": "Conclusões\nNotamos que \\(\\theta_0\\) tende a ser melhor recuperado para valores maiores de n. Entretanto, o aumento do valor de n também implica numa maior quantidade de valores simulados rejeitados, indicando que a priori Cauchy(0,1) não é apropriada para este caso e/ou que este método é ineficiente ante a outras possibilidades."
  },
  {
    "objectID": "presentations/npa/lista1.html#solução-2",
    "href": "presentations/npa/lista1.html#solução-2",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "Solução",
    "text": "Solução\nPodemos utilizar a decomposição de Cholesky para gerar valores de uma normal multivariada à partir da geração de valores normal univariado.\n\n\nShow the code\ncholesky &lt;- function(n, mu, Sigma) {\n  d &lt;- length(mu)\n  C &lt;- chol(Sigma)\n  Z &lt;- matrix(rnorm(n*d), nrow=n, ncol=d)\n  X &lt;- Z %*% C + matrix(mu, n, d, byrow=TRUE)\n  X\n  }\n\nn = 200\nmu = c(0,1,2)\nSigma = matrix(c(1,-.5,.5,\n                 -.5,1,-.5,\n                 .5,-.5,1),3,3,byrow = T)\n\nvalores = cholesky(n, mu, Sigma)\n\nhead(valores)\n\n\n           [,1]       [,2]      [,3]\n[1,]  1.1521826  0.1872210 5.0230496\n[2,]  0.4588406 -0.1879680 3.4164468\n[3,]  1.2312254  0.1287265 2.5375316\n[4,] -0.4598803  2.1113306 0.3159352\n[5,]  0.1932503  0.7523005 2.5183003\n[6,]  0.7210722  0.3108864 3.6447183"
  },
  {
    "objectID": "presentations/npa/lista1.html#solução-3",
    "href": "presentations/npa/lista1.html#solução-3",
    "title": "Lista 1 — Geração de NPA’s (Números Pseudo-Aleatórios)",
    "section": "Solução",
    "text": "Solução\n\n\nShow the code\na1 &lt;- .5\na2 &lt;- .8\nb1 &lt;- 1\nb2 &lt;- 2\nrho &lt;- 0.7\n\nn &lt;- 1000\n\nset.seed(150167636)\nu1 &lt;- rnorm(n)\nset.seed(251106723)\nu2 &lt;- rnorm(n)\n\nz1 &lt;- ((sqrt(1+rho) + sqrt(1-rho))/2)*u1 + (((sqrt(1+rho) - sqrt(1-rho))/2)/2)*u2\nz2 &lt;- ((sqrt(1+rho) - sqrt(1-rho))/2)*u1 + (((sqrt(1+rho) + sqrt(1-rho))/2)/2)*u2\n\nt1 &lt;- b1 * ((1/2)*a1*z1 + sqrt((((1/2)*a1*z1)^2)+1)^2)\nt2 &lt;- b2 * ((1/2)*a2*z2 + sqrt((((1/2)*a2*z2)^2)+1)^2)\n\nz &lt;- kde2d(t1, t2, n = n)\nplot(t1, t2)\ncontour(z, lwd = 1, add = TRUE,col = hcl.colors(10, \"Spectral\"))"
  },
  {
    "objectID": "presentations/montecarlo/lista4.html#solução",
    "href": "presentations/montecarlo/lista4.html#solução",
    "title": "Lista 4 — Métodos de Monte Carlo",
    "section": "Solução",
    "text": "Solução\n\n\nShow the code\nN = 1000\nalpha = .05\nn = 10\np = 2.262157\nlambda = seq(0,20,by=1)\nnlambda = length(lambda)\ncobertura &lt;- numeric(nlambda)\n\nfor (i in 1:nlambda){\n  Lambda = lambda[i]\n  capturado = 0\n  for (j in 1:N){\n    x = rpois(n,Lambda)\n    xbar = mean(x)\n    sigma2 = (1/(n-1)) * sum((x-xbar)^2)\n    ici = xbar - (p * sqrt(sigma2/n))\n    ics = xbar + (p * sqrt(sigma2/n))\n    if (Lambda &gt;= ici & Lambda &lt;= ics){\n      capturado = capturado + 1}\n    }\n  cobertura[i] = capturado/N\n}\n\nplot(lambda, cobertura, type = \"l\", ylim = c(0.9, 1), \n     xlab = expression(lambda), ylab = \"Coeficiente de confiança\",\n     main = \"Estimativa de Monte Carlo para o coeficiente de confiança\")\nabline(h = 1 - alpha, col = \"red\")\n\n\n\n\n\n\n\n\n\nNotamos pelo gráfico que os intervalos contém o verdadeiro parâmetro empiricamente aproximadamente a quantidade teórica esperada de 95%, para valores de \\(\\lambda=\\{1,2,3,...,20\\}\\). Este resultado, porém, difere para valores muito pequenos de \\(\\lambda\\), onde a sensibilidade da estimativa empírica para \\(n=10\\) não é tão poderosa, como pode-se observar no gráfico a seguir, para valores de \\(\\lambda\\) de 0,025 a 1, de 0,025 em 0,025:\n\n\nShow the code\nlambda = seq(0.025,1,by=0.025)\nnlambda = length(lambda)\ncobertura &lt;- numeric(nlambda)\n\nfor (i in 1:nlambda){\n  Lambda = lambda[i]\n  capturado = 0\n  for (j in 1:N){\n    x = rpois(n,Lambda)\n    xbar = mean(x)\n    sigma2 = (1/(n-1)) * sum((x-xbar)^2)\n    ici = xbar - (p * sqrt(sigma2/n))\n    ics = xbar + (p * sqrt(sigma2/n))\n    if (Lambda &gt;= ici & Lambda &lt;= ics){\n      capturado = capturado + 1}\n    }\n  cobertura[i] = capturado/N\n}\n\nplot(lambda, cobertura, type = \"l\", ylim = c(0.9, 1), \n     xlab = expression(lambda), ylab = \"Coeficiente de confiança\",\n     main = \"Estimativa de Monte Carlo para o coeficiente de confiança\")\nabline(h = 1 - alpha, col = \"red\")\n\n\n\n\n\n\n\n\n\nPara este cenário, seria necessário aumentar o valor de \\(n\\)."
  },
  {
    "objectID": "presentations/montecarlo/lista4.html#solução-1",
    "href": "presentations/montecarlo/lista4.html#solução-1",
    "title": "Lista 4 — Métodos de Monte Carlo",
    "section": "Solução",
    "text": "Solução\n\n\nShow the code\nalpha = 0.05\nN = 1000\nm = c(10, 20, 50, 100, 500)\nepsilon = seq(0, 1, by = 0.05)\n\nrejeicao = matrix(0, nrow = length(m), ncol = length(epsilon))\nrownames(rejeicao) = paste0(\"n=\", m)\ncolnames(rejeicao) = paste0(\"e=\", epsilon)\n\nfor (i in 1:length(m)) {\n  n = m[i]\n  for (j in 1:length(epsilon)) {\n    eps = epsilon[j]\n    rejeicoes = 0\n    for (k in 1:N) {\n      x = rnorm(n, mean = 0, sd = 1)\n      n_contaminados = round(eps * n)\n      y = c(rnorm(n_contaminados, mean = 50, sd = 1),rnorm(n - n_contaminados, mean = 0, sd = 1))\n      teste = t.test(x, y)\n      if (teste$p.value &lt; alpha) {\n        rejeicoes = rejeicoes + 1\n      }\n    }\n    rejeicao[i, j] = rejeicoes / N\n  }\n}\n\n\n\n\nShow the code\nknitr::kable(rejeicao)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne=0\ne=0.05\ne=0.1\ne=0.15\ne=0.2\ne=0.25\ne=0.3\ne=0.35\ne=0.4\ne=0.45\ne=0.5\ne=0.55\ne=0.6\ne=0.65\ne=0.7\ne=0.75\ne=0.8\ne=0.85\ne=0.9\ne=0.95\ne=1\n\n\n\n\nn=10\n0.038\n0.049\n0\n0.000\n0.000\n0\n0\n0.999\n0.999\n0.999\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nn=20\n0.048\n0.000\n0\n0.001\n0.868\n1\n1\n1.000\n1.000\n1.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nn=50\n0.050\n0.000\n1\n1.000\n1.000\n1\n1\n1.000\n1.000\n1.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nn=100\n0.036\n0.991\n1\n1.000\n1.000\n1\n1\n1.000\n1.000\n1.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nn=500\n0.053\n1.000\n1\n1.000\n1.000\n1\n1\n1.000\n1.000\n1.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\nPela Tabela acima, nota-se que já para pequenos valores de \\(\\epsilon\\) e/ou maiores valores de \\(n\\), a maior parte dos testes irá rejeitar. Isto se dá pelo fato da contaminação ser bastante severa, em que ao adicionar uma pequena proporção de uma contaminação \\(\\sim N(50,1)\\), o teste t irá rejeitar a igualdade de médias em relação à amostra gerada da distribuição \\(N(0,1)\\).\nEsta evolução pode ser melhor observada pelo gráfico da curva de potência em função de \\(\\epsilon\\) para cada tamanho de amostra:\n\n\nShow the code\ndf = melt(rejeicao)\ncolnames(df) = c(\"n\",\"e\",\"rejeicao\")\n\nggplot(df, aes(x = e, y = rejeicao, color = factor(n), group = factor(n))) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = alpha, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Taxa de rejeição do teste T\",\n    x = \"Proporção da contaminação\",\n    y = \"Proporção de rejeições de H0\",\n    color = \"Tamanho da amostra\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "presentations/em/lista3.html#solução",
    "href": "presentations/em/lista3.html#solução",
    "title": "Lista 3 — Algoritmo EM",
    "section": "Solução",
    "text": "Solução\nComo o EMV baseada na log-verossimilhança com dados censurados é\n\\[\\mathbf{\\hat{\\theta}=(\\frac{n}{m}\\bar{y})^{-1}}.\\]\nPodemos tomar uma estimativa inicial de \\(\\theta\\) a partir da definição da esperança de uma v.a. exponencial não censurada, isto é, \\(\\mathbb{E}[Y] = \\frac{1}{\\theta} \\rightarrow \\theta = \\frac{1}{\\bar{y}}\\). Utilizando esta estimativa inicial, calculamos a esperança condicional dos dados censurados, tal que \\(\\mathbb{E}[z_i]=y_i+\\frac{1}{\\theta}\\). Com isto, calculamos o EMV tal que\n\\[\n\\begin{equation}\n\\begin{split}\n\\hat{\\theta} & = \\left(\\frac{n}{m}\\bar{y}\\right)^{-1} \\\\\n& = \\frac{1}{\\frac{n}{m}\\bar{y}} \\\\\n& = \\frac{1}{\\left(\\frac{n}{m}\\frac{1}{n}\\sum_{i=1}^n y_i\\right)} \\\\\n& = \\frac{1}{\\left(\\frac{1}{m}\\sum_{i=1}^n y_i\\right)} \\\\\n& = \\frac{m}{\\sum_{i=1}^n y_i} \\\\\n\\end{split}\n\\end{equation}\n\\]\nE faremos esta atualização até obter uma estimativa estabilizada do parâmetro \\(\\theta\\), recalculando a esperança condicional de \\(z_i\\) para cada novo \\(\\theta_i,i&gt;0\\in\\mathbb{Z}.\\)\n\n\nShow the code\ntheta &lt;- 1 / mean(time) \n\nfor (i in 1:1000) {\n  # Passo E: \"Plugando\" a esperança calculada para os dados censurados\n  time_recalc &lt;- time\n  censura &lt;- (status == 0)\n  time_recalc[censura] &lt;- time[censura] + 1 / theta\n  \n  # Passo M: maximizando\n  theta_new &lt;- sum(status) / sum(time_recalc)\n  \n  if (abs(theta_new - theta) &lt; 1e-6) break\n  theta &lt;- theta_new\n}\n\ncat(\"Estimativa final de theta:\", round(theta,3),\"(em\",i,\"iterações).\")\n\n\nEstimativa final de theta: 0.081 (em 14 iterações).\n\n\nNote que, como foi dado o cálculo do EMV, sequer foi necessário expandir ou calcular a verossimilhança, bastando atualizar os valores dos parâmetros diretamente nos dados. Este caminho simplificado somente é possível por conta das propriedades de perda de memória da v.a. exponencial, visto que o novo \\(\\hat{\\theta}\\) calculado também seguirá distribuição exponencial, permitindo a iteração direta."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "",
    "text": "Show the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, readxl, janitor,xgboost,tidymodels,vip,rpart.plot,\n               conflicted,tictoc,finetune,doParallel,DataExplorer,future,glmnet,\n               data.table,summarytools,knitr,compareGroups,bonsai,discrim,\n               baguette,naivebayes)\n\nregisterDoParallel(cores = parallel::detectCores())\n\ntidymodels_prefer()\ndf = read_csv(\"data.csv\")"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelagem",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelagem",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelagem",
    "text": "Modelagem\nNum contexto em que temos tantas covariáveis, tantas observações e, apesar de alguns indicativos observados na análise exploratória, não é possível observar um padrão óbvio que indique o salário do indivíduo. Para isso, utilizei do recurso da modelagem com apoio computacional afim de tornar possível esta análise. Um diferencial deste trabalho é a utilização do framework tidymodels, que é bastante verborrágico e permite uma compreensão das etapas do modelo pela leitura do código, além de eficiência e praticidade."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#parâmetros-gerais",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#parâmetros-gerais",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Parâmetros gerais",
    "text": "Parâmetros gerais\nIrei testar diversos modelos e fazer comparação de resultados destes, mas utilizarei a mesma receita para todos.\n\n\nShow the code\nset.seed(150167636)\nsplit = initial_split(df, prop = .8, strata = salary)\ntrain = training(split)\ntest = testing(split)\n\ncv_folds &lt;- vfold_cv(train, \n                     v = 5, \n                     strata = salary)\n\nrecipe &lt;- recipe(salary ~ .,\n                 data = train) %&gt;% \n  update_role(fnlwgt, new_role = \"case_weight\") %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ starts_with(\"occupation\"):starts_with(\"race\") + \n                  starts_with(\"occupation\"):starts_with(\"sex\") +\n                  starts_with(\"hours_per_week\"):starts_with(\"sex\"))\n\ndados_preparados &lt;- recipe %&gt;% \n  prep() %&gt;% \n  juice()\nkable(head(dados_preparados))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation_num\ncapital_gain\ncapital_loss\nhours_per_week\nsalary\nworkclass_not_working\nworkclass_private\nworkclass_self_employed\nworkclass_unknown\neducation_low_education\noccupation_Others\noccupation_services\noccupation_white_collar\nrace_Asian.Pac.Islander\nrace_Black\nrace_Other\nrace_White\nsex_Male\ncountry_United.States\ncivil_status_Single\noccupation_Others_x_race_Asian.Pac.Islander\noccupation_Others_x_race_Black\noccupation_Others_x_race_Other\noccupation_Others_x_race_White\noccupation_services_x_race_Asian.Pac.Islander\noccupation_services_x_race_Black\noccupation_services_x_race_Other\noccupation_services_x_race_White\noccupation_white_collar_x_race_Asian.Pac.Islander\noccupation_white_collar_x_race_Black\noccupation_white_collar_x_race_Other\noccupation_white_collar_x_race_White\noccupation_Others_x_sex_Male\noccupation_services_x_sex_Male\noccupation_white_collar_x_sex_Male\nsex_Male_x_hours_per_week\n\n\n\n\n0.0314592\n1.25e-05\n1.1354862\n0.1510791\n-0.2161513\n-0.0353722\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n-0.0353722\n\n\n0.8376443\n1.35e-05\n1.1354862\n-0.1461666\n-0.2161513\n-2.2198774\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n-2.2198774\n\n\n-0.0418303\n3.49e-05\n-0.4209394\n-0.1461666\n-0.2161513\n-0.0353722\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.0353722\n\n\n-0.7747258\n5.48e-05\n1.1354862\n-0.1461666\n-0.2161513\n-0.0353722\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0.0000000\n\n\n0.7643547\n2.59e-05\n-1.9773649\n-0.1461666\n-0.2161513\n-1.9771546\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n\n\n-1.1411735\n1.98e-05\n1.1354862\n-0.1461666\n-0.2161513\n-0.8444482\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0.0000000\n\n\n\n\n\nPodemos ver um pedaço da forma geral dos dados preparados para serem inseridos no modelo."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-1-regressão-logística",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-1-regressão-logística",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 1: Regressão logística",
    "text": "Modelo 1: Regressão logística\nComo a variável resposta é binária, o primeiro modelo que podemos tentar seria o logístico\n\n\nShow the code\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(glm_spec)\n\nglm_fit &lt;- glm_wf %&gt;%\n  fit(data = train)\n\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.7372041\n0.4866916\n-3.5694143\n0.0003578\n\n\nage\n0.4055276\n0.0227845\n17.7984123\n0.0000000\n\n\neducation_num\n0.8441476\n0.0438024\n19.2717200\n0.0000000\n\n\ncapital_gain\n2.2344546\n0.0800493\n27.9134654\n0.0000000\n\n\ncapital_loss\n0.2691885\n0.0165243\n16.2904180\n0.0000000\n\n\nhours_per_week\n0.3348932\n0.0455127\n7.3582312\n0.0000000\n\n\nworkclass_not_working\n-11.7789355\n117.4336842\n-0.1003029\n0.9201039\n\n\nworkclass_private\n-0.0699743\n0.0559738\n-1.2501244\n0.2112541\n\n\nworkclass_self_employed\n-0.3486299\n0.0730037\n-4.7755074\n0.0000018\n\n\nworkclass_unknown\n-0.5554042\n1.8010975\n-0.3083699\n0.7578009\n\n\neducation_low_education\n0.1502829\n0.0759029\n1.9799356\n0.0477108\n\n\noccupation_Others\n0.1946984\n2.0025527\n0.0972251\n0.9225476\n\n\noccupation_services\n0.8077583\n0.8054694\n1.0028417\n0.3159372\n\n\noccupation_white_collar\n0.4311601\n0.5736424\n0.7516183\n0.4522806\n\n\nrace_Asian.Pac.Islander\n0.7500891\n0.5035234\n1.4896806\n0.1363082\n\n\nrace_Black\n0.4686787\n0.4650463\n1.0078109\n0.3135452\n\n\nrace_Other\n0.1107415\n0.6625542\n0.1671433\n0.8672573\n\n\nrace_White\n0.5799804\n0.4454281\n1.3020741\n0.1928910\n\n\nsex_Male\n0.1722064\n0.1804855\n0.9541286\n0.3400185\n\n\ncountry_United.States\n0.2594948\n0.0762643\n3.4025743\n0.0006675\n\n\ncivil_status_Single\n-2.4131329\n0.0517315\n-46.6472524\n0.0000000\n\n\noccupation_Others_x_race_Asian.Pac.Islander\n-1.1841893\n1.4582220\n-0.8120775\n0.4167472\n\n\noccupation_Others_x_race_Black\n-0.4225307\n1.3041276\n-0.3239949\n0.7459419\n\n\noccupation_Others_x_race_Other\n1.4529448\n1.6820682\n0.8637847\n0.3877062\n\n\noccupation_Others_x_race_White\n0.0266894\n1.2087874\n0.0220795\n0.9823846\n\n\noccupation_services_x_race_Asian.Pac.Islander\n-1.2872096\n0.8711872\n-1.4775350\n0.1395323\n\n\noccupation_services_x_race_Black\n-1.1793043\n0.8168805\n-1.4436680\n0.1488324\n\n\noccupation_services_x_race_Other\n-1.0749159\n1.4074752\n-0.7637192\n0.4450346\n\n\noccupation_services_x_race_White\n-1.1017327\n0.7831182\n-1.4068537\n0.1594708\n\n\noccupation_white_collar_x_race_Asian.Pac.Islander\n-0.3004502\n0.6078681\n-0.4942687\n0.6211164\n\n\noccupation_white_collar_x_race_Black\n0.0860022\n0.5730514\n0.1500777\n0.8807033\n\n\noccupation_white_collar_x_race_Other\n-0.3731938\n0.8639278\n-0.4319734\n0.6657608\n\n\noccupation_white_collar_x_race_White\n0.1513841\n0.5481445\n0.2761755\n0.7824132\n\n\noccupation_Others_x_sex_Male\n-0.3821967\n0.3098797\n-1.2333712\n0.2174373\n\n\noccupation_services_x_sex_Male\n0.2564560\n0.2581336\n0.9935012\n0.3204658\n\n\noccupation_white_collar_x_sex_Male\n0.1581810\n0.1879843\n0.8414584\n0.4000912\n\n\nsex_Male_x_hours_per_week\n0.0586262\n0.0513488\n1.1417232\n0.2535691"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-logístico",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-logístico",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo logístico",
    "text": "Importâncias para o modelo logístico\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- glm_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4556  649\n         1  388  920\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_logit = augment(glm_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-2-regressão-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-2-regressão-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 2: Regressão Lasso",
    "text": "Modelo 2: Regressão Lasso\nComo observado na receita do modelo, existem 38 covariáveis nesta modelagem. Diversas abordagens podem ser utilizadas para selecionar as covariáveis de maior importância, sendo uma dessas a regressão lasso, que penaliza coeficientes e torna-os 0 em caso de insignificância.\nEste é um modelo que contém um hiperparâmetro, portanto iremos ajustar um grid para escolher o melhor possível.\n\n\nShow the code\nlasso_spec &lt;- logistic_reg(penalty = tune(),\n                           mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(lasso_spec)\n\ngrid &lt;- grid_regular(penalty(),\n                     levels = 100)\n\nplan(multisession)\nset.seed(150167636)\nlasso_res &lt;- lasso_wf %&gt;%\n  tune_grid(resamples = cv_folds,\n            grid = grid,\n            metrics = metric_set(roc_auc))\n\n\n\n\nShow the code\nlasso_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, penalty) %&gt;%\n  pivot_longer(penalty,\n               names_to = \"hiperparâmetro\",\n               values_to = \"valor\") %&gt;%\n  ggplot(aes(valor, mean)) +\n  geom_point()+\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 10),\n                          axis.text.y = element_text(size = 10),\n                          axis.title.x = element_text(size = 10),\n                          axis.title.y = element_text(size = 10),\n                          plot.title = element_text(size = 10, hjust = 0.5))+\n  labs(x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\nPodemos ver que um menor valor de penalização é benéfico ao modelo, visto a importância relativa das covariáveis serem altas neste caso"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Estimativa dos parâmetros para o modelo Lasso",
    "text": "Estimativa dos parâmetros para o modelo Lasso\n\n\nShow the code\nbest_params = lasso_res %&gt;%\n  select_best(metric = \"roc_auc\")\n\nbest_wf = finalize_workflow(lasso_wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n-1.1948793\n0.0007391\n\n\nage\n0.3943655\n0.0007391\n\n\neducation_num\n0.7870445\n0.0007391\n\n\ncapital_gain\n2.1066124\n0.0007391\n\n\ncapital_loss\n0.2622005\n0.0007391\n\n\nhours_per_week\n0.3315385\n0.0007391\n\n\nworkclass_not_working\n-1.9072610\n0.0007391\n\n\nworkclass_private\n-0.0252262\n0.0007391\n\n\nworkclass_self_employed\n-0.2793742\n0.0007391\n\n\nworkclass_unknown\n-0.1482158\n0.0007391\n\n\neducation_low_education\n0.0470209\n0.0007391\n\n\noccupation_Others\n-0.1167739\n0.0007391\n\n\noccupation_services\n-0.0488290\n0.0007391\n\n\noccupation_white_collar\n0.3271691\n0.0007391\n\n\nrace_Asian.Pac.Islander\n0.0246369\n0.0007391\n\n\nrace_Black\n0.0000000\n0.0007391\n\n\nrace_Other\n-0.2075199\n0.0007391\n\n\nrace_White\n0.0681928\n0.0007391\n\n\nsex_Male\n0.1888269\n0.0007391\n\n\ncountry_United.States\n0.2273567\n0.0007391\n\n\ncivil_status_Single\n-2.3832942\n0.0007391\n\n\noccupation_Others_x_race_Asian.Pac.Islander\n-0.5597923\n0.0007391\n\n\noccupation_Others_x_race_Black\n-0.1860392\n0.0007391\n\n\noccupation_Others_x_race_Other\n0.5816320\n0.0007391\n\n\noccupation_Others_x_race_White\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_Asian.Pac.Islander\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_Black\n-0.0567795\n0.0007391\n\n\noccupation_services_x_race_Other\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_White\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_race_Asian.Pac.Islander\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_race_Black\n0.1298919\n0.0007391\n\n\noccupation_white_collar_x_race_Other\n-0.2649066\n0.0007391\n\n\noccupation_white_collar_x_race_White\n0.2695555\n0.0007391\n\n\noccupation_Others_x_sex_Male\n-0.3617209\n0.0007391\n\n\noccupation_services_x_sex_Male\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_sex_Male\n0.1420720\n0.0007391\n\n\nsex_Male_x_hours_per_week\n0.0511361\n0.0007391"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo Lasso",
    "text": "Importâncias para o modelo Lasso\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4571  652\n         1  373  917\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_lasso = augment(final_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-3-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-3-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 3: XGBoost",
    "text": "Modelo 3: XGBoost\nO XGBoost é um modelo de Gradient boosting baseado em árvores, que costuma performar bem em tarefas como esta, de classificação com diversas covariáveis\nEste é um modelo de Boosting, ou seja, o “encaixe” de diversos modelos fracos afim da construção de um modelo robusto a partir da combinação destes resultados.\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#ajuste-do-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#ajuste-do-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Ajuste do XGBoost",
    "text": "Ajuste do XGBoost\nTambém iremos realizar o fine tuning de alguns hiperparâmetros deste modelo, no caso o número de árvores e a profundidade destas, afim de obter o melhor modelo.\n\n\nShow the code\nmodel = boost_tree(mode = \"classification\",\n                   trees = tune(),\n                   tree_depth = tune()\n                   ) %&gt;%\n  set_engine(\"xgboost\")\n\nwf = workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(model)\n\ngrid = wf %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 3)\n\nplan(multisession)\nset.seed(150167636)\ntune_res = tune_grid( \n  wf,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(accuracy, roc_auc, sens,spec)\n  )\n\n\n\n\nShow the code\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, trees:tree_depth) %&gt;%\n  pivot_longer(trees:tree_depth,\n               names_to = \"hiperparâmetro\",\n               values_to = \"valor\") %&gt;%\n  ggplot(aes(valor, mean, color = hiperparâmetro)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~hiperparâmetro, scales = \"free_x\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 10),\n                          axis.text.y = element_text(size = 10),\n                          axis.title.x = element_text(size = 10),\n                          axis.title.y = element_text(size = 10),\n                          plot.title = element_text(size = 10, hjust = 0.5))+\n  labs(x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_params = tune_res %&gt;%\n  select_best(metric = \"roc_auc\")\nbest_params\n\n\n# A tibble: 1 × 3\n  trees tree_depth .config             \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1  2000          1 Preprocessor1_Model3\n\n\nVemos que a melhor combinação de hiperparâmetros encontrada é utilizando 2000 árvores de tamanho 1"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o XGBoost",
    "text": "Importâncias para o XGBoost\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nbest_wf = finalize_workflow(wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\n\n\n\nShow the code\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4636  584\n         1  308  985\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_xgboost = augment(final_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/churn/relatorio.html",
    "href": "presentations/churn/relatorio.html",
    "title": "Relatório técnico — Análise de churn",
    "section": "",
    "text": "Bruno Gondim Toledo"
  },
  {
    "objectID": "presentations/churn/relatorio.html#taxa-de-churn",
    "href": "presentations/churn/relatorio.html#taxa-de-churn",
    "title": "Relatório técnico — Análise de churn",
    "section": "Taxa de churn",
    "text": "Taxa de churn\n\n\nMostrar o código\nchurn_rate = df['cancelado'].value_counts(normalize=True)['SIM'] * 100\nchurn_rate = f\"{churn_rate:.2f}\".replace(\".\", \",\")\n\nprint(f\"Taxa global de churn: {churn_rate}%\")\n\n\nTaxa global de churn: 58,85%\n\n\nObserva-se uma taxa de churn superior a 50% nos dados, indicando uma alta evasão de clientes do plano de saúde. Esta taxa é preocupante, visto que pode indicar problemas com o produto ou com o perfil do cliente atendido pela empresa."
  },
  {
    "objectID": "presentations/churn/relatorio.html#quantidade-de-cancelamentos-por-grupos",
    "href": "presentations/churn/relatorio.html#quantidade-de-cancelamentos-por-grupos",
    "title": "Relatório técnico — Análise de churn",
    "section": "Quantidade de cancelamentos por grupos",
    "text": "Quantidade de cancelamentos por grupos\n\n\nMostrar o código\nfaixa_renda = df.groupby(['faixa_renda', 'cancelado'], observed=True).size().unstack()\nfaixa_renda['total'] = faixa_renda.sum(axis=1)\nfaixa_renda['% NÃO'] = (faixa_renda['NÃO'] / faixa_renda['total'] * 100).round(2)\nfaixa_renda['% SIM'] = (faixa_renda['SIM'] / faixa_renda['total'] * 100).round(2)\n\nstyled_faixa_renda = (\n    faixa_renda.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=faixa_renda.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_faixa_renda)\n\nplt.figure(figsize=(9, 6))\nsns.countplot(data=df, x='faixa_renda', hue='cancelado', palette=['steelblue','tomato'], \n              order=df['faixa_renda'].value_counts().index)\nplt.title('Distribuição de Cancelamentos por Faixa de Renda')\nplt.xlabel('Faixa de Renda')\nplt.ylabel('Contagem')\nplt.legend(title='Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\ntotal\n% NÃO\n% SIM\n\n\nfaixa_renda\n \n \n \n \n \n\n\n\n\nAlta renda\n7.402,00\n2.641,00\n10.043,00\n73,70\n26,30\n\n\nBaixa renda\n7.598,00\n50.777,00\n58.375,00\n13,02\n86,98\n\n\nMédia renda\n22.356,00\n0,00\n22.356,00\n100,00\n0,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nEm geral, clientes de baixa renda tendem a ter alta taxa de cancelamento (86,98% dos clientes de baixa renda cancelaram). Dentre os clientes de renda média, não houve cancelamento algum. Para os clientes de alta renda, a evasão foi de 26,3%.\nEstes resultados indicam que podem ser realizadas ações para clientes de faixas de renda específicas — em especial os de baixa renda — na busca da reversão da alta taxa de churn."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-idade-na-adesão",
    "href": "presentations/churn/relatorio.html#cancelamento-por-idade-na-adesão",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por idade na adesão",
    "text": "Cancelamento por idade na adesão\n\n\nMostrar o código\nidade_na_adesao = df.groupby('cancelado', observed=True)['idadenaadesao'].describe().transpose().round(2)\n\nstyled_idade_na_adesao = (\n    idade_na_adesao.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=idade_na_adesao.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_idade_na_adesao)\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df, y='cancelado', x='idadenaadesao', palette=['steelblue','tomato'])\nplt.title('Distribuição de Idade na Adesão por Status de Cancelamento')\nplt.xlabel('Idade na Adesão')\nplt.ylabel('Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\n\n\ncount\n37.356,00\n53.418,00\n\n\nmean\n29,17\n29,43\n\n\nstd\n16,91\n21,13\n\n\nmin\n0,00\n0,00\n\n\n25%\n16,00\n10,00\n\n\n50%\n30,00\n28,00\n\n\n75%\n40,00\n44,00\n\n\nmax\n82,00\n94,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nEm relação à idade na adesão, os clientes que não cancelaram apresentam idade mediana de 30 anos, estando os primeiro e terceiro quartis nas idades 16 e 40 anos, respectivamente. Os clientes que cancelaram apresentam idade mediana de 28 anos, estando os primeiro e terceiro quartis nas idades 10 e 44 anos. Em geral, as idades média e mediana dos grupos são similares, em que clientes que cancelaram o plano apresentam idade mais heterogênea que clientes que não cancelaram."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-tempo-de-plano-em-meses",
    "href": "presentations/churn/relatorio.html#cancelamento-por-tempo-de-plano-em-meses",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por tempo de plano em meses",
    "text": "Cancelamento por tempo de plano em meses\n\n\nMostrar o código\ntempo_de_plano_meses = df.groupby('cancelado', observed=True)['tempo_de_plano_meses'].describe().transpose().round(2)\n\nstyled_tempo_de_plano_meses = (\n    tempo_de_plano_meses.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=tempo_de_plano_meses.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_tempo_de_plano_meses)\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df, y='cancelado', x='tempo_de_plano_meses', palette=['steelblue','tomato'])\nplt.title('Distribuição de Tempo de Plano por Status de Cancelamento')\nplt.xlabel('Tempo de Plano (meses)')\nplt.ylabel('Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\n\n\ncount\n37.356,00\n53.418,00\n\n\nmean\n301,42\n229,05\n\n\nstd\n7,72\n45,73\n\n\nmin\n280,00\n128,00\n\n\n25%\n295,00\n192,00\n\n\n50%\n303,00\n233,00\n\n\n75%\n309,00\n268,00\n\n\nmax\n311,00\n311,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuanto ao tempo do plano em meses, os clientes que cancelaram o plano apresentam média e mediana inferiores aos que não cancelaram o plano. Os clientes que não cancelaram o plano apresentam tempo mínimo de 280 meses, valor este superior ao terceiro quartil do tempo de permanência dos clientes que cancelaram. Para clientes com tempo igual ou superior a 295 meses de permanência, a massa de dados se concentra dentre os que não cancelaram o plano, indicando uma probabilidade de cancelamento desprezível após este tempo de permanência, se analisado somente esta variável.\nEste resultado preliminar indica que ações de permanência no plano podem ser convenientes para reversão da alta taxa de churn, visto que marginalmente clientes que estão no plano a mais tempo apresentam menor propensão ao cancelamento deste."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-pelo-sexo-do-cliente",
    "href": "presentations/churn/relatorio.html#cancelamento-pelo-sexo-do-cliente",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento pelo sexo do cliente",
    "text": "Cancelamento pelo sexo do cliente\n\n\nMostrar o código\ndf['sexo'] = df['sexo'].map({'F': 'Feminino', 'M': 'Masculino'})\n\nsexo = df.groupby(['sexo', 'cancelado'], observed=True).size().unstack()\nsexo['total'] = sexo.sum(axis=1)\nsexo['% NÃO'] = (sexo['NÃO'] / sexo['total'] * 100).round(2)\nsexo['% SIM'] = (sexo['SIM'] / sexo['total'] * 100).round(2)\n\nstyled_sexo = (\n    sexo.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=sexo.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_sexo)\n\nplt.figure(figsize=(9, 6))\nsns.countplot(data=df, x='sexo', hue='cancelado', palette=['steelblue','tomato'])\nplt.title('Distribuição de Cancelamentos por Sexo')\nplt.xlabel('Sexo')\nplt.ylabel('Contagem')\nplt.legend(title='Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\ntotal\n% NÃO\n% SIM\n\n\nsexo\n \n \n \n \n \n\n\n\n\nFeminino\n21.432,00\n29.367,00\n50.799,00\n42,19\n57,81\n\n\nMasculino\n15.198,00\n23.006,00\n38.204,00\n39,78\n60,22\n\n\n\n\n\n\n\n\n\n\n\n\n\nExistem mais clientes do sexo feminino do que clientes do sexo masculino. Existe uma taxa ligeiramente superior de cancelamento entre clientes do sexo feminino do que clientes do sexo masculino. Esta taxa não aparenta ser significativa, possivelmente tratando-se de ruído branco."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-unidade-da-federação-do-cliente",
    "href": "presentations/churn/relatorio.html#cancelamento-por-unidade-da-federação-do-cliente",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por unidade da federação do cliente",
    "text": "Cancelamento por unidade da federação do cliente\n\n\nMostrar o código\nuf_stats = df.groupby('uf', observed=True)['cancelado'].value_counts(normalize=True).unstack() * 100\nuf_stats = uf_stats.reset_index().melt(id_vars='uf', value_name='taxa', var_name='status')\nuf_stats_sim = uf_stats[uf_stats['status'] == 'SIM']\n\nfig = px.choropleth(\n    uf_stats_sim,\n    geojson=\"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\",\n    locations='uf',\n    featureidkey=\"properties.sigla\",\n    color='taxa',\n    scope='south america',\n    color_continuous_scale='YlOrRd',\n    title='Taxa de Cancelamento por UF (%)',\n    labels={'taxa': 'Taxa de Cancelamento (%)'}\n)\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\nfig.show()\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nO mapa coroplético evidencia diferenças estaduais e regionais com relação a taxa de churn. Regiões como sudeste e sul apresentam relativa homogeneidade, enquanto centro-oeste, norte e, principalmente, nordeste, apresentam maior heterogeneidade de taxa entre os estados. É possível que políticas de reversão possam ser aplicadas a nível estadual ou regional. Entretanto, a massa de dados se concentra em clientes do sudeste, o que pode afetar a sensibilidade de um modelo preditivo baseado nestes dados utilizando esta covariável. Desta forma, uma agregação possível é por região do cliente — ainda que isto possa mascarar heterogeneidades regionais, esta pode ser uma etapa necessária para obter um modelo melhor ajustado."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-inadimplência",
    "href": "presentations/churn/relatorio.html#cancelamento-por-inadimplência",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por inadimplência",
    "text": "Cancelamento por inadimplência\n\n\nMostrar o código\ninadimplencia = pd.crosstab(df['inadimplente'], df['cancelado'], normalize='index') * 100\n\nstyled_inadimplencia = (\n    inadimplencia.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=inadimplencia.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_inadimplencia)\n\nplt.figure(figsize=(9, 6))\nsns.countplot(data=df, x='inadimplente', hue='cancelado', palette=['steelblue','tomato'])\nplt.title('Distribuição de Cancelamentos por Inadimplência')\nplt.xlabel('Inadimplente')\nplt.ylabel('Contagem')\nplt.legend(title='Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\ninadimplente\n \n \n\n\n\n\nNÃO\n51,81\n48,19\n\n\nSIM\n12,06\n87,94\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota-se que os clientes adimplentes são maioria. Dentre os clientes inadimplentes, a grande maioria cancelou o plano. Dentre os adimplentes, a distribuição é razoavelmente uniforme entre cancelamento e não cancelamento.\nAnalisando marginalmente o efeito desta covariável, podemos dizer que a inadimplência do cliente aparenta ser fator significativo para cancelamento do plano. Ações de permanência para clientes inadimplentes podem ser delineadas para reversão do cenário de alto churn."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-quantidade-de-consultas-do-cliente-nos-últimos-12-meses",
    "href": "presentations/churn/relatorio.html#cancelamento-por-quantidade-de-consultas-do-cliente-nos-últimos-12-meses",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por quantidade de consultas do cliente nos últimos 12 meses",
    "text": "Cancelamento por quantidade de consultas do cliente nos últimos 12 meses\n\n\nMostrar o código\nqtd_consultas = df.groupby('cancelado', observed=True)['qtd_consultas_12m'].describe().transpose().round(2)\n\nstyled_qtd_consultas = (\n    qtd_consultas.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=qtd_consultas.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_qtd_consultas)\n\nplt.figure(figsize=(9, 6))\nax = sns.histplot(data=df, x='qtd_consultas_12m', hue='cancelado', \n                 bins=range(0, df['qtd_consultas_12m'].max() + 7, 7),\n                 multiple='dodge', kde=True,\n                 palette={'SIM': 'tomato', 'NÃO': 'steelblue'})\n\nlegend_elements = [Patch(facecolor='tomato', label='SIM'),\n                   Patch(facecolor='steelblue', label='NÃO')]\n\nax.legend(handles=legend_elements, title='Cancelado', fontsize=\"large\")\n\nplt.title('Distribuição de Consultas nos Últimos 12 Meses')\nplt.xlabel('Quantidade de Consultas')\nplt.ylabel('Frequência')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\n\n\ncount\n37.356,00\n53.418,00\n\n\nmean\n12,09\n12,07\n\n\nstd\n10,69\n10,80\n\n\nmin\n1,00\n1,00\n\n\n25%\n5,00\n5,00\n\n\n50%\n9,00\n9,00\n\n\n75%\n15,00\n15,00\n\n\nmax\n60,00\n60,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nA quantidade de consultas nos últimos 12 meses apresenta estatísticas de ordem e localização proporcionais se comparado clientes que cancelaram ou não o plano. Isto indica que esta variável, isoladamente, não explica bem o cancelamento do plano."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-quantidade-de-internações-do-cliente-nos-últimos-12-meses",
    "href": "presentations/churn/relatorio.html#cancelamento-por-quantidade-de-internações-do-cliente-nos-últimos-12-meses",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por quantidade de internações do cliente nos últimos 12 meses",
    "text": "Cancelamento por quantidade de internações do cliente nos últimos 12 meses\n\n\nMostrar o código\nqtd_internacoes = df.groupby('cancelado', observed = True)['qtd_internacoes_12m'].describe().transpose().round(2)\n\nstyled_qtd_internacoes = (\n    qtd_internacoes.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=qtd_internacoes.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_qtd_internacoes)\n\n\nplt.figure(figsize=(9, 6))\nsns.histplot(data=df, x='qtd_internacoes_12m', hue='cancelado', \n             bins=range(0, df['qtd_internacoes_12m'].max() + 7, 7),\n             multiple='dodge', kde=True,\n                 palette={'SIM': 'tomato', 'NÃO': 'steelblue'})\n\nlegend_elements = [Patch(facecolor='tomato', label='SIM'),\n                   Patch(facecolor='steelblue', label='NÃO')]\n\nax.legend(handles=legend_elements, title='Cancelado', fontsize=\"large\")\n\nplt.title('Distribuição de Internações nos Últimos 12 Meses')\nplt.xlabel('Quantidade de Internações')\nplt.ylabel('Frequência')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\n\n\ncount\n37.356,00\n53.418,00\n\n\nmean\n1,30\n1,30\n\n\nstd\n1,29\n1,30\n\n\nmin\n0,00\n0,00\n\n\n25%\n1,00\n1,00\n\n\n50%\n1,00\n1,00\n\n\n75%\n2,00\n2,00\n\n\nmax\n9,00\n9,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nA quantidade de internações nos últimos 12 meses apresenta estatísticas de ordem e localização proporcionais ou exatamente iguais se comparado clientes que cancelaram ou não o plano. Isto indica que esta variável, isoladamente, não explica bem o cancelamento do plano."
  },
  {
    "objectID": "presentations/churn/relatorio.html#cancelamento-por-valor-da-mensalidade-do-plano",
    "href": "presentations/churn/relatorio.html#cancelamento-por-valor-da-mensalidade-do-plano",
    "title": "Relatório técnico — Análise de churn",
    "section": "Cancelamento por valor da mensalidade do plano",
    "text": "Cancelamento por valor da mensalidade do plano\n\n\nMostrar o código\nvalor_mensalidade = df.groupby('cancelado', observed = True)['valor_mensalidade'].describe().transpose().round(2)\n\nstyled_valor_mensalidade = (\n    valor_mensalidade.fillna(0)\n    .round(2)\n    .style\n    .format(\"{:,.2f}\", subset=valor_mensalidade.columns,thousands=\".\", decimal=\",\", na_rep=\"0\")\n)\n\ndisplay(styled_valor_mensalidade)\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df, y='cancelado', x='valor_mensalidade', palette=['steelblue','tomato'])\nplt.title('Distribuição de Valor de Mensalidade por Status de Cancelamento')\nplt.xlabel('Valor da Mensalidade')\nplt.ylabel('Cancelado')\nplt.show()\n\n\n\n\n\n\n\n\ncancelado\nNÃO\nSIM\n\n\n\n\ncount\n23.644,00\n33.731,00\n\n\nmean\n898,77\n928,94\n\n\nstd\n349,60\n426,84\n\n\nmin\n550,00\n550,00\n\n\n25%\n550,00\n550,00\n\n\n50%\n720,00\n720,00\n\n\n75%\n1.030,00\n1.030,00\n\n\nmax\n1.870,00\n1.870,00\n\n\n\n\n\n\n\n\n\n\n\n\n\nO valor da mensalidade apresenta estatísticas de ordem e localização similares em ambos os grupos (clientes que cancelaram, clientes que não cancelaram). Isto indica que esta variável não explica bem o cancelamento ou não do plano, se analisada isoladamente."
  },
  {
    "objectID": "presentations/churn/relatorio.html#renda-por-inadimplência-por-cancelamento",
    "href": "presentations/churn/relatorio.html#renda-por-inadimplência-por-cancelamento",
    "title": "Relatório técnico — Análise de churn",
    "section": "Renda por inadimplência por cancelamento",
    "text": "Renda por inadimplência por cancelamento\n\n\nMostrar o código\ndf_prop = (df.groupby(['faixa_renda', 'inadimplente', 'cancelado'], observed = False)\n           .size()\n           .reset_index(name='n'))\n           \ndf_prop['prop'] = (df_prop.groupby(['faixa_renda', 'inadimplente'], observed = False)['n']\n                   .transform(lambda x: x / x.sum()))\n\ndf_prop['faixa_renda'] = pd.Categorical(df_prop['faixa_renda'], \n                                       categories=['Baixa renda', 'Média renda', 'Alta renda'],\n                                       ordered=True)\n\nplt.figure(figsize=(9, 6))\ng = sns.FacetGrid(df_prop, col='inadimplente', height=4, aspect=1, sharey=True)\n\ndef draw_stacked_bar(data, **kwargs):\n    data = data.sort_values('cancelado', ascending=False)\n    bottom = None\n    for cancelado in ['NÃO', 'SIM']:\n        subset = data[data['cancelado'] == cancelado]\n        color = 'steelblue' if cancelado == 'NÃO' else 'tomato'\n        plt.bar(x=subset['faixa_renda'], \n                height=subset['prop'], \n                bottom=bottom,\n                color=color,\n                label=cancelado)\n        if bottom is None:\n            bottom = subset['prop'].values\n        else:\n            bottom += subset['prop'].values\n\ng.map_dataframe(draw_stacked_bar)\n\ng.set_axis_labels('Faixa de Renda', 'Proporção')\ng.set_titles('Inadimplente: {col_name}')\ng.fig.legend(handles=legend_elements, \n             title='Cancelado',\n             loc='upper center',\n             bbox_to_anchor=(.9, 1.1),\n             ncol=2)\n\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 864x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nAo analisar conjuntamente a faixa de renda, inadimplência e cancelamento, nota-se que para os clientes de alta renda, o inadimplento é variável significativa para indicar o cancelamento ou não do plano. Isto indica que pode ser necessária uma política específica para clientes inadimplentes de alta renda, visto que este perfil predomina para o cancelamento do plano ante clientes de alta renda não inadimplentes."
  },
  {
    "objectID": "presentations/churn/relatorio.html#conclusão-da-análise-exploratória",
    "href": "presentations/churn/relatorio.html#conclusão-da-análise-exploratória",
    "title": "Relatório técnico — Análise de churn",
    "section": "Conclusão da análise exploratória",
    "text": "Conclusão da análise exploratória\nComparando os grupos de clientes que cancelaram ou não o plano utilizando análises bivariadas, nota-se que dos dados fornecidos, apenas alguns apresentam indícios de significância na determinação do cancelamento ou não do plano.\nAs variáveis mais significativas — visualmente — foram renda, tempo em meses do plano, unidade da federação do cliente e inadimplência.\nAlém disso, nota-se interação entre a faixa de renda e o inadimplento para explicar o cancelamento do plano dentre os clientes de alta renda. A maior parte dos clientes de alta renda que cancelaram o plano estavam inadimplentes.\nTestes estatísticos para diferença de média ou mediana nos grupos não são adequados, pelo tamanho do conjunto de dados (90.774 instâncias, após remoção de dados absurdos), pois seriam viesados pelo tamanho amostral e detectariam significância em diversos casos pouco significativos.\nO agrupamento das unidades da federação em regiões (Norte, Nordeste, Sudeste, Centro-Oeste e Sul) manteria a significância da diferença entre os grupos, entretanto poderia mascarar algumas heterogeneidades regionais (Como o estado do Piauí, que difere significativamente da tendência dos demais estados do Nordeste). Este agrupamento será realizado na modelagem pois a quantidade de instâncias é insuficiente na estratificação dos grupos para algumas unidades da federação.\nNota-se portanto que a alta taxa de churn aparenta estar mais relacionada com o perfil do cliente do que com o produto da empresa em si, visto que covariáveis mais relacionadas ao produto, como quantidade de consultas, quantidade de internações e valor da mensalidade não apresentaram diferenças entre clientes que cancelaram ou não o plano. Isto pode indicar maior necessidade de compreensão e ações relacionadas ao perfil do cliente do que a readequação do produto oferecido."
  },
  {
    "objectID": "presentations/churn/relatorio.html#política-para-valores-ausentes",
    "href": "presentations/churn/relatorio.html#política-para-valores-ausentes",
    "title": "Relatório técnico — Análise de churn",
    "section": "Política para valores ausentes",
    "text": "Política para valores ausentes\nDiversos tratamentos podem ser realizados no conjunto de dados para lidar com dados ausentes. Ainda na fase da análise exploratória, optou-se por remover algumas instâncias com valores ausentes, sendo elas: idade na adesão ao plano inferior a zero anos ou superior a 100 anos; tempo de plano em meses inferior à zero e unidade da federação não preenchida. Em muitos casos, cada instância é preciosa e devemos evitar ao máximo sua eliminação. Entretanto, neste contexto, a quantidade de observações é suficientemente robusta, permitindo a exclusão de um pequeno número delas sem prejuízo à análise. Para além disso, julguei que o mau preenchimento destas colunas poderia indicar problema no preenchimento de demais informações da instância, o que poderia levar a um viés no modelo, caso fossem imputados valores mais coerentes, como tempo médio ou mediano, ou ainda rótulo de unidade da federação (que, nesse caso, quase certamente seria preenchido com valores mais frequentes — isto é, estados localizados no sudeste).\nPara valores ausentes nas covariáveis numéricas referentes a quantidade de consultas e internações, julguei apropriado realizar a imputação com a mediana, tendo visto que:\n\nestas covariáveis não apresentavam grande diferença entre clientes que cancelaram ou não na análise exploratória;\nas respectivas distribuições incorriam em forte fuga à normalidade, sendo mais indicado a imputação pela mediana neste caso, em detrimento da média.\n\nA covariável sexo também apresentava valores ausentes e, visto sua marginal insignificância aparente na etapa exploratória na determinação do cancelamento ou não por parte do cliente, optou-se por simplesmente não incluir esta covariável na modelagem."
  },
  {
    "objectID": "presentations/churn/relatorio.html#construção-de-features",
    "href": "presentations/churn/relatorio.html#construção-de-features",
    "title": "Relatório técnico — Análise de churn",
    "section": "Construção de features",
    "text": "Construção de features\nNa análise exploratória, construiu-se apenas uma feature: Região (regiao), baseada na unidade da federação do cliente. A construção de features, isto é, novas covariáveis como modificações e/ou combinação das covariáveis existentes, podem ser cruciais na etapa de modelagem, ao custo de redução do entendimento e explicabilidade do modelo ao final. Desta forma, evitou-se construir features mais complexas em uma primeiro momento, para preservar a explicabilidade do modelo."
  },
  {
    "objectID": "presentations/churn/relatorio.html#o-modelo-geral",
    "href": "presentations/churn/relatorio.html#o-modelo-geral",
    "title": "Relatório técnico — Análise de churn",
    "section": "O modelo geral",
    "text": "O modelo geral\nSerá considerada para modelagem a variável dependente “cancelado” (SIM/NÃO) como resposta, sendo explicada pelas demais covariáveis selecionadas, à excessão de: id_cliente, titularidade, sexo e unidade da federação.\nid_cliente é simplesmente a identificação única de cada cliente, não havendo sentido em sua inserção para modelagem (servindo apenas para remoção de duplicatas); sexo por problemas de valores ausentes e baixo indicativo de significância na análise exploratória; titularidade pela ausência de variância; e unidade da federação pelo baixo número de instâncias para algumas unidades da federação, o que confundia o modelo — para esta, optou-se pela feature agregada Região (regiao)."
  },
  {
    "objectID": "presentations/churn/relatorio.html#pré-processamento",
    "href": "presentations/churn/relatorio.html#pré-processamento",
    "title": "Relatório técnico — Análise de churn",
    "section": "Pré processamento",
    "text": "Pré processamento\nUma camada adicional de filtragem foi adicionada nesta etapa: a remoção das instâncias cuja renda observada foi “Média”. Isto por que, conforme observado na etapa exploratória, existe a separação perfeita — nenhum cliente de renda média cancelou o plano. Isto certamente levaria a um modelo com problemas de generalização, visto que quase certamente um cliente de renda média sempre seria classificado como não cancelado, ainda que no futuro possa haver algum cliente de renda média que por ventura cancele o plano. Para além disso, esta informação certamente distorceria os parâmetros dos modelos, inclusive podendo levar modelos mais sensíveis à separação perfeita — como a regressão logística — a ter problemas de convergência. Como a remoção destas observações ainda preserva um volume de dados extremamente satisfatório, julgo ser seguro simplesmente optar por não inseri-los no modelo.\nOutras informações identificadas como potencialmente não relevantes na etapa exploratória serão mantidas nesta etapa, podendo ser removidas na comparação de modelos posteriormente.\n\n\nMostrar o código\ndf = df.drop(columns=['id_cliente', 'titularidade', 'sexo', 'uf'])\ndf['cancelado'] = df['cancelado'].apply(lambda x: 1 if x == 'SIM' else 0)\n\ndf = df[df['faixa_renda'].isin(['Baixa renda', 'Alta renda'])].copy()\n\nX = df.drop(columns=['cancelado'])\ny = df['cancelado']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=150167636, stratify=y)\n\ncategorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore',drop='first'))\n])\n\nlog_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p))\n])\n\ntempo_plano_transformer = Pipeline([\n    ('log_transform', log_transformer),\n    ('scaler', StandardScaler())\n])\n\noutras_numericas_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\nnumeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\nfeatures_para_log = ['tempo_de_plano_meses']\nfeatures_outras_numericas = [f for f in numeric_features if f not in features_para_log]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('log_tempo', tempo_plano_transformer, features_para_log),\n        ('num', outras_numericas_transformer, features_outras_numericas),\n        ('cat', categorical_transformer, categorical_features)\n    ])"
  },
  {
    "objectID": "presentations/churn/relatorio.html#definindo-modelos",
    "href": "presentations/churn/relatorio.html#definindo-modelos",
    "title": "Relatório técnico — Análise de churn",
    "section": "Definindo modelos",
    "text": "Definindo modelos\nDiversos modelos podem ser escolhidos para a tarefa de classificação binária. Naive Bayes, regressão logística, árvores de decisão, florestas aleatórias, árvores com bagging, XGBoost, Gradient Boost e demais modelos. Além disso, é crucial realizar o fine tuning dos parâmetros de cada modelo, visto que cada um deles contém diversos hiperparâmetros, e a combinação ótima em geral só pode ser encontrada por força bruta. Diversas estratégias podem ser pensadas para teste de hiperparâmetros, como um grid plano, um quadrado latino, uma busca aleatória, varreduras bayesianas, etc. Em geral, opta-se pelo método mais simples — a busca plana — e testa-se métodos mais sofisticados caso hiperparâmetros ideais não sejam encontrados, ou se houver problema de tempo de execução das buscas no grid.\n\n\nMostrar o código\nmodels = {\n    'Naive_Bayes': {\n        'model': GaussianNB(),\n        'params': {\n            'var_smoothing': np.logspace(0,-9, num=100)\n        }\n    },\n    'Logistic_Regression': {\n        'model': LogisticRegression(),\n        'params': {\n            'C': np.logspace(-4, 4, 20),\n            'penalty': ['l1', 'l2'],\n            'solver': ['liblinear']\n        }\n    },\n    'Decision_Tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'max_depth': [None, 10, 20, 30, 50],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n    },\n    'Bagging_Tree': {\n        'model': BaggingClassifier(base_estimator=DecisionTreeClassifier()),\n        'params': {\n            'n_estimators': [10, 50, 100],\n            'base_estimator__max_depth': [5, 10, 20]\n        }\n    },\n    'Random_Forest': {\n        'model': RandomForestClassifier(),\n        'params': {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [None, 10, 20, 30],\n            'min_samples_split': [2, 5, 10]\n        }\n    }\n}"
  },
  {
    "objectID": "presentations/churn/relatorio.html#validação-cruzada-ajuste-dos-modelos-e-tuning-de-hiperparâmetros",
    "href": "presentations/churn/relatorio.html#validação-cruzada-ajuste-dos-modelos-e-tuning-de-hiperparâmetros",
    "title": "Relatório técnico — Análise de churn",
    "section": "Validação cruzada, ajuste dos modelos e tuning de hiperparâmetros",
    "text": "Validação cruzada, ajuste dos modelos e tuning de hiperparâmetros\n\n\nMostrar o código\nresults = {}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=150167636)\n\nfor name, config in models.items():\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('feature_selection', VarianceThreshold()),\n        ('model', config['model'])\n    ])\n    \n    params = {f'model__{key}': value for key, value in config['params'].items()}\n    \n    search = RandomizedSearchCV(\n        pipeline,\n        param_distributions=params,\n        n_iter=20,\n        cv=cv,\n        scoring='roc_auc',\n        n_jobs=-1,\n        random_state=150167636,\n        return_train_score=True\n    )\n    \n    search.fit(X_train, y_train)\n    \n    y_pred = search.best_estimator_.predict(X_test)\n    y_proba = search.best_estimator_.predict_proba(X_test)[:, 1]\n    \n    results[name] = {\n        'search_cv': search,\n        'best_model': search.best_estimator_,\n        'best_params': search.best_params_,\n        'best_score': search.best_score_,\n        'test_metrics': {\n            'accuracy': accuracy_score(y_test, y_pred),\n            'roc_auc': roc_auc_score(y_test, y_proba),\n            'f1': f1_score(y_test, y_pred),\n            'precision': precision_score(y_test, y_pred),\n            'recall': recall_score(y_test, y_pred)\n        }\n    }\n\n\nA validação é uma etapa crucial na modelagem. Os modelos sempre buscarão melhor ajuste possível aos dados, entretanto este ajuste pode significar a compreensão perfeita dos dados que forem a eles fornecidos, porém na avaliação de um dado futuro posterior, este modelo pode incorrer em erros em função de sobreajuste (overfitting), isto é, o modelo aprende exatamente os padrões dos dados que foram fornecidos, mas é incapaz de generalizar os resultados para novas observações. Para isso, uma das estratégias é separar o conjunto de dados em treino e teste, em geral utilizando 80% dos dados para treino e 20% dos dados para teste. Desta forma, as métricas do modelo serão avaliadas no conjunto de testes, conjunto este não utilizado para ajuste do modelo, para verificar o quão bem o modelo compreende padrões para além de decorar os dados que a ele foram fornecidos.\nA validação cruzada é uma extensão desta ideia. Não iremos apenas ajustar os modelos, mas também selecionar as melhores combinações possíveis de hiperparâmetros deles. Para isso, podemos dividir o conjunto de dados em treino, teste e validação. Porém, quanto mais separações fazemos, menos dados temos para treino, e prescindir de uma boa quantidade destes pode não ser viável em algumas situações. Para isso, esta metodologia fragmenta o conjunto de treino em v segmentos (v-folds), utilizando uma parte desta para avaliação de hiperparâmetro durante o ajuste do modelo, e as 4 partes restantes para teste. Além disso, a implementação embaralha e troca a ordem desses segmentos a cada ajuste, garantindo a confiabilidade das métricas produzidas.\nDesta forma, podemos avaliar o modelo, testar conjuntos de hiperparâmetros e testar a performance do modelo utilizando uma abordagem segura, confiável e crível.\n\n\nMostrar o código\nmetrics = []\nfor name, result in results.items():\n    y_pred = result['best_model'].predict(X_test)\n    y_proba = result['best_model'].predict_proba(X_test)[:, 1]\n    \n    metrics.append({\n        'Modelo': name,\n        'Acurácia': accuracy_score(y_test, y_pred),\n        'ROC AUC': roc_auc_score(y_test, y_proba),\n        'F1-Score': f1_score(y_test, y_pred),\n        'Precisão': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'Especificidade': recall_score(y_test, y_pred, pos_label=0)\n    })\n\nmetrics_df = pd.DataFrame(metrics).sort_values(['Acurácia', 'ROC AUC'], ascending=False)\n\nmetrics_df['Acurácia'] = metrics_df['Acurácia'].map('{:.2%}'.format)\nmetrics_df['ROC AUC'] = metrics_df['ROC AUC'].map('{:.3f}'.format)\nmetrics_df['F1-Score'] = metrics_df['F1-Score'].map('{:.3f}'.format)\nmetrics_df['Precisão'] = metrics_df['Precisão'].map('{:.2%}'.format)\nmetrics_df['Recall'] = metrics_df['Recall'].map('{:.2%}'.format)\nmetrics_df['Especificidade'] = metrics_df['Especificidade'].map('{:.2%}'.format)\n\nmetrics_df\n\n\n\n\n\n\n\n\n\n\nModelo\nAcurácia\nROC AUC\nF1-Score\nPrecisão\nRecall\nEspecificidade\n\n\n\n\n3\nBagging_Tree\n94.74%\n0.987\n0.966\n96.92%\n96.32%\n89.10%\n\n\n4\nRandom_Forest\n94.67%\n0.987\n0.966\n97.35%\n95.78%\n90.70%\n\n\n2\nDecision_Tree\n94.32%\n0.982\n0.964\n96.67%\n96.03%\n88.23%\n\n\n1\nLogistic_Regression\n93.96%\n0.985\n0.961\n96.32%\n95.94%\n86.93%\n\n\n0\nNaive_Bayes\n93.25%\n0.979\n0.956\n96.90%\n94.37%\n89.23%\n\n\n\n\n\n\n\n\nO Quadro acima mostra as principais métricas associadas aos modelos testados. Nota-se que todos os modelos performaram relativamente bem em todas as métricas selecionadas, com destaque para a área sob a curva (ROC AUC). Isto indica que qualquer um dos modelos poderia ser utilizado para previsão, com F1-Score acima de 95%, que em geral é a métrica mais importante para avaliação de churn.\n\n\nMostrar o código\ndef plot_search_results(results):\n    n_models = len(results)\n    fig, axes = plt.subplots(n_models, 1, figsize=(8, 4 * n_models))\n    \n    if n_models == 1:\n        axes = [axes]\n    \n    for (name, result), ax in zip(results.items(), axes):\n        search_cv = result['search_cv']\n        cv_results = search_cv.cv_results_\n        \n        scores = cv_results['mean_test_score']\n        ax.scatter(range(len(scores)), scores, alpha=0.6)\n        ax.set_title(f'Performance do {name}', pad=20)\n        ax.set_xlabel('Combinação de hiperparâmetros')\n        ax.set_ylabel('ROC AUC Score')\n        ax.axhline(result['best_score'], linestyle='--', color='red')\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout(pad=3.0)\n    plt.show()\n\nplot_search_results(results)\n\n\n\n\n\n\n\n\n\nOs gráficos acima demonstram a necessidade de realizar o tuning de hiperparâmetros. Notamos que a performance de todos os modelos oscilam a depender da combinação de hiperparâmetros utilizada. Estaremos especialmente interessados nos modelos cuja combinação de hiperparâmetros foi a melhor possível, segundo alguma métrica selecionada."
  },
  {
    "objectID": "presentations/churn/relatorio.html#análise-de-importâncias-dos-melhores-modelos",
    "href": "presentations/churn/relatorio.html#análise-de-importâncias-dos-melhores-modelos",
    "title": "Relatório técnico — Análise de churn",
    "section": "Análise de importâncias dos melhores modelos",
    "text": "Análise de importâncias dos melhores modelos\n\n\nMostrar o código\ndef get_feature_names_from_column_transformer(column_transformer):\n    output_features = []\n\n    for name, trans, cols in column_transformer.transformers_:\n        if name == 'remainder':\n            continue\n\n        if isinstance(trans, Pipeline):\n            try:\n                if hasattr(trans[-1], 'get_feature_names_out'):\n                    names = trans[-1].get_feature_names_out(cols)\n                else:\n                    names = cols\n            except:\n                names = cols\n        else:\n            try:\n                if hasattr(trans, 'get_feature_names_out'):\n                    names = trans.get_feature_names_out(cols)\n                else:\n                    names = cols\n            except:\n                names = cols\n\n        output_features.extend(names)\n\n    return output_features\n\nrf_model = results['Random_Forest']['best_model']\nfeature_names = get_feature_names_from_column_transformer(rf_model.named_steps['preprocessor'])\nimportances = rf_model.named_steps['model'].feature_importances_\n\nrf_importances = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values(by='importance', ascending=False).head(15)\n\nbag_model = results['Bagging_Tree']['best_model']\nfeature_names_bag = get_feature_names_from_column_transformer(bag_model.named_steps['preprocessor'])\nbase_estimators = bag_model.named_steps['model'].estimators_\nimportances_list = [tree.feature_importances_ for tree in base_estimators]\navg_importances = np.mean(importances_list, axis=0)\n\nbag_importances = pd.DataFrame({\n    'feature': feature_names_bag,\n    'importance': avg_importances\n}).sort_values(by='importance', ascending=False).head(15)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), sharey=True)\n\nsns.barplot(data=rf_importances, x='importance', y='feature', palette='viridis', ax=ax1)\nax1.set_title(\"Random Forest\")\n\nsns.barplot(data=bag_importances, x='importance', y='feature', palette='crest', ax=ax2)\nax2.set_title(\"Bagging Tree\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nA Figura acima mostra as features mais importantes nos modelos que melhor perfomaram — bagging tree e random forest. Da Figura, podemos inferir que o fator mais importante para explicar o churn de clientes é o tempo de permanência no plano, em que clientes mais antigos são muito menos propensos a cancelar. Além disso, pode-se dizer que a renda também é característica importante, em que clientes de baixa renda são mais propensos a cancelar o plano em relação a categoria de referência (clientes de alta renda). Outros fatores foram menos relevantes, mas ainda podem ser considerados — como a inadimplência. Com qualquer um destes modelos, estamos munidos de poderosa ferramenta para prever o churn de um cliente, bem como ter alguma noção dos fatores mais importantes na determinação deste cenário. Em especial o modelo de random forest que, por ter obtido as melhores métricas ante aos outros testados, será o modelo escolhido e utilizado para previsão.\n\nProblemáticas desta abordagem\nEm geral estamos diante de um dos seguintes cenários no contexto da ciência de dados: previsão ou explicabilidade de fatores para áreas não técnicas. Busca-se dentre as opções o modelo com melhores métricas preditivas, como os modelos apresentados anteriormente. Porém, estes modelos costumam perder explicabilidade proporcionalmente com a sua capacidade preditiva. E isto pode ser um problema, pois dificulta a comunicação dos resultados, e a elaboração de ações para reversão de cenários apresentados pelo modelo. No caso concreto, temos interesse na reversão da alta taxa de churn, e queremos saber que tipo de atitudes e ações podemos adotar para a reversão deste cenário, bem como, se possível, metrificar o impacto de cada um dos fatores para traçarmos as principais estratégias, utilizando por exemplo uma matriz SWOT. Estes modelos indicam parcialmente a direção dos esforços, mas a metrificação exata dessa importância mostra-se bastante complicada. Além disso, foram ignoradas algumas conclusões parciais da etapa de análise exploratória, como a inclusão de covariáveis que aparentemente não eram significativamente diferentes entre os grupos que cancelaram e que não cancelaram o plano. Nota-se sobretudo o domínio da covariável relativa ao tempo de plano do cliente, que certamente é significativa, mas não é o único fator que pode ser acompanhado para elaboração de ações estratégicas.\n\n\nSolução\nDesta forma, uma abordagem que pode e deve ser adotada é a construção de um modelo auxiliar. Os modelos anteriores são excelentes para previsão, e poderão ser utilizados para este propósito. Entretanto, a construção de um modelo com maior parcimônia e lastro com a análise exploratória se faz necessária para extrair interpretação direta de coeficientes do modelo, tornando possível uma comunicação mais assertiva com a área administrativa sobre os impactos e a magnitude destes na análise de churn, possibilitando assim a formulação de estratégias eficientes para reversão deste cenário."
  },
  {
    "objectID": "presentations/churn/relatorio.html#o-modelo-interpretável",
    "href": "presentations/churn/relatorio.html#o-modelo-interpretável",
    "title": "Relatório técnico — Análise de churn",
    "section": "O modelo interpretável",
    "text": "O modelo interpretável\nPara a tarefa de classificação binária, um modelo especialmente interessante para interpretação de coeficientes é a regressão logística. Isto por que, por utilizar uma função de ligação logito e ter pressuposto de linearidade nos parâmetros, caso se ajuste bem, pode ser interpretado de seus coeficientes o aumento ou diminuição da chance de cancelamento do plano. Desta forma, poderemos quantificar não só quais fatores são mais importantes, mas o quão mais importantes são no cálculo dessa chance.\nPara obter resultados coerentes, camadas adicionais de pré processamento serão realizadas para ajuste deste modelo. A regressão logística apresenta graves problemas em casos de separação perfeita dos dados, por conta da função logito. Nestes casos, os coeficientes “explodem” e perdem a interpretabilidade, ainda que o modelo esteja com métricas de previsão boas. Como a intenção deste modelo é a interpretação dos parâmetros (se preservadas métricas de ajuste aceitáveis), etapas adicionais de engenharia de features são necessárias, e serão descritas a seguir:\n\nA covariável tempo de plano em meses será categorizada, transformada em 4 categorias segundo seus quantis. Isto irá evitar a separação perfeita dos dados;\nComo revelado na etapa exploratória, as covariáveis renda e inadimplência são melhor interpretadas se analisadas conjuntamente. Desta forma, será criada uma nova variável, que é a combinação entre os fatores destas duas, para melhor demonstrar este efeito;\nDemais covariáveis indicadas como pouco significativas na etapa de análise exploratória serão desconsideradas.\n\nDesta forma, iremos ajustar o modelo explicando o cancelamento baseado em categorias da nova feature tempo de plano, inadimplência combinada com renda e região (agrupamento de unidades da federação segundo classificação do IBGE).\n\n\nMostrar o código\ntempo_plano_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('binning', KBinsDiscretizer(n_bins=4, encode='onehot-dense', strategy='quantile')),\n    ('remove_first', FunctionTransformer(lambda x: x[:, 1:]))\n])\n\ndef create_interactions(X):\n    X = X.copy()\n    X['faixa_renda'] = X['faixa_renda'].map({'Alta renda': 0, 'Baixa renda': 1})\n    X['inadimplente'] = X['inadimplente'].map({'NÃO': 0, 'SIM': 1})\n    \n    interactions = pd.DataFrame({\n        'alta_nao': (X['faixa_renda'] == 0) & (X['inadimplente'] == 0),\n        'alta_sim': (X['faixa_renda'] == 0) & (X['inadimplente'] == 1),\n        'baixa_nao': (X['faixa_renda'] == 1) & (X['inadimplente'] == 0),\n        'baixa_sim': (X['faixa_renda'] == 1) & (X['inadimplente'] == 1)\n    }).astype(int)\n    \n    return interactions[['alta_sim', 'baixa_nao', 'baixa_sim']].values\n\ninteraction_transformer = Pipeline([\n    ('interaction', FunctionTransformer(create_interactions))\n])\n\nregiao_transformer = Pipeline([\n    ('onehot', OneHotEncoder(drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('tempo_plano', tempo_plano_transformer, ['tempo_de_plano_meses']),\n        ('interaction', interaction_transformer, ['faixa_renda', 'inadimplente']),\n        ('regiao', regiao_transformer, ['regiao'])\n    ],\n    remainder='drop'\n)\n\nmodel = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(penalty='l2', solver='liblinear'))\n])\n\nmodel.fit(X_train, y_train)\n\nfeature_names = []\n\nfeature_names.extend([f'tempo_plano_bin_{i}' for i in range(1, 4)])\n\nfeature_names.extend([\n    'renda_alta_inadimplente_sim',\n    'renda_baixa_inadimplente_nao', \n    'renda_baixa_inadimplente_sim'\n])\n\nregiao_ohe = model.named_steps['preprocessor'].named_transformers_['regiao'].named_steps['onehot']\nfeature_names.extend(regiao_ohe.get_feature_names_out(['regiao']))\n\nassert len(feature_names) == len(model.named_steps['classifier'].coef_[0])\n\n\n\nMétricas do modelo interpretável\n\n\nMostrar o código\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n\nmetrics = {\n    \"Métrica\": [\"Acurácia\", \"Precisão\", \"Sensibilidade (Recall)\", \"Especificidade\", \"F1-Score\"],\n    \"Valor\": [\n        f\"{accuracy_score(y_test, y_pred):.1%}\",\n        f\"{precision_score(y_test, y_pred):.1%}\",\n        f\"{recall_score(y_test, y_pred):.1%}\",\n        f\"{recall_score(y_test, y_pred, pos_label=0):.1%}\",\n        f\"{f1_score(y_test, y_pred):.3f}\"\n    ]\n}\n\npd.DataFrame(metrics)\n\n\n\n\n\n\n\n\n\n\nMétrica\nValor\n\n\n\n\n0\nAcurácia\n93.2%\n\n\n1\nPrecisão\n97.2%\n\n\n2\nSensibilidade (Recall)\n94.1%\n\n\n3\nEspecificidade\n90.2%\n\n\n4\nF1-Score\n0.956\n\n\n\n\n\n\n\n\n\n\nMatriz de confusão do modelo interpretável\n\n\nMostrar o código\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Não Cancelou\", \"Cancelou\"])\ndisp.plot(cmap='Blues', values_format='d')\nplt.title(\"Matriz de Confusão\")\nplt.grid(False)\nplt.show()\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCurva ROC do modelo interpretável\n\n\nMostrar o código\nplt.figure(figsize=(8, 6))\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nauc_value = roc_auc_score(y_test, y_proba)\n\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_value:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('Taxa de Falsos Positivos')\nplt.ylabel('Taxa de Verdadeiros Positivos')\nplt.title('Curva ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nO principal objetivo deste modelo será a extração de interpretação dos parâmetros. Como ajustou-se um modelo mais parcimonioso, é esperado que suas métricas sejam inferiores ao modelo saturado ajustado anteriormente. Isto é, para previsão, os modelos anteriores são ligeiramente melhores do que o modelo ajustado nesta etapa. Isso não significa que o modelo possa ser ruim: suas métricas devem ainda ser satisfatórias.\nConforme observado na análise de diagnóstico acima, como valores das principais métricas, área sob a curva e matriz de confusão, notamos uma ligeira piora de algumas métricas em relação ao modelo saturado, em especial na precisão de classificação de verdadeiros negativos. Entretanto, observando o conjunto das métricas em geral, pode-se dizer que este modelo se ajustou bem aos dados, em especial observando as métricas de F1-Score e área sob a curva — ambas performando acima de 95% — que são essenciais neste contexto.\nDesta forma, podemos ter segurança na interpretação dos parâmetros deste modelo, e comunicação de seus resultados para as outras áreas da empresa\n\n\nMostrar o código\nlogreg = model.named_steps['classifier']\nX_trans = model.named_steps['preprocessor'].transform(X_train)\n\ntry:\n    pred_probs = logreg.predict_proba(X_trans)\n    sample_weights = pred_probs[:, 1] * (1 - pred_probs[:, 1])\n    X_design = np.hstack([np.ones((X_trans.shape[0], 1)), X_trans])\n    \n    weighted_X = sample_weights[:, np.newaxis] * X_design\n    cov_matrix = np.linalg.inv(safe_sparse_dot(X_design.T, weighted_X))\n    \nexcept Exception as e:\n    print(f\"Erro no cálculo da matriz de covariância: {e}\")\n    n_features = X_trans.shape[1]\n    cov_matrix = np.eye(n_features + 1) * 0.01\n\nse = np.sqrt(np.diag(cov_matrix))[1:] if cov_matrix is not None else np.zeros(X_trans.shape[1])\nz_scores = logreg.coef_[0] / se\np_values = stats.norm.sf(np.abs(z_scores)) * 2\n\nbin_edges = model.named_steps['preprocessor'].named_transformers_['tempo_plano'].named_steps['binning'].bin_edges_[0]\n\nfeature_map = {\n    'tempo_plano_bin_1': f\"Tempo: {bin_edges[1]:.0f}-{bin_edges[2]:.0f} meses\",\n    'tempo_plano_bin_2': f\"Tempo: {bin_edges[2]:.0f}-{bin_edges[3]:.0f} meses\",\n    'tempo_plano_bin_3': f\"Tempo: &gt;{bin_edges[3]:.0f} meses\",\n    'renda_alta_inadimplente_sim': \"Alta renda + Inadimplente\",\n    'renda_baixa_inadimplente_nao': \"Baixa renda + Não inadimplente\",\n    'renda_baixa_inadimplente_sim': \"Baixa renda + Inadimplente\",\n    'regiao_Norte': \"Região Norte\",\n    'regiao_Nordeste': \"Região Nordeste\",\n    'regiao_Sudeste': \"Região Sudeste\",\n    'regiao_Sul': \"Região Sul\"\n}\n\nresults_df = pd.DataFrame({\n    'Variável': [feature_map.get(f, f) for f in feature_names],\n    'Coeficiente': logreg.coef_[0],\n    'Erro Padrão': se,\n    'z-value': z_scores,\n    'p-value': ['&lt;0.0001' if p &lt; 0.0001 else f'{p:.4f}' for p in p_values],\n    'Odds Ratio': np.round(np.exp(logreg.coef_[0]), 4)\n})\n\nresults_df = results_df.reindex(logreg.coef_[0].argsort()[::-1]).reset_index(drop=True)\n\nprint(results_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n\n\n                      Variável  Coeficiente  Erro Padrão  z-value p-value  Odds Ratio\n    Baixa renda + Inadimplente       4.8816       0.0801  60.9565 &lt;0.0001    131.8379\nBaixa renda + Não inadimplente       2.9060       0.0608  47.7791 &lt;0.0001     18.2839\n          Tempo: 205-253 meses       1.9264       0.7675   2.5101  0.0121      6.8648\n     Alta renda + Inadimplente       1.7975       0.1147  15.6764 &lt;0.0001      6.0344\n                    Região Sul       0.7726       0.0863   8.9543 &lt;0.0001      2.1654\n                Região Sudeste       0.5548       0.0656   8.4537 &lt;0.0001      1.7416\n                  Região Norte       0.2458       0.1386   1.7728  0.0763      1.2786\n               Região Nordeste      -0.1179       0.0669  -1.7632  0.0779      0.8888\n          Tempo: 253-291 meses      -4.6209       0.2655 -17.4069 &lt;0.0001      0.0098\n             Tempo: &gt;291 meses      -7.9566       0.2653 -29.9950 &lt;0.0001      0.0004\n\n\n\n\nResultados do modelo e interpretação\nA utilização do p-valor inferior a 0,05 é uma convenção útil na estatística, especialmente para estudos com amostras pequenas. Neste caso, visto a grande massa de dados, será considerado significante apenas coeficientes com p-valor inferior a 0,0001 — critério mais apropriado para este volume de dados –, isto é, significantes sob qualquer nível de confiança.\nFixada a categoria de referência: Clientes de alta renda não inadimplentes, moradores do centro-oeste e com tempo de plano até 205 meses, podemos ver como a alteração de um destes fatores — mantidos os restantes inalterados — impacta na chance de cancelamento do plano.\nDos fatores que aumentam a chance de cancelamento, destaca-se que clientes de baixa renda e inadimplentes apresentam chance de cancelamento 130 vezes superior em relação a categoria de referência. Recomenda-se prioridade para este recorte nas ações estratégicas para permanência no plano. Clientes de baixa renda não inadimplentes também apresentam chance de cancelamento 18 vezes superior aos clientes da categoria de referência. Isto indica que uma ação com vista na permanência para clientes de baixa renda, independente de inadimplência, pode ser crucial para reversão da alta taxa de churn. Ainda assim, a inadimplência é um fator relevante, pois clientes de alta renda inadimplentes apresentam uma chance superior a 6 vezes à chance de cancelamento do plano de clientes de alta renda adimplentes, o que indica que a reversão da inadimplência pode ser importante para permanência de clientes com este recorte de renda. Quanto a diferenças regionais, clientes das regiões Sul apresentam o dobro de chance de cancelamento — e 75% maior chance para clientes da região Sudeste —, indicando que políticas regionalizadas podem ser relevantes para reversão da alta taxa de churn nesas regiões.\nO fator que diminui a chance de cancelamento é o tempo de permanência no plano. Clientes com permanência no plano entre 253 e 291 meses apresentam menos de 1% da chance de cancelamento dos clientes da categoria de referência, enquanto clientes com tempo de permanência superior a 291 meses apresentam chance de cancelamento de apenas 0,03% em relação aos clientes da categoria de referência, indicando que ações de permanência no plano por mais tempo podem levar a um cenário em que menos clientes cancelem o plano por já serem clientes muito antigos."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bruno Gondim",
    "section": "",
    "text": "Statistician — University of Brasília (UnB)\nHere are personal applications of machine learning, data science and statistics. I’m also interested in programming, data structure and modeling. I believe in a more equal and fair world."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Bruno Gondim",
    "section": "Education",
    "text": "Education\nUniversity of Brasília (UnB) | Federal District, Brazil | BSc in Statistics | Dez 2015 – Feb 2025\nUniversity of Brasília (UnB) | Distrito Federal, Brazil | MSc in Statistics | Mar 2025 - Mar 2027"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bruno Gondim",
    "section": "Experience",
    "text": "Experience\nProject manager at ESTAT - Jan 2022 - Jan 2023\nSTF Intern | Data analysis and statistics center (NUADE) | July 2023 – Feb 2025"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#introdução",
    "href": "presentations/birnbaum/seminario.html#introdução",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Introdução",
    "text": "Introdução\n\n\n\n\nA distribuição Birnbaum-Saunders (Birnbaum and Saunders, 1969) é utilizada para descrever tempo de vida por fadiga (exposição cumulativa a danos, vibrações, etc) de materiais expostos à ciclos homogêneos. O processo de fadiga descrito pelos autores é:\n\nUm material sujeito a ciclo de cargas repetitivos, que produzem desgaste no material;\nA falha no material dar-se-á quando a magnitude do desgaste excede um limite \\(\\omega\\);\nOs ciclos são homogêneos (a sequência de estresse aplicada ao material é sempre a mesma);\nA extensão da fissura (\\(X_i\\)) causada pela carga \\(l_i\\) é uma variável aleatória com dependência nas cargas e fissuras acumuladas até o i-ésimo ciclo;\nO tamanho total da fissura causados no j-ésimo ciclo (\\(Y_j\\)) também é uma variável aleatória com média \\(\\mu\\) e variância \\(\\sigma^2 &lt; \\infty\\);\nO tamanho das fissuras \\(Y_j\\) em diferentes ciclos são independentes."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#a-distribuição-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#a-distribuição-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "A distribuição Birnbaum-Saunders",
    "text": "A distribuição Birnbaum-Saunders\nPara este problema, os autores definem que: Seja T o tempo total até a ocorrência da falha e cumprida as condições de regularidade, podemos dizer que T segue distribuição Birnbaum-Saunders (BS), tal que \\(T \\sim BS(\\alpha,\\beta)\\), e \\[T = \\beta\\biggl(\\frac{\\alpha}{2}Z+\\sqrt{(\\frac{\\alpha}{2}Z)^2+1} \\biggl),\\] onde \\(\\alpha\\) é um parâmetro de forma, \\(\\beta\\) é parâmetro de localização (mediana), e \\(Z\\) é uma variável aleatória com distribuição normal padrão.\nEm uma aplicação, os parâmetros \\(\\alpha\\) e \\(\\beta\\) devem ser estimados."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#reparametrização-da-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#reparametrização-da-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Reparametrização da Birnbaum-Saunders",
    "text": "Reparametrização da Birnbaum-Saunders\nEm geral, iremos trabalhar com uma reparametrização da distribuição Birnbaum-Saunders, especialmente para regressão. Rieck e Nedelman (1991) mostram a relação da distribuição Birnbaum-Saunders com a distribuição seno hiperbólico normal.\nSeja \\(T \\sim BS(\\alpha,\\beta)\\), então \\(Y \\sim SHN(\\alpha,log(\\beta),\\sigma=2)\\), onde \\(\\alpha\\) é parâmetro de forma, \\(\\beta\\) é parâmetro de localização, e \\(\\sigma\\) é parâmetro de de escala fixo.\nDesta forma, a distribuição seno hiperbólica normal pode também ser chamada de distribuição log-Birnbaum-Saunders (LBS)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#regressão-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#regressão-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Regressão Birnbaum-Saunders",
    "text": "Regressão Birnbaum-Saunders\nAssumindo \\(T_i\\) variável resposta, podemos construir um modelo de regressão Birbaum-Saunders, tal que: \\[T_i = \\beta_i\\varphi_i = exp(\\mu_i)\\varphi_i=exp(\\mathbf{x}_i^T\\mathbf{\\eta})\\varphi_i, i = 1,2,...,n.\\] onde \\(T_i\\) e \\(\\beta_i = exp(\\mu_i)\\) são a variável resposta e a mediana para a i-ésima observação; \\(\\mathbf{\\eta} = (\\eta_0,\\eta_1,...,\\eta_p)^T\\) é o vetor de parâmetros desconhecidos a serem estimados pela regressão, \\(\\mathbf{x}_i^T=(1,x_{i1},...,x_{ip})\\) são os valores das \\(p\\) variáveis explicativas, e \\(\\varphi_i \\sim BS(\\alpha,1)\\) é o erro do modelo, tal que \\(T_i \\sim BS(\\alpha,\\beta)\\)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#função-de-verossimilhança",
    "href": "presentations/birnbaum/seminario.html#função-de-verossimilhança",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Função de verossimilhança",
    "text": "Função de verossimilhança\nPara construir a função de verossimilhança, será necessário reparametrizar a distribuição \\(T_i \\sim BS(\\alpha,\\beta_i)\\) para \\(Y_i = log(T_i) \\sim LBS(\\alpha,log(\\beta_i))\\), tal que \\[log(T_i) = Y_i = \\mu_i + \\epsilon_i = \\mathbf{x}_i^T\\mathbf{\\eta}+\\epsilon_i, i=1,...,n.\\]\nDesta forma, a função de verossimilhança para \\(\\mathbf{\\theta}=(\\alpha,\\mathbf{\\eta}^T)^T\\) não terá solução analítica, sendo necessário estimar o vetor de parâmetros \\(\\mathbf{\\eta}\\) utilizando métodos iterativos de otimização não linear.\nDesta estimativa obtida, podemos obter a estimativa de máxima verossimilhança do parâmetro \\(\\alpha\\), tal que: \\[\\hat{\\alpha} = \\sqrt{\\frac{4}{n}\\sum_{i=1}^nsinh^2\\biggl(\\frac{y_i-\\mathbf{x}_i^T\\mathbf{\\hat{\\eta}}}{2}\\biggl)}.\\]"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#estimador-de-mínimos-quadrados",
    "href": "presentations/birnbaum/seminario.html#estimador-de-mínimos-quadrados",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Estimador de mínimos quadrados",
    "text": "Estimador de mínimos quadrados\nPodemos também utilizar o estimador de mínimos quadrados para \\(\\mathbf{\\eta}\\), da forma \\[\\hat{\\mathbf{\\eta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\\] onde \\(\\mathbf{y}\\) é o vetor de observações, e \\(\\mathbf{X}\\) é a matriz de desenho do modelo. Entretanto, este método é menos eficiente que o estimador de máxima verossimilhança, conforme observado por Rieck e Nedelman (1991)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#distribuição-bs-reparametrizada-pela-média",
    "href": "presentations/birnbaum/seminario.html#distribuição-bs-reparametrizada-pela-média",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Distribuição BS reparametrizada pela média",
    "text": "Distribuição BS reparametrizada pela média\nSantos-Neto et al. (2012) propuseram uma reparametrização da distribuição Birnbaum-Saunders, sendo: \\(\\alpha=\\frac{2}{\\delta},\\beta=\\frac{\\delta\\mu}{\\delta+1},\\) tal que \\(\\delta=\\frac{2}{\\alpha^2},\\mu=\\beta\\Bigl(1+\\frac{\\alpha^2}{2}\\Bigl)\\), sendo \\(\\delta&gt;0,\\mu&gt;0\\) parâmetros de forma e média, respectivamente.\nNesta parametrização, a função de densidade de probabilidade fica definida por: \\[f_Y(y|\\mu,\\delta)=\\frac{exp(\\frac{\\delta}{2}\\sqrt{\\delta+1})}{\\sqrt{16\\pi\\mu}y^{\\frac{3}{2}}}\\Bigl(y+\\frac{\\delta\\mu}{\\delta+1}\\Bigl)exp\\Biggl(-\\frac{\\delta}{4}\\Biggl[\\frac{y(\\delta+1)}{\\delta\\mu}+\\frac{\\delta\\mu}{y(\\delta+1)}\\Biggl]\\Biggl)\\]"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#regressão-bs-reparametrizada-pela-média",
    "href": "presentations/birnbaum/seminario.html#regressão-bs-reparametrizada-pela-média",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Regressão BS reparametrizada pela média",
    "text": "Regressão BS reparametrizada pela média\nSendo \\(RBS(\\mu,\\delta)\\) a reparametrização pela média da distribuição Birnbaum-Saunders original, Leiva et al. (2014) propõe um modelo de regressão em que, seja \\(Y \\sim RBS(\\mu,\\delta)\\), a média de \\(Y_i\\) satisfaz a relação: \\[h(\\mu_i)=\\mathbf{x}_i^T\\mathbf{\\beta}, i=1,...,n,\\] em que \\(\\mathbf{\\beta}=(\\beta_1,...,\\beta_p)^T,p&lt;n\\) é vetor de coeficientes de regressão a serem estimados, e \\(\\mathbf{x}_i=(x_{i1},x_{i2},...,x_{ip})^T\\) são observações de p regressores, e \\(\\mu_i=h^{-1}(\\mathbf{x}_i^T\\mathbf{\\beta}),\\) com \\(h(.)\\) sendo uma função de ligação positiva e duas vezes diferenciável.\nSob a distribuição RBS, \\(\\mathbb{E}(Y)=\\mu\\) e \\(Var(Y) = \\mu^2 CV(Y)^2\\), onde \\(CV(Y) = \\frac{\\sqrt{2\\delta+5}}{(\\delta+1)}\\in (0,\\sqrt{5})\\) é o coeficiente de variação de Y.\nA variância de \\(Y_i\\) é função de \\(\\mu_i\\), logo estaremos modelando também a variância."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#vantagens-e-desvantagens-dos-modelos-bs",
    "href": "presentations/birnbaum/seminario.html#vantagens-e-desvantagens-dos-modelos-bs",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Vantagens e desvantagens dos modelos BS",
    "text": "Vantagens e desvantagens dos modelos BS\n\n\nVantagens\n\n\nMais uma família para obter um ajuste melhor aos dados;\nPode ser utilizada para dados censurados;\nResultados do modelo têm interpretação fácil;\nImplementado no R!\n\n\n\nDesvantagens\n\n\nDistribuição pouco conhecida;\nLimitações conceituais e práticas;\nDefinição matemática complexa;\nPouco material prático disponível de forma fácil."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#aplicação",
    "href": "presentations/birnbaum/seminario.html#aplicação",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Aplicação",
    "text": "Aplicação\nUtilizarei o conjunto de dados cpd do pacote faraway, referente a projeção de vendas e o verdadeiro número de vendas de 20 produtos\n\n\n\nlibrary(tidyverse)\nlibrary(gamlss)\ndata(cpd,package='faraway')\nattach(cpd)\np1 = cpd %&gt;% \n  gather() %&gt;%\n  ggplot(aes(value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~key, scales = 'free_x') +\n  labs(x = '', y = '', title = '') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nVemos que tanto a variável resposta, actual, quanto a covariável explicativa, projected, são obviamente positivas, e também assimétricas à direta."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#primeiro-ajuste",
    "href": "presentations/birnbaum/seminario.html#primeiro-ajuste",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Primeiro ajuste",
    "text": "Primeiro ajuste\nVeremos qual distribuição o pacote gamlss recomenda para melhor ajuste aos dados\n\nmod = fitDist(actual, type=\"realplus\") \n\n\nmod$fits\n\n      IG     BCPE   IGAMMA   LOGNO2    LOGNO      GIG       GG    BCCGo \n319.7223 320.9136 320.9552 321.5667 321.5667 321.7220 322.8726 323.0453 \n    BCCG      EXP  PARETO2       GP PARETO2o      GB2      WEI     WEI2 \n323.0453 323.5004 324.1676 324.1676 324.1676 324.8727 324.9636 324.9636 \n    WEI3     BCTo       GA   exGAUS \n324.9636 325.0453 325.3356 352.5137 \n\n\nVemos que o pacote GAMLSS oferece diversas opções de ajuste para este conjunto de dados. Entretanto, estamos especialmente interessados em tentar ajustar os dados utilizando o modelo RBS"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#ajuste-rbs",
    "href": "presentations/birnbaum/seminario.html#ajuste-rbs",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Ajuste RBS",
    "text": "Ajuste RBS\nPrimeiramente, é necessário baixar e instalar o pacote RBS, que não está disponível no CRAN, utilizando o comando\n\ndevtools::install_github(\"santosneto/RBS\")\n\n\nlibrary(RBS)\n\nAgora, tentaremos o ajuste utilizando a distribuição RBS, e aproveitando da sintaxe e funcionalidades do pacote gamlss\n\n\nAjustando o modelo\n\nmodel0 = gamlss::gamlss(actual ~ projected, family=RBS(mu.link=\"identity\"),method=CG())\n\n\nsummary(model0)\n\n******************************************************************\nFamily:  c(\"RBS\", \"BirnbaumSaunders\") \n\nCall:  \ngamlss::gamlss(formula = actual ~ projected, family = RBS(mu.link = \"identity\"),  \n    method = CG()) \n\nFitting method: CG() \n\n------------------------------------------------------------------\nMu link function:  identity\nMu Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.80360   18.08128   0.155    0.879    \nprojected    1.08047    0.07339  14.722 4.16e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  log\nSigma Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.9020     0.3162   12.34 6.56e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  20 \nDegrees of Freedom for the fit:  3\n      Residual Deg. of Freedom:  17 \n                      at cycle:  8 \n \nGlobal Deviance:     247.755 \n            AIC:     253.755 \n            SBC:     256.7422 \n******************************************************************\n\n\n\nModelando também a variância\n\nmodel = gamlss::gamlss(actual ~ projected, sigma.formula = ~projected, family=RBS(mu.link=\"identity\"),method=CG())\n\n\nsummary(model)\n\n******************************************************************\nFamily:  c(\"RBS\", \"BirnbaumSaunders\") \n\nCall:  \ngamlss::gamlss(formula = actual ~ projected, sigma.formula = ~projected,  \n    family = RBS(mu.link = \"identity\"), method = CG()) \n\nFitting method: CG() \n\n------------------------------------------------------------------\nMu link function:  identity\nMu Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.81320   21.00592   1.657    0.117    \nprojected    0.95759    0.01903  50.318   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  log\nSigma Coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.1348243  0.4749559   6.600  6.1e-06 ***\nprojected   0.0010326  0.0003238   3.189   0.0057 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  20 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  16 \n                      at cycle:  20 \n \nGlobal Deviance:     241.0335 \n            AIC:     249.0335 \n            SBC:     253.0165 \n******************************************************************"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#diagnósticos",
    "href": "presentations/birnbaum/seminario.html#diagnósticos",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Diagnósticos",
    "text": "Diagnósticos\nPodemos realizar a rotina normal de análise de diagnósticos do modelo, utilizando as funcionalidades do pacote gamlss\n\n\n\nwp(model)\n\n\n\n\n\n\n\n\n\n\nplot(model)\n\n\n\n\n\n\n\n\n******************************************************************\n          Summary of the Quantile Residuals\n                           mean   =  0.2395729 \n                       variance   =  1.024173 \n               coef. of skewness  =  -0.1452511 \n               coef. of kurtosis  =  1.76144 \nFilliben correlation coefficient  =  0.9877058 \n******************************************************************"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#referências",
    "href": "presentations/birnbaum/seminario.html#referências",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Referências",
    "text": "Referências\nBirnbaum, Z. and Saunders, S. (1969). A new family of life distributions. Journal of Applied Probability, 6:319–327.\nSantos-Neto, M., Cysneiros, F., Leiva, V., and Ahmed, S. (2012). On new parameterizations of the Birnbaum-Saunders distribution. Pakistan Journal of Statistics, 28:1–26.\nSantos-Neto, M., Cysneiros, F., Leiva, V., and Barros, M. (2014). On new parameterizations of the Birnbaum-Saunders distribution and its moments, estimation and application. Revstat Statistical Journal, 12:247–272.\nLeiva, V., Santos-Neto, M., Cysneiros, F., and Barros, M. (2014). BirnbaumSaunders statistical modelling: a new approach. Statistical Modelling, 14:21–48.\nRieck, J. and Nedelman, J. (1991). A log-linear model for the Birnbaum-Saunders distribution. Technometrics, 3:51–60.\nNotas de aula do Prof. Helton Saulo. Disciplina Tópicos em Estatística 2 — Modelagem com apoio computacional. UnB, 2º/2024.\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#introdução",
    "href": "presentations/classificacao_salario/trabalho.html#introdução",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Introdução",
    "text": "Introdução\n\n\n\nNeste trabalho, buscou-se estudar um conjunto censitário sintético com diversas características demográficas, com objetivo fim de entender a relação entre covariáveis o salário — variável esta que se encontra binarizada no conjunto de dados, sendo\n\n0: Renda anual de até 50.000 dólares\n1: Renda anual acima de 50.000 dólares\n\nOs dados são públicos e podem ser acessados em Kaggle."
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#preparação",
    "href": "presentations/classificacao_salario/trabalho.html#preparação",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Preparação",
    "text": "Preparação\nOs dados disponíveis no Kaggle já haviam passado por algumas etapas de transformações, mas para o objetivo deste trabalho achei pertinente realizar mais algumas rotinas de transformações nas covariáveis\n\ndf$salary = factor(df$salary)\ndf = clean_names(df)\n\ndf$fnlwgt = df$fnlwgt/sum(df$fnlwgt)\n\ndf = df %&gt;%\n  mutate(marital_status = factor(marital_status),\n         relationship = factor(relationship),\n         race = factor(race),\n         sex = factor(sex),\n         country = factor(ifelse(country == 'United-States',country,'Others')),\n         education = case_when(\n           education %in% c(\"Bachelors\",\"Masters\",\"Assoc-acdm\",\n                            \"Assoc-voc\",\"Doctorate\",\"Prof-school\") ~ \"high_education\",\n           .default = \"low_education\"),\n         education = factor(education),\n         occupation = case_when(\n           occupation %in% c(\"Adm-clerical\", \"Exec-managerial\", \"Prof-specialty\", \"Tech-support\", \"Sales\") ~ \"white_collar\", # Ocupações que geralmente envolvem trabalho em escritório ou administrativo.\n           occupation %in% c(\"Craft-repair\", \"Farming-fishing\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Transport-moving\") ~ \"blue_collar\", # Ocupações que envolvem trabalho manual ou técnico.\n           occupation %in% c(\"Other-service\", \"Priv-house-serv\", \"Protective-serv\") ~ \"services\", # Ocupações no setor de serviços.\n           occupation %in% c(\"Armed-Forces\",\"?\") ~ \"Others\"),\n         occupation = factor(occupation),\n         civil_status = ifelse(marital_status %in% c('Never-married','Divorced','Separated', 'Widowed') | relationship %in% c('Not-in-family','Unmarried'),\"Single\",\"Non-single\"),\n         civil_status = factor(civil_status),\n         workclass = case_when(\n           workclass %in% c(\"Federal-gov\", \"Local-gov\", \"State-gov\") ~ \"government\",\n           workclass == \"Private\" ~ \"private\",\n           workclass %in% c(\"Self-emp-inc\", \"Self-emp-not-inc\") ~ \"self_employed\",\n           workclass %in% c(\"Never-worked\", \"Without-pay\") ~ \"not_working\",\n           workclass == \"?\" ~ \"unknown\"),\n         workclass = factor(workclass)\n  ) %&gt;%\n  select(-marital_status,-relationship)"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#section",
    "href": "presentations/classificacao_salario/trabalho.html#section",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "",
    "text": "Análise exploratória\n\nApós as transformações, observamos em nossos dados 12 covariáveis de aspectos demográficos dos grupos populacionais bastante heterogêneos e de tamanhos desiguais, quando separado nos grupos de salário até 50 mil dólares anuais e acima de 50 mil dólares anuais.\n\n\n\n\n\n\n\n\n&lt;50k\n&gt;50k\np.overall\n\n\n\n\n\nN=24720\nN=7841\n\n\n\nage\n34.0 [25.0;46.0]\n44.0 [36.0;51.0]\n0.000\n\n\nworkclass:\n\n\n&lt;0.001\n\n\ngovernment\n3010 (12.2%)\n1341 (17.1%)\n\n\n\nnot_working\n21 (0.08%)\n0 (0.00%)\n\n\n\nprivate\n17733 (71.7%)\n4963 (63.3%)\n\n\n\nself_employed\n2311 (9.35%)\n1346 (17.2%)\n\n\n\nunknown\n1645 (6.65%)\n191 (2.44%)\n\n\n\neducation:\n\n\n0.000\n\n\nhigh_education\n5981 (24.2%)\n4535 (57.8%)\n\n\n\nlow_education\n18739 (75.8%)\n3306 (42.2%)\n\n\n\neducation_num\n9.00 [9.00;10.0]\n12.0 [10.0;13.0]\n0.000\n\n\noccupation:\n\n\n0.000\n\n\nblue_collar\n8362 (33.8%)\n1700 (21.7%)\n\n\n\nOthers\n1660 (6.72%)\n192 (2.45%)\n\n\n\nservices\n3744 (15.1%)\n349 (4.45%)\n\n\n\nwhite_collar\n10954 (44.3%)\n5600 (71.4%)\n\n\n\nrace:\n\n\n&lt;0.001\n\n\nAmer-Indian-Eskimo\n275 (1.11%)\n36 (0.46%)\n\n\n\nAsian-Pac-Islander\n763 (3.09%)\n276 (3.52%)\n\n\n\nBlack\n2737 (11.1%)\n387 (4.94%)\n\n\n\nOther\n246 (1.00%)\n25 (0.32%)\n\n\n\nWhite\n20699 (83.7%)\n7117 (90.8%)\n\n\n\nsex:\n\n\n0.000\n\n\nFemale\n9592 (38.8%)\n1179 (15.0%)\n\n\n\nMale\n15128 (61.2%)\n6662 (85.0%)\n\n\n\ncapital_gain\n0.00 [0.00;0.00]\n0.00 [0.00;0.00]\n0.000\n\n\ncapital_loss\n0.00 [0.00;0.00]\n0.00 [0.00;0.00]\n&lt;0.001\n\n\nhours_per_week\n40.0 [35.0;40.0]\n40.0 [40.0;50.0]\n0.000\n\n\ncountry:\n\n\n&lt;0.001\n\n\nOthers\n2721 (11.0%)\n670 (8.54%)\n\n\n\nUnited-States\n21999 (89.0%)\n7171 (91.5%)\n\n\n\ncivil_status:\n\n\n0.000\n\n\nNon-single\n8357 (33.8%)\n6702 (85.5%)\n\n\n\nSingle\n16363 (66.2%)\n1139 (14.5%)"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#covariáveis-numéricas",
    "href": "presentations/classificacao_salario/trabalho.html#covariáveis-numéricas",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Covariáveis numéricas",
    "text": "Covariáveis numéricas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\neducation_num\ncapital_gain\ncapital_loss\nhours_per_week\n\n\n\n\n\nMin. :17.00\nMin. : 1.00\nMin. : 0\nMin. : 0.0\nMin. : 1.00\n\n\n\n1st Qu.:28.00\n1st Qu.: 9.00\n1st Qu.: 0\n1st Qu.: 0.0\n1st Qu.:40.00\n\n\n\nMedian :37.00\nMedian :10.00\nMedian : 0\nMedian : 0.0\nMedian :40.00\n\n\n\nMean :38.58\nMean :10.08\nMean : 1078\nMean : 87.3\nMean :40.44\n\n\n\n3rd Qu.:48.00\n3rd Qu.:12.00\n3rd Qu.: 0\n3rd Qu.: 0.0\n3rd Qu.:45.00\n\n\n\nMax. :90.00\nMax. :16.00\nMax. :99999\nMax. :4356.0\nMax. :99.00"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#covariáveis-categóricas",
    "href": "presentations/classificacao_salario/trabalho.html#covariáveis-categóricas",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Covariáveis categóricas",
    "text": "Covariáveis categóricas"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelagem",
    "href": "presentations/classificacao_salario/trabalho.html#modelagem",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelagem",
    "text": "Modelagem\nNum contexto em que temos tantas covariáveis, tantas observações e, apesar de alguns indicativos observados na análise exploratória, não é possível observar um padrão óbvio que indique o salário do indivíduo. Para isso, utilizei do recurso da modelagem com apoio computacional afim de tornar possível esta análise. Um diferencial deste trabalho é a utilização do framework tidymodels, que é bastante verborrágico e permite uma compreensão das etapas do modelo pela leitura do código, além de eficiência e praticidade"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#parâmetros-gerais",
    "href": "presentations/classificacao_salario/trabalho.html#parâmetros-gerais",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Parâmetros gerais",
    "text": "Parâmetros gerais\nIrei testar diversos modelos e fazer comparação de resultados destes, mas utilizarei a mesma “receita” para todos\n\nset.seed(150167636)\nsplit = initial_split(df, prop = .8, strata = salary)\ntrain = training(split)\ntest = testing(split)\n\ncv_folds &lt;- vfold_cv(train, \n                     v = 5, \n                     strata = salary)\n\nrecipe &lt;- recipe(salary ~ .,\n                 data = train) %&gt;% \n  update_role(fnlwgt, new_role = \"case_weight\") %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ starts_with(\"occupation\"):starts_with(\"race\") + \n                  starts_with(\"occupation\"):starts_with(\"sex\") +\n                  starts_with(\"hours_per_week\"):starts_with(\"sex\"))\n\ndados_preparados &lt;- recipe %&gt;% \n  prep() %&gt;% \n  juice()\nhead(dados_preparados)\n\n# A tibble: 6 × 38\n      age   fnlwgt education_num capital_gain capital_loss hours_per_week salary\n    &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt; \n1  0.0315  1.25e-5         1.14         0.151       -0.216        -0.0354 0     \n2  0.838   1.35e-5         1.14        -0.146       -0.216        -2.22   0     \n3 -0.0418  3.49e-5        -0.421       -0.146       -0.216        -0.0354 0     \n4 -0.775   5.48e-5         1.14        -0.146       -0.216        -0.0354 0     \n5  0.764   2.59e-5        -1.98        -0.146       -0.216        -1.98   0     \n6 -1.14    1.98e-5         1.14        -0.146       -0.216        -0.844  0     \n# ℹ 31 more variables: workclass_not_working &lt;dbl&gt;, workclass_private &lt;dbl&gt;,\n#   workclass_self_employed &lt;dbl&gt;, workclass_unknown &lt;dbl&gt;,\n#   education_low_education &lt;dbl&gt;, occupation_Others &lt;dbl&gt;,\n#   occupation_services &lt;dbl&gt;, occupation_white_collar &lt;dbl&gt;,\n#   race_Asian.Pac.Islander &lt;dbl&gt;, race_Black &lt;dbl&gt;, race_Other &lt;dbl&gt;,\n#   race_White &lt;dbl&gt;, sex_Male &lt;dbl&gt;, country_United.States &lt;dbl&gt;,\n#   civil_status_Single &lt;dbl&gt;, …"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-1-regressão-logística",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-1-regressão-logística",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 1: Regressão logística",
    "text": "Modelo 1: Regressão logística\nComo a variável resposta é binária, o primeiro modelo que podemos tentar seria o logístico\n\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(glm_spec)\n\nglm_fit &lt;- glm_wf %&gt;%\n  fit(data = train)\n\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  print(n=Inf)\n\n# A tibble: 37 × 5\n   term                                   estimate std.error statistic   p.value\n   &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                             -1.74      0.487    -3.57   3.58e-  4\n 2 age                                      0.406     0.0228   17.8    7.27e- 71\n 3 education_num                            0.844     0.0438   19.3    9.28e- 83\n 4 capital_gain                             2.23      0.0800   27.9    1.83e-171\n 5 capital_loss                             0.269     0.0165   16.3    1.15e- 59\n 6 hours_per_week                           0.335     0.0455    7.36   1.86e- 13\n 7 workclass_not_working                  -11.8     117.       -0.100  9.20e-  1\n 8 workclass_private                       -0.0700    0.0560   -1.25   2.11e-  1\n 9 workclass_self_employed                 -0.349     0.0730   -4.78   1.79e-  6\n10 workclass_unknown                       -0.555     1.80     -0.308  7.58e-  1\n11 education_low_education                  0.150     0.0759    1.98   4.77e-  2\n12 occupation_Others                        0.195     2.00      0.0972 9.23e-  1\n13 occupation_services                      0.808     0.805     1.00   3.16e-  1\n14 occupation_white_collar                  0.431     0.574     0.752  4.52e-  1\n15 race_Asian.Pac.Islander                  0.750     0.504     1.49   1.36e-  1\n16 race_Black                               0.469     0.465     1.01   3.14e-  1\n17 race_Other                               0.111     0.663     0.167  8.67e-  1\n18 race_White                               0.580     0.445     1.30   1.93e-  1\n19 sex_Male                                 0.172     0.180     0.954  3.40e-  1\n20 country_United.States                    0.259     0.0763    3.40   6.68e-  4\n21 civil_status_Single                     -2.41      0.0517  -46.6    0        \n22 occupation_Others_x_race_Asian.Pac.Is…  -1.18      1.46     -0.812  4.17e-  1\n23 occupation_Others_x_race_Black          -0.423     1.30     -0.324  7.46e-  1\n24 occupation_Others_x_race_Other           1.45      1.68      0.864  3.88e-  1\n25 occupation_Others_x_race_White           0.0267    1.21      0.0221 9.82e-  1\n26 occupation_services_x_race_Asian.Pac.…  -1.29      0.871    -1.48   1.40e-  1\n27 occupation_services_x_race_Black        -1.18      0.817    -1.44   1.49e-  1\n28 occupation_services_x_race_Other        -1.07      1.41     -0.764  4.45e-  1\n29 occupation_services_x_race_White        -1.10      0.783    -1.41   1.59e-  1\n30 occupation_white_collar_x_race_Asian.…  -0.300     0.608    -0.494  6.21e-  1\n31 occupation_white_collar_x_race_Black     0.0860    0.573     0.150  8.81e-  1\n32 occupation_white_collar_x_race_Other    -0.373     0.864    -0.432  6.66e-  1\n33 occupation_white_collar_x_race_White     0.151     0.548     0.276  7.82e-  1\n34 occupation_Others_x_sex_Male            -0.382     0.310    -1.23   2.17e-  1\n35 occupation_services_x_sex_Male           0.256     0.258     0.994  3.20e-  1\n36 occupation_white_collar_x_sex_Male       0.158     0.188     0.841  4.00e-  1\n37 sex_Male_x_hours_per_week                0.0586    0.0513    1.14   2.54e-  1"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-logístico",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-logístico",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo logístico",
    "text": "Importâncias para o modelo logístico\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- glm_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4556  649\n         1  388  920"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-2-regressão-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-2-regressão-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 2: Regressão Lasso",
    "text": "Modelo 2: Regressão Lasso\nComo observado na receita do modelo, existem 38 covariáveis nesta modelagem. Diversas abordagens podem ser utilizadas para selecionar as covariáveis de maior importância, sendo uma dessas a regressão lasso, que penaliza coeficientes e torna-os 0 em caso de insignificância.\nEste é um modelo que contém um hiperparâmetro, portanto iremos ajustar um grid para escolher o melhor possível.\n\n\n\nlasso_spec &lt;- logistic_reg(penalty = tune(),\n                           mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(lasso_spec)\n\ngrid &lt;- grid_regular(penalty(),\n                     levels = 100)\n\nplan(multisession)\nset.seed(150167636)\nlasso_res &lt;- lasso_wf %&gt;%\n  tune_grid(resamples = cv_folds,\n            grid = grid,\n            metrics = metric_set(roc_auc))\n\n\n\n\n\n\n\n\n\n\n\nPodemos ver que um menor valor de penalização é benéfico ao modelo, visto a importância relativa das covariáveis serem altas neste caso"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Estimativa dos parâmetros para o modelo Lasso",
    "text": "Estimativa dos parâmetros para o modelo Lasso\n\nbest_params = lasso_res %&gt;%\n  select_best(metric = \"roc_auc\")\n\nbest_wf = finalize_workflow(lasso_wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  print(n=Inf)\n\n# A tibble: 37 × 3\n   term                                              estimate  penalty\n   &lt;chr&gt;                                                &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                                        -1.19   0.000739\n 2 age                                                 0.394  0.000739\n 3 education_num                                       0.787  0.000739\n 4 capital_gain                                        2.11   0.000739\n 5 capital_loss                                        0.262  0.000739\n 6 hours_per_week                                      0.332  0.000739\n 7 workclass_not_working                              -1.91   0.000739\n 8 workclass_private                                  -0.0252 0.000739\n 9 workclass_self_employed                            -0.279  0.000739\n10 workclass_unknown                                  -0.148  0.000739\n11 education_low_education                             0.0470 0.000739\n12 occupation_Others                                  -0.117  0.000739\n13 occupation_services                                -0.0488 0.000739\n14 occupation_white_collar                             0.327  0.000739\n15 race_Asian.Pac.Islander                             0.0246 0.000739\n16 race_Black                                          0      0.000739\n17 race_Other                                         -0.208  0.000739\n18 race_White                                          0.0682 0.000739\n19 sex_Male                                            0.189  0.000739\n20 country_United.States                               0.227  0.000739\n21 civil_status_Single                                -2.38   0.000739\n22 occupation_Others_x_race_Asian.Pac.Islander        -0.560  0.000739\n23 occupation_Others_x_race_Black                     -0.186  0.000739\n24 occupation_Others_x_race_Other                      0.582  0.000739\n25 occupation_Others_x_race_White                      0      0.000739\n26 occupation_services_x_race_Asian.Pac.Islander       0      0.000739\n27 occupation_services_x_race_Black                   -0.0568 0.000739\n28 occupation_services_x_race_Other                    0      0.000739\n29 occupation_services_x_race_White                    0      0.000739\n30 occupation_white_collar_x_race_Asian.Pac.Islander   0      0.000739\n31 occupation_white_collar_x_race_Black                0.130  0.000739\n32 occupation_white_collar_x_race_Other               -0.265  0.000739\n33 occupation_white_collar_x_race_White                0.270  0.000739\n34 occupation_Others_x_sex_Male                       -0.362  0.000739\n35 occupation_services_x_sex_Male                      0      0.000739\n36 occupation_white_collar_x_sex_Male                  0.142  0.000739\n37 sex_Male_x_hours_per_week                           0.0511 0.000739"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo Lasso",
    "text": "Importâncias para o modelo Lasso\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4571  652\n         1  373  917"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-3-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-3-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 3: XGBoost",
    "text": "Modelo 3: XGBoost\nO XGBoost é um modelo de Gradient boosting baseado em árvores, que costuma performar bem em tarefas como esta, de classificação com diversas covariáveis\nEste é um modelo de Boosting, ou seja, o “encaixe” de diversos modelos fracos afim da construção de um modelo robusto a partir da combinação destes resultados.\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#ajuste-do-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#ajuste-do-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Ajuste do XGBoost",
    "text": "Ajuste do XGBoost\nTambém iremos realizar o fine tuning de alguns hiperparâmetros deste modelo, no caso o número de árvores e a profundidade destas, afim de obter o melhor modelo.\n\n\n\nmodel = boost_tree(mode = \"classification\",\n                   trees = tune(),\n                   tree_depth = tune()\n                   ) %&gt;%\n  set_engine(\"xgboost\")\n\nwf = workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(model)\n\ngrid = wf %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 3)\n\nplan(multisession)\nset.seed(150167636)\ntune_res = tune_grid( \n  wf,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(accuracy, roc_auc, sens,spec)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  trees tree_depth .config             \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1  2000          1 Preprocessor1_Model3\n\n\nVemos que a melhor combinação de hiperparâmetros encontrada é utilizando 2000 árvores de tamanho 1"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o XGBoost",
    "text": "Importâncias para o XGBoost\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4636  584\n         1  308  985"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#comparando-desempenho-dos-modelos",
    "href": "presentations/classificacao_salario/trabalho.html#comparando-desempenho-dos-modelos",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Comparando desempenho dos modelos",
    "text": "Comparando desempenho dos modelos\n\n\nModelo logístico\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8407800\n\n\nspecificity\n0.5863607\n\n\nsensitivity\n0.9215210\n\n\n\n\n\n\nLasso\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8426224\n\n\nspecificity\n0.5844487\n\n\nsensitivity\n0.9245550\n\n\n\n\n\n\nXGBoost\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8630431\n\n\nspecificity\n0.6277884\n\n\nsensitivity\n0.9377023\n\n\n\n\n\n\n\nO maior desafio para estes dados era capturar a especificidade (salário &gt;50k). Vemos que todos os modelos tiveram dificuldade com esta métrica, porém houve um ganho sensível do modelo de árvore em relação aos modelos baseados em regressão."
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#referências",
    "href": "presentations/classificacao_salario/trabalho.html#referências",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Referências",
    "text": "Referências\nFonte dos dados.\nhttps://www.tidymodels.org\nDocumentação XGBoost.\nPinheiro, João Manoel Herrera. Um estudo sobre Algoritmos de Boosting e a Otimização de Hiperparâmetros Utilizando Optuna. São Carlos, SP. 2023.\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html",
    "href": "presentations/classificação binária de decisões/trabalho_final.html",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "",
    "text": "Mostrar o código\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(MASS,tidyverse, readxl, janitor,xgboost,tidymodels,vip,rpart.plot,\n               conflicted,tictoc,finetune,doParallel,DataExplorer,future,\n               gridExtra,compareGroups,DT,naivebayes,discrim,baguette,progressr,\n               corrplot,skimr,GGally,glmnet,class,themis,kknn,gt,ranger,\n               kernlab,qgraph)\n\nconflicts_prefer(dplyr::filter)\ntidymodels_prefer()\n\nknitr::opts_chunk$set(echo = TRUE)\nMostrar o código\ndf &lt;- read_excel(\"../dados/decisoes_24.xlsx\", na = c(\"-\",NA))\nPrimeiramente, os dados necessitam de algumas etapas de engenharia de features, afim de obter variáveis mais adequadas para modelagem, eliminando também fatores redundantes, codificando de forma melhorada para visualização gráfica e binarização do tipo de decisão.\nMostrar o código\ndf = clean_names(df)\n\ndf &lt;- df %&gt;%\n  mutate(favoravel_reu = case_when(\n    andamento_decisao %in% c(\"Agravo de instrumento provido\", \"Agravo provido e desde logo provido o RE\", \n                             \"Agravo provido e desde logo provido parcialmente o RE\", \"Agravo provido e determinada a devolução pelo regime da repercussão geral\", \n                             \"Concedida a ordem\", \"Concedida a ordem de ofício\", \"Concedida a segurança\", \"Concedida a suspensão\", \n                             \"Concedida em parte a ordem\", \"Concedida em parte a segurança\", \"Conhecido e provido\", \n                             \"Conhecido e provido em parte\", \"Homologado o acordo\", \"Procedente\", \"Procedente em parte\", \n                             \"Provido\", \"Provido em parte\", \"Revogada a prisão\") ~ '1',\n    andamento_decisao %in% c(\"Agravo não provido\", \"Agravo provido e desde logo negado seguimento ao RE\", \n                             \"Conhecido e negado provimento\", \"Conhecido em parte e nessa parte negado provimento\", \n                             \"Denegada a ordem\", \"Denegada a segurança\", \"Denegada a suspensão\", \n                             \"Determinado arquivamento\", \"Extinto o processo\", \"Improcedente\", \n                             \"Não provido\", \"Negado seguimento\", \"Negado seguimento por ausência de preliminar, art. 327 do RISTF\", \n                             \"Rejeitada a denúncia\", \"Rejeitada a queixa\", \"Rejeitados\") ~ '0',\n    TRUE ~ \"outros\"\n  )) %&gt;%\n  dplyr::filter(favoravel_reu %in% c('0','1')) %&gt;%\n  mutate(ramo_direito = str_extract(ramo_direito, \"^[^|]+\") %&gt;% str_trim()) %&gt;%\n  select(classe,nome_ministro_a,indicador_virtual,origem_da_decisao,ramo_direito,favoravel_reu)%&gt;%\n  mutate(across(everything(), as.factor)) %&gt;%\n  mutate(indicador_virtual = factor(ifelse(indicador_virtual == \"MONOCRÁTICA\", \"Monocrática\", \"Não monocrática\"))) %&gt;%\n  select(-origem_da_decisao) %&gt;%\n  rename(tipo = indicador_virtual,\n         ministro = nome_ministro_a)\n\n# Análise descritiva ----\ndata = df %&gt;% mutate(favoravel_reu = factor(ifelse(favoravel_reu == 0,\"Decisão desfavorável ao réu\",\"Decisão favorável ao réu\"))) %&gt;% as.data.frame()\n\nlevels(data$ramo_direito) &lt;- c(\"Educação\", # \"DIREITO À EDUCAÇÃO\"  \n                               \"Adm Público\", # \"DIREITO ADMINISTRATIVO E OUTRAS MATÉRIAS DE DIREITO PÚBLICO\"\n                               \"Ambiental\", # \"DIREITO AMBIENTAL\"\n                               \"Assistencial\", # \"DIREITO ASSISTENCIAL\" \n                               \"Civil\", # \"DIREITO CIVIL\"\n                               \"Criança e Adolescente\", # \"DIREITO DA CRIANÇA E DO ADOLESCENTE\"\n                               \"Saúde\", # \"DIREITO DA SAÚDE\"\n                               \"Consumidor\", # \"DIREITO DO CONSUMIDOR\"\n                               \"Trabalho\", # \"DIREITO DO TRABALHO\"\n                               \"Eleitoral\", # \"DIREITO ELEITORAL\"\n                               \"Eleitoral STF\", # \"DIREITO ELEITORAL E PROCESSO ELEITORAL DO STF\"\n                               \"Internacional\", # \"DIREITO INTERNACIONAL\"\n                               \"Marítimo\", # \"DIREITO MARÍTIMO\"\n                               \"Penal\", # \"DIREITO PENAL\"\n                               \"Penal Militar\", #\"DIREITO PENAL MILITAR\"\n                               \"Previdenciário\", # \"DIREITO PREVIDENCIÁRIO\"\n                               \"Processual Civil e Trabalho\", # \"DIREITO PROCESSUAL CIVIL E DO TRABALHO\"\n                               \"Processual Penal\", # \"DIREITO PROCESSUAL PENAL\"\n                               \"Processual Penal Militar\", # \"DIREITO PROCESSUAL PENAL MILITAR\"\n                               \"Tributário\", # \"DIREITO TRIBUTÁRIO\"\n                               \"Alta Complexidade\", # \"QUESTÕES DE ALTA COMPLEXIDADE, GRANDE IMPACTO E REPERCUSSÃO\"\n                               \"Registros Públicos\" # \"REGISTROS PÚBLICOS\"\n                               )\n\nlevels(data$ministro) &lt;- c(\"Min. Alexandre de Moraes\",\n                                  \"Min. André Mendonça\",\n                                  \"Min. Cármen Lúcia\",\n                                  \"Min. Cristiano Zanin\",\n                                  \"Min. Dias Toffoli\",\n                                  \"Min. Edson Fachin\",\n                                  \"Min. Flávio Dino\",\n                                  \"Min. Gilmar Mendes\",\n                                  \"Min. Luiz Fux\",\n                                  \"Min. Nunes Marques\")\n\nrm(df)\nPodemos construir gráficos, afim de avaliar se existe variância nas covariáveis — pré requisito fundamental para uma modelagem classificatória.\nMostrar o código\np1 = ggplot(data, aes(x = classe, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np2 = ggplot(data, aes(x = ministro, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np3 = ggplot(data, aes(x = tipo, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np4 = ggplot(data, aes(x = ramo_direito, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-classe",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-classe",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Decisão por classe",
    "text": "Decisão por classe\n\n\nMostrar o código\np1 \n\n\n\n\n\n\n\n\n\nNotamos que é bastante heterogêneo o tipo de decisão de acordo com a classe processual, o que indica que esta variável possivelmente será importante na modelagem."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#ministro",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#ministro",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Ministro",
    "text": "Ministro\n\n\nMostrar o código\np2\n\n\n\n\n\n\n\n\n\nQuanto ao ministro que proferiu a decisão, existe uma distribuição mais uniforme do que em relação à classe. Ainda assim, é possível ver diferenças em alguns casos, e também a combinação desta covariável com outras pode ser bastante heterogênea."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#tipo-de-decisão",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#tipo-de-decisão",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Tipo de decisão",
    "text": "Tipo de decisão\n\n\nMostrar o código\np3\n\n\n\n\n\n\n\n\n\nAqui vemos uma grande diferença, onde decisões não monocráticas são fortemente mais inclinadas a serem favoráveis ao réu ante as monocráticas. Ainda assim, é importante notar que as decisões monocráticas representam mais de 90% do conjunto de dados."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-ramo-do-direito",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-ramo-do-direito",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Decisão por ramo do direito",
    "text": "Decisão por ramo do direito\n\n\nMostrar o código\np4\n\n\n\n\n\n\n\n\n\nAssim como na classe do processo, o ramo do direito também é bastante heterogêneo, e possivelmente significativo na modelagem.\nPodemos produzir uma tabela para entender melhor estas proporções para cada um dos dois grupos\n\n\nMostrar o código\ncomp = compareGroups(favoravel_reu ~ .,\n                     data=data, method = 4, max.ylev=100, max.xlev=100)\ntable = createTable(comp)\nexport2md(table,\n          strip=TRUE,\n          first.strip=TRUE,\n          format='html',\n          size = 10,\n          caption = \"\")\n\n\n\n\n\n\n\nDecisão desfavorável ao réu\nDecisão favorável ao réu\np.overall\n\n\n\n\n\nN=30256\nN=8554\n\n\n\nclasse:\n\n\n.\n\n\nAC\n1 (0.00%)\n0 (0.00%)\n\n\n\nACO\n25 (0.08%)\n40 (0.47%)\n\n\n\nADC\n3 (0.01%)\n3 (0.04%)\n\n\n\nADI\n99 (0.33%)\n136 (1.59%)\n\n\n\nADO\n2 (0.01%)\n3 (0.04%)\n\n\n\nADPF\n26 (0.09%)\n35 (0.41%)\n\n\n\nAI\n26 (0.09%)\n5 (0.06%)\n\n\n\nAO\n89 (0.29%)\n10 (0.12%)\n\n\n\nAP\n9 (0.03%)\n343 (4.01%)\n\n\n\nAR\n65 (0.21%)\n8 (0.09%)\n\n\n\nARE\n7779 (25.7%)\n1042 (12.2%)\n\n\n\nCC\n8 (0.03%)\n48 (0.56%)\n\n\n\nEP\n2 (0.01%)\n0 (0.00%)\n\n\n\nExt\n43 (0.14%)\n29 (0.34%)\n\n\n\nHC\n10429 (34.5%)\n551 (6.44%)\n\n\n\nHD\n4 (0.01%)\n0 (0.00%)\n\n\n\nInq\n14 (0.05%)\n0 (0.00%)\n\n\n\nMI\n20 (0.07%)\n0 (0.00%)\n\n\n\nMS\n286 (0.95%)\n55 (0.64%)\n\n\n\nPet\n295 (0.98%)\n116 (1.36%)\n\n\n\nPPE\n20 (0.07%)\n1 (0.01%)\n\n\n\nRcl\n5877 (19.4%)\n4316 (50.5%)\n\n\n\nRE\n3133 (10.4%)\n1699 (19.9%)\n\n\n\nRHC\n1878 (6.21%)\n106 (1.24%)\n\n\n\nRMS\n104 (0.34%)\n8 (0.09%)\n\n\n\nRvC\n15 (0.05%)\n0 (0.00%)\n\n\n\nTPA\n4 (0.01%)\n0 (0.00%)\n\n\n\nministro:\n\n\n&lt;0.001\n\n\nMin. Alexandre de Moraes\n2759 (9.12%)\n1207 (14.1%)\n\n\n\nMin. André Mendonça\n3168 (10.5%)\n1007 (11.8%)\n\n\n\nMin. Cármen Lúcia\n2592 (8.57%)\n726 (8.49%)\n\n\n\nMin. Cristiano Zanin\n3472 (11.5%)\n860 (10.1%)\n\n\n\nMin. Dias Toffoli\n2950 (9.75%)\n919 (10.7%)\n\n\n\nMin. Edson Fachin\n2884 (9.53%)\n597 (6.98%)\n\n\n\nMin. Flávio Dino\n3948 (13.0%)\n595 (6.96%)\n\n\n\nMin. Gilmar Mendes\n2874 (9.50%)\n1119 (13.1%)\n\n\n\nMin. Luiz Fux\n2830 (9.35%)\n711 (8.31%)\n\n\n\nMin. Nunes Marques\n2779 (9.18%)\n813 (9.50%)\n\n\n\ntipo:\n\n\n0.000\n\n\nMonocrática\n30118 (99.5%)\n7941 (92.8%)\n\n\n\nNão monocrática\n138 (0.46%)\n613 (7.17%)\n\n\n\nramo_direito:\n\n\n.\n\n\nEducação\n72 (0.24%)\n36 (0.42%)\n\n\n\nAdm Público\n5665 (18.7%)\n1946 (22.7%)\n\n\n\nAmbiental\n175 (0.58%)\n29 (0.34%)\n\n\n\nAssistencial\n11 (0.04%)\n0 (0.00%)\n\n\n\nCivil\n770 (2.54%)\n127 (1.48%)\n\n\n\nCriança e Adolescente\n101 (0.33%)\n5 (0.06%)\n\n\n\nSaúde\n134 (0.44%)\n74 (0.87%)\n\n\n\nConsumidor\n209 (0.69%)\n21 (0.25%)\n\n\n\nTrabalho\n1597 (5.28%)\n2218 (25.9%)\n\n\n\nEleitoral\n191 (0.63%)\n13 (0.15%)\n\n\n\nEleitoral STF\n19 (0.06%)\n1 (0.01%)\n\n\n\nInternacional\n83 (0.27%)\n43 (0.50%)\n\n\n\nMarítimo\n1 (0.00%)\n0 (0.00%)\n\n\n\nPenal\n4816 (15.9%)\n377 (4.41%)\n\n\n\nPenal Militar\n79 (0.26%)\n10 (0.12%)\n\n\n\nPrevidenciário\n530 (1.75%)\n95 (1.11%)\n\n\n\nProcessual Civil e Trabalho\n2664 (8.80%)\n1502 (17.6%)\n\n\n\nProcessual Penal\n10414 (34.4%)\n1375 (16.1%)\n\n\n\nProcessual Penal Militar\n92 (0.30%)\n12 (0.14%)\n\n\n\nTributário\n2607 (8.62%)\n666 (7.79%)\n\n\n\nAlta Complexidade\n22 (0.07%)\n4 (0.05%)\n\n\n\nRegistros Públicos\n4 (0.01%)\n0 (0.00%)\n\n\n\n\n\n\n\n\n\nO teste qui-quadrado que acompanha a tabela na ultima coluna mostra diferença entre as categorias para todos os casos."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#divisão-treino-teste",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#divisão-treino-teste",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Divisão treino-teste",
    "text": "Divisão treino-teste\n\n\nMostrar o código\ndata = data %&gt;%\n  mutate(favoravel_reu = factor(ifelse(favoravel_reu == \"Decisão desfavorável ao réu\",0,1)))\n\nset.seed(150167636)\nsplit &lt;- initial_split(data, prop = 0.80, strata = favoravel_reu)\ntreino &lt;- training(split)\nteste &lt;- testing(split)\n\n\n\n\nMostrar o código\nset.seed(150167636)\ncv_folds &lt;- vfold_cv(treino, \n                     v = 3, \n                     strata = favoravel_reu)\n\n\nFaremos uma divisão clássica de treino e teste, com proporção 80-20. Estratificamos estes conjuntos pela variável explicativa, e utilizaremos validação cruzada para validação de hiperparâmetros dos modelos.\nFizemos já também a re-condificação da variável resposta, sendo 0: decisão desfavorável ao réu, e 1: decisão favorável ao réu."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#receita",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#receita",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Receita",
    "text": "Receita\n\n\nMostrar o código\nreceita &lt;- recipe(favoravel_reu ~ ., data = data) %&gt;%\n  # step_interact(terms = ~ all_factor_predictors():all_factor_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_numeric_predictors())\n\n\nNeste caso, utilizaremos uma receita bem simples, simplesmente criando dummys para todas as covariáveis, e removendo alguma possível coluna de apenas zeros."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#visualizando-dados-após-aplicar-receita",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#visualizando-dados-após-aplicar-receita",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Visualizando dados após aplicar receita",
    "text": "Visualizando dados após aplicar receita\n\n\nMostrar o código\nreceita %&gt;%\n  prep() %&gt;%\n  juice() %&gt;%\n  head(10) %&gt;%\n  datatable()\n\n\n\n\n\n\nPodemos ver agora que nossos dados são simplesmente colunas de zeros e uns."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#workflow",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#workflow",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Workflow",
    "text": "Workflow\nO nosso objetivo neste trabalho não será somente tentar modelar estes dados, mas testar diversos modelos entre os vistos (e também alguns não vistos) em aula, afim de fazer comparação de eficiência entre os modelos para este conjunto de dados. O {framework} do {tidymodels} favorece este tipo de aplicação, sendo possível trabalhar com diversos modelos existentes no pacote, seja para regressão, seja para classificação (nesta caso, estaremos fazendo classificação), e depois aproveitar de funções do tidymodels para compará-los todos de uma vez.\nSelecionou-se o Knn, Naive Bayes, discriminantes linear e quadrático, regressão logística, árvore de decisão, floresta de decisão de floresta de decisão com bagging para esta aplicação. Foram testados preliminarmente modelos de SVM e XGBoost também, porém estes demoravam demais (mais de um dia executando), portanto não se mostraram nada parcimoniosos, o que inviabilizaram a sua utilização neste relatório. Havia também a intenção de utilização do Catboost, que possivelmente performaria bem sobre este conjunto de dados, que é totalmente categórico. Infelizmente, o suporte para R foi depreciado, e até tentamos utilizá-lo em sua forma legada, mas não funcionou apesar de diversas tentativas. Desta forma, ficamos com 8 modelos.\n\n\nMostrar o código\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\nnbayes_spec &lt;- naive_Bayes(smoothness = tune(), Laplace = tune()) %&gt;%\n  set_engine(\"klaR\") %&gt;%\n  set_mode(\"classification\")\n\nlda_spec &lt;- discrim_linear() %&gt;%\n  set_engine(\"MASS\") %&gt;%\n  set_mode(\"classification\")\n\nqda_spec &lt;- discrim_quad() %&gt;%\n  set_engine(\"MASS\") %&gt;%\n  set_mode(\"classification\")\n\nreg_log_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(engine = \"glmnet\", standardize = FALSE) %&gt;%\n  set_mode(\"classification\")\n\ndec_tree &lt;- decision_tree(cost_complexity = tune(),\n                          min_n = tune(),\n                          tree_depth = tune()) %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nbagg_tree &lt;- bag_tree(cost_complexity = tune(),\n                      min_n = tune(),\n                      tree_depth = tune()) %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nrandom_forestst &lt;- rand_forest(min_n = tune(),\n                             trees = tune()) %&gt;%\n  set_engine(engine = \"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\nMostrar o código\nwf = workflow_set(\n  preproc = list(receita),\n  models = list(\n    KNN = knn_spec,\n    Naive_Bayes = nbayes_spec,\n    LDA = lda_spec,\n    QDA = qda_spec,\n    Reg_log = reg_log_spec,\n    decision = dec_tree,\n    bag_tree = bagg_tree,\n    random_forest = random_forestst\n  )\n) %&gt;%\n  mutate(wflow_id = gsub(\"(recipe_)\", \"\", wflow_id))\n\n\nDestes modelos, iremos fazer diversos ajustes de hiperparâmetros, afim de selecionar não só o melhor modelo, mas também a melhor combinação de hiperparâmetros. construiu-se portanto com grid de 20 combinações de hiperparâmetros para cada modelo, utilizando um hipercubo latino. Estes serão avaliados por validação cruzada. Fizemos a divisão treino-teste 80%/20%, e do conjunto de treino selecionamos a técnica v-folds cross validation, com \\(v=3\\). Salvo os modelos de discriminante linear e quadrático que não realizamos ajuste de hiperparâmetros — serão treinados portanto \\(3 \\times 2 = 6\\) modelos deste tipo — os demais modelos serão treinados \\(20 \\times 3 = 60\\) vezes cada, totalizando \\(20 \\times 3 \\times 6 + 2 \\times 6 = 366\\) modelos ao todo que serão ajustados, para um conjunto de dados relativamente grande. Portanto, este ajuste demorou cerca de 20 minutos para rodar, o que é relativamente aceitável dado o tamanho do conjunto de dados e a quantidade de modelos que estamos ajustando.\n\n\nMostrar o código\nplan(multisession)\n\ngrid_ctrl = control_grid(\n  save_pred = TRUE,\n  parallel_over = \"resamples\",\n  save_workflow = TRUE\n)\n\ngrid_results = wf %&gt;%\n  workflow_map(\n    seed = 150167636,\n    resamples = cv_folds,\n    grid = 20,\n    control = grid_ctrl\n  )"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#comparando-modelos",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#comparando-modelos",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Comparando modelos",
    "text": "Comparando modelos\n\n\nMostrar o código\nautoplot(grid_results)\n\n\n\n\n\n\n\n\n\nPelo gráfico comparando os modelos ajustados, notamos que o que teve melhor ajuste aparenta ter sido o modelo de florestas aleatórias, obtendo a maior acurácia e maior proporção de área sobre a curva ROC. Árvores com bagging e árvores de decisão também performaram bem, indicando que neste caso os modelos de árvores aparentam ter sido os melhores.\n\n\nMostrar o código\nautoplot(grid_results, select_best = TRUE, metric = \"roc_auc\")\n\n\n\n\n\n\n\n\n\nAnalisando a performance da melhor combinação de hiperparâmetros para cada modelo testado, notamos novamente que os três melhores modelos são os baseados em árvores. O pior algoritmo foi o KNN, seguido do naive bayes.\n\n\nMostrar o código\nbest_set_linear = grid_results %&gt;% \n  extract_workflow_set_result(\"Reg_log\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_knn = grid_results %&gt;% \n  extract_workflow_set_result(\"KNN\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_nbayes = grid_results %&gt;%\n  extract_workflow_set_result(\"Naive_Bayes\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_lda = grid_results %&gt;% \n  extract_workflow_set_result(\"LDA\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_qda = grid_results %&gt;% \n  extract_workflow_set_result(\"QDA\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_rand_fore = grid_results %&gt;% \n  extract_workflow_set_result(\"random_forest\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_decision = grid_results %&gt;% \n  extract_workflow_set_result(\"decision\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_bag = grid_results %&gt;% \n  extract_workflow_set_result(\"bag_tree\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nresultado_teste &lt;- function(rc_rslts, fit_obj, par_set, split_obj) {\n  res &lt;- rc_rslts %&gt;%\n    extract_workflow(fit_obj) %&gt;%\n    finalize_workflow(par_set) %&gt;%\n    last_fit(split = split_obj,\n             metrics = metric_set(\n              accuracy,roc_auc,\n              f_meas,precision,\n              recall,spec,kap))\n  res\n}\n\nresultado_teste_reg_log &lt;- resultado_teste(grid_results, \"Reg_log\", best_set_linear, split)\nresultado_teste_knn &lt;- resultado_teste(grid_results, \"KNN\", best_set_knn, split)\nresultado_teste_lda &lt;- resultado_teste(grid_results, \"LDA\", best_set_lda, split)\nresultado_teste_qda &lt;- resultado_teste(grid_results, \"QDA\", best_set_qda, split)\nresultado_teste_naive &lt;- resultado_teste(grid_results, \"Naive_Bayes\", best_set_nbayes, split)\nresultado_teste_decision &lt;- resultado_teste(grid_results, \"decision\", best_set_decision, split)\nresultado_teste_bag &lt;- resultado_teste(grid_results, \"bag_tree\", best_set_bag, split)\nresultado_teste_random_forest &lt;- resultado_teste(grid_results, \"random_forest\", best_set_rand_fore, split)\n\nmetrics_table &lt;- rbind(collect_metrics(resultado_teste_reg_log)$.estimate, \n                       collect_metrics(resultado_teste_knn)$.estimate, \n                       collect_metrics(resultado_teste_lda)$.estimate, \n                       collect_metrics(resultado_teste_qda)$.estimate, \n                       collect_metrics(resultado_teste_naive)$.estimate, \n                       collect_metrics(resultado_teste_decision)$.estimate, \n                       collect_metrics(resultado_teste_bag)$.estimate,\n                       collect_metrics(resultado_teste_random_forest)$.estimate)\n\nmetrics_table &lt;- round(metrics_table, 4)\n\nrow_names &lt;- c(\"Regressão Logística\", \"KNN\", \"Discriminante Linear\", \"Discriminante Quadrático\", \"Naive Bayes\", \"Árvore de Decisão\", \"Bagged Tree\", \"Floresta Aleatória\")\n\nmetrics_table &lt;- cbind(row_names, metrics_table)\n\nmetrics_table &lt;- metrics_table %&gt;% \n  as_tibble()\n\ncolnames(metrics_table) &lt;- c(\"Método\", \"Acurácia\", \"Curva Roc\", \"f_means\", \"Precisão\", \"Recall\", \"Especificidade\", \"Kappa\")\n\nmetrics_table &lt;- metrics_table %&gt;% \n  mutate(Acurácia = as.numeric(Acurácia), \n         `Curva Roc` = as.numeric(`Curva Roc`), \n         f_means = as.numeric(f_means), \n         Precisão = as.numeric(Precisão), \n         Recall = as.numeric(Recall), \n         Especificidade = as.numeric(Especificidade), \n         Kappa = as.numeric(Kappa)) %&gt;% \n  arrange(desc(Acurácia), desc(`Curva Roc`), desc(f_means), desc(Kappa)) %&gt;%\n  select(Método,Acurácia,`Curva Roc`,Especificidade)\n  \nmetrics_table %&gt;%  \n  gt::gt() %&gt;% \n  gt::tab_header(\n    title = gt::html(\"&lt;b&gt; Resultado dos modelos nos dados de teste&lt;/b&gt;\"), \n    subtitle = glue::glue(\"De acordo com algumas métricas\")) %&gt;% \n  gt::data_color(\n    columns = Acurácia, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 10, palette = \"Green\"), \n      domain = c(min(metrics_table$Acurácia), max(metrics_table$Acurácia)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n  gt::data_color(\n    columns = `Curva Roc`, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 10, palette = \"Green\"), \n      domain = c(min(metrics_table$`Curva Roc`), max(metrics_table$`Curva Roc`)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n    gt::data_color(\n    columns = Especificidade, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 8, palette = \"Red\"), \n      domain = c(max(metrics_table$Especificidade),min(metrics_table$Especificidade)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n  cols_align(align = \"center\", columns = everything()) %&gt;% \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ), \n    locations = cells_body(columns = c(Acurácia, `Curva Roc`))\n  )\n\n\n\n\n\n\n\n\n\nResultado dos modelos nos dados de teste\n\n\nDe acordo com algumas métricas\n\n\nMétodo\nAcurácia\nCurva Roc\nEspecificidade\n\n\n\n\nFloresta Aleatória\n0.8256\n0.8961\n0.3699\n\n\nÁrvore de Decisão\n0.8244\n0.8951\n0.3714\n\n\nBagged Tree\n0.8230\n0.8941\n0.3694\n\n\nRegressão Logística\n0.8122\n0.8872\n0.3407\n\n\nDiscriminante Linear\n0.8046\n0.8815\n0.3344\n\n\nNaive Bayes\n0.7796\n0.8761\n0.0000\n\n\nKNN\n0.7795\n0.8760\n0.0004\n\n\nDiscriminante Quadrático\n0.7306\n0.8088\n0.3705\n\n\n\n\n\n\n\n\nDesta tabela, vemos a real situação do ajuste. Apesar de bons valores de acurácia e roc auc, a realidade é que todos os modelos performaram mal quando analisada a métrica especificidade. Ou seja, alguns foram até eficientes em predizer verdadeiros zeros, mas todos foram ruins em predizer verdadeiros um (alguns piores que outros). Desta forma, todos os modelos apresentam uma alta taxa de erro tipo 1. Na realidade, estes dados eram bastante desbalanceados, com em torno de 30mil zeros e 8mil uns. Desta forma, é razoável dizer que nenhum modelo pode ser escolhido para esta análise.\n\n\nMostrar o código\npredictions &lt;- resultado_teste_random_forest %&gt;%\n  collect_predictions()\n\nconfusion_matrix &lt;- predictions %&gt;%\n  conf_mat(truth = favoravel_reu, estimate = .pred_class)\n\nautoplot(confusion_matrix, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\nVisualizando a matriz de confusão do melhor modelo (floresta aleatória), notamos o observado acima. A classificação de verdadeiros 1 (decisão favorável ao réu) é extremamente ineficaz.\nDesta forma, podemos concluir que este conjunto de dados é de separação difícil, sendo necessário um estudo muito mais aprofundado das covariáveis — é interessante notar que estes dados são públicos, e foram obtidos de Corte aberta. No entanto, nem todas as covariáveis do processo estão disponíveis de forma pública e direta, como por exemplo o número de embargos que o processo sofreu, algumas tipificações extras, o mérito do processo, etc. Portanto, para tentar realizar uma análise classificatória, é necessário um conhecimento e um conjunto de dados muito mais holístico a fim de obter um ajuste mais preciso."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html",
    "href": "presentations/estocasticos/estocasticos.html",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "",
    "text": "Modelagem de dados e previsão de eventos futuros tem sido o garantidor do mercado de trabalho em estatística estar aquecido. Este tipo de inteligência aplicada à negócios vem se tornando essencial em qualquer setor, inclusive no poder Judiciário. Apesar deste não depender de faturamento para sua existência por ser inalienável ao setor público, tem sido do interesse dos próprios tribunais saber indicadores e estatísticas relacionadas ao próprio tribunal. E a modelagem surge neste contexto para agregar valor às decisões estratégicas deste poder.\nBuscou-se trabalhos de temas semelhantes, que visam modelar a quantidade de decisões em processos em tribunais. Entretanto, não consegui localizar nenhum; motivo esse talvez por ser tema sensível, no sentido de poder levar indiretamente a comparação de rendimentos entre juízes em um tribunal, seja pela possível falta de pertinência do assunto, seja pela tentativa inédita de modelar algo do tipo.\nEntretando, buscou-se trabalhos que modelam outros tipos de contagem usando de processo de Poisson não homogêneo, como (DINCER;DEMIR;YALÇIN,2022).\nNa metodologia, com base no livro ROSS (2010), apresento toda a metodologia e suporte teórico adotado na seção de análises. Na seção aplicação computacional iremos trazer uma possível solução simulada do problema proposto, e na seção de resultados apresento as conclusões gerais."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#dia-útil",
    "href": "presentations/estocasticos/estocasticos.html#dia-útil",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Dia útil",
    "text": "Dia útil\nPor mais hiperbólico que pareça, não foi possível localizar uma definição fechada de dia útil. Portanto, consideraremos como dia útil, todos os dias do ano excetuado os feriados, finais de semana (sábado e domingo) e recessos coletivos. Pegando como base o ano de 2023, iríamos considerar como dia útil para o tribunal selecionado para a aplicação computacional todos os dias com excessão de:\n\nTodos os sábados e todos os domingos do ano;\nDias 01/01/2023 a 06/01/2023 - Recesso Forense;\nDias 20/02/2023 a 22/02/2023 - Carnaval & quarta-feira de cinzas;\nDias 5/04/2023 a 7/04/2023 - Quarta e quinta-feira Santa e Paixão de Cristo;\nDia 21/04/2023 - Tiradentes;\nDia 1/05/2023 - Dia do trabalhador;\nDia 08/06/2023 - Corpus Christi;\nDia 11/08/2023 - Dia do Advogado;\nDia 7/09/2023 - Independência;\nDia 12/10/2023 - Dia de Nossa Senhora Aparecida;\nDias 01/11/2023 e 02/11/2023 - Dia de Todos os Santos e Finados;\nDia 15/11/2023 - Proclamação da República;\nDia 08/12/2023 - Dia da Justiça;\nDias 20/12/2023 a 31/12/2023 - Recesso Forense.\n\nNote que feriados que se sobrepõe ou que são aos finais de semana, só foram anotados uma vez, para evitar confusão. Note ainda que é trivial inserir as férias individuais de juízes e ministros desta forma.\nPortanto, se fixado o ano, pode-se pensar no intervalo \\((0,t]\\) como sendo o primeiro instante do primeiro dia do ano até o último instante do último dia do ano, podemos ordenar os dias por índice, tal que o primeiro dia do ano (1º de janeiro) representa o dia 1, o dia 1º de Fevereiro representa o dia 32, até o dia 31 de dezembro, representando o dia 365. O restante dos dias são dias úteis.\nCom isso, teremos 134 dias que não são dias úteis no ano de 2023, sendo estes os dias de índice: 1, 2, 3, 4, 5, 6, 7, 8, 14, 15, 21, 22, 28, 29, 35, 36, 42, 43, 49, 50, 51, 52, 53, 56, 57, 63, 64, 70, 71, 77, 78, 84, 85, 91, 92, 95, 96, 97, 98, 99, 105, 106, 111, 112, 113, 119, 120, 121, 126, 127, 133, 134, 140, 141, 147, 148, 154, 155, 159, 161, 162, 168, 169, 175, 176, 182, 183, 189, 190, 196, 197, 203, 204, 210, 211, 217, 218, 223, 224, 225, 231, 232, 238, 239, 245, 246, 250, 252, 253, 259, 260, 266, 267, 273, 274, 280, 281, 285, 287, 288, 294, 295, 301, 302, 305, 306, 308, 309, 315, 316, 319, 322, 323, 329, 330, 336, 337, 342, 343, 344, 350, 351, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364 e 365. Iremos chamar o vetor que contém estes valores acima de \\(\\textbf{v}_{dnu}\\), sendo seu complementar \\(\\textbf{v}_{du}\\) o vetor que contém todos os dias de 1 a 365 não contidos no vetor acima, representando assim os dias úteis (\\(du\\))."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#processo-de-contagem",
    "href": "presentations/estocasticos/estocasticos.html#processo-de-contagem",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Processo de contagem",
    "text": "Processo de contagem\nUm processo de contagem, definido por DINCER;DEMIR;YALÇIN (2022 apud BAS,2019) , é um processo estocástico, em que \\(M(t),t \\geq 0\\) é não negativo, com suporte em \\(\\mathbb{Z}\\) não decrescente \\(\\forall t \\geq 0\\). \\(M(t)\\) é o número total de eventos que ocorrem até o tempo \\(t\\), e \\(M(t,t+h)=M(t+h)-M(t)\\) denota o número de eventos ocorridos no intervalo \\((t,t+h],h&gt;0\\). Além disso, um processo de contagem, em que o número de eventos ocorridos em intervalos de tempo disjuntos são independentes e tem incrementos independentes. Um processo de contagem tem incrementos estacionários se a distribuição de \\(M(t,t+h)\\) depende apenas do comprimento do intervalo de tempo."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#variável-aleatória-poisson",
    "href": "presentations/estocasticos/estocasticos.html#variável-aleatória-poisson",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Variável aleatória Poisson",
    "text": "Variável aleatória Poisson\nSe \\(M(t)\\) denota o número de eventos que ocorrem num intervalo de tempo específico, este é chamado variável aleatória Poisson. A variável aleatória Poisson possui função de massa de probabilidade:\n\\[\\begin{equation}\n    \\mathbb{P}(M(t)=k)=\\frac{e^{-\\lambda_t}\\lambda_t^k}{k!}\n\\end{equation}\\]\nonde \\(\\lambda_t\\) é o parâmetro da distribuição Poisson e representa a taxa de eventos que ocorrem no intervalo de tempo especificado, além de ser a variância e esperança de \\(M(t)\\) ser igual a \\(\\lambda_t\\)."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#processo-de-poisson",
    "href": "presentations/estocasticos/estocasticos.html#processo-de-poisson",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Processo de Poisson",
    "text": "Processo de Poisson\nUm processo de Poisson é definido por ROSS(2010) como:\n\\[\\begin{equation} \\label{eq1}\nN(t)  \\stackrel{}{\\sim} Poisson(m_(t)), i=1,2,...,n.\n\\end{equation}\\]\nE:\n\\[\\begin{equation}\n    \\mathbb{P}\\{N(t) = x\\} = \\frac{[m(t)]^{x}}{x!}e^{-m(t)}\n\\end{equation}\\]\nOnde \\(\\frac{o(h)}{h} \\rightarrow 0, h \\rightarrow 0\\)\n\nProcesso de Poisson homogêneo\nSe o processo de Poisson \\(M(t)\\) têm função de intensidade constante \\(\\lambda_t=\\lambda \\ \\forall \\ (t,t+s]\\), então este é chamado um processo de Poisson homogêneo. (DINCER;DEMIR;YALÇIN,2022) \n\n\nProcesso de Poisson não homogêneo\nSe o processo de Poisson \\(M(t)\\) tem função de intensidade variando com o tempo, este é um Processo de Poisson não homogêneo. A função massa de probabilidade deste processo é definida como:\n\\[\\begin{equation}\n    \\mathbb{P}(M(t,t+h)=x)=\\frac{\\left( \\int_t^{t+h}\\lambda(z)dz \\right)^x}{x!}e^{-\\int_t^{t+h}\\lambda(z)dz}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juiz-em-um-mês",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juiz-em-um-mês",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados por um juiz em um mês?",
    "text": "Quantos processos são julgados por um juiz em um mês?\nTomando Janeiro de 2023 (dias 1 a 31) como exemplo, o número de processos julgados pelo i-ésimo juiz será uma variável aleatória Poisson, com média:\n\\[\\begin{equation} \\label{eq3}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=31}\\Lambda_i(s)ds \\ ; \\{s = 30\\} \\\\\n    = \\lambda_i\\int_{t=0}^{t=8}\\Lambda_{i(2)}ds \\\\ + \\ \\lambda_i\\int_{t=8}^{t=13}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=13}^{t=15}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=15}^{t=20}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=20}^{t=22}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=22}^{t=27}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=27}^{t=29}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=29}^{t=31}\\Lambda_{i(1)}ds\\\\\n\\end{split}\n\\end{equation}\\]\nNote que podemos chamar uma variável \\(z\\) para substituir \\(t\\) nos limites de integração, afim de não precisar respeitar a ordenação dos dias e considerar apenas a passagem de tempo, tal que:\n\\[\\begin{equation}\n\\begin{cases}\n    t = i \\rightarrow \\text{i-ésimo dia} \\\\\n    z = i \\rightarrow \\text{i-dias}\n\\end{cases}\n\\end{equation}\\]\nPortanto, teremos\n\\[\\begin{equation} \\label{eq4}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=31}\\lambda_i(s)ds &= \\lambda_i \\left( \\int_{z=0}^{z=17}\\Lambda_{i(1)}ds \\right) + \\lambda_i \\left( \\int_{z=17}^{z=31}\\Lambda_{i(2)}ds \\right)\\\\\n    &= \\lambda_i \\left[ \\left( \\int_{z=0}^{z=17}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=17}^{z=31}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juíz-em-um-ano",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juíz-em-um-ano",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados por um juíz em um ano?",
    "text": "Quantos processos são julgados por um juíz em um ano?\nExpandindo o raciocínio empregado anteriormente, podemos dizer que a quantidade de processos julgados pelo i-ésimo juiz no ano de 2023, com tempo \\(t \\in \\textbf{v}_{du} \\cup \\textbf{v}_{dnu}\\) será Poisson com média:\n\\[\\begin{equation} \\label{eq5}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=365}\\lambda_i(s)ds\n    &= \\lambda_i \\left[ \\left( \\int_{z=0}^{z=231}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=231}^{z=365}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-em-um-tribunal-em-um-ano",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-em-um-tribunal-em-um-ano",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados em um tribunal em um ano?",
    "text": "Quantos processos são julgados em um tribunal em um ano?\nFinalmente, podemos responder nossa pergunta inicial, aproveitando dos resultados das equações anteriores, temos que a quantidade de processos julgados em um tribunal (N(t=365)) será Poisson com média:\n\\[\\begin{equation} \\label{eq6}\n\\begin{split}\n    N(t=365) = \\sum_{i=1}^{n} \\lambda_i \\left[ \\left( \\int_{z=0}^{z=231}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=231}^{z=365}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#referências",
    "href": "presentations/estocasticos/estocasticos.html#referências",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Referências",
    "text": "Referências\n\nRoss, S. M. (2010). Introduction to Probability Models. Academic Press.\nDincer, E., Demir, S. M., & Yalçın, B. A. (2022)."
  },
  {
    "objectID": "presentations/nlp/index.html#introdução",
    "href": "presentations/nlp/index.html#introdução",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Introdução",
    "text": "Introdução\n\n\nNo campo do direito, a aplicação de técnicas estatísticas vem sendo testada em diversos âmbitos, inclusive no Supremo Tribunal Federal do Brasil.\nÉ de interesse do tribunal a aplicação destas técnicas para agrupamento de processos. Um agrupador poderia ajudar a identificar processos semelhantes, trabalho este feito manualmente.\nEste trabalho busca estudar e aplicar algumas destas técnicas para o desenvolvimento de uma aplicação prática no STF, com objetivo de agrupar processos de controle concentrado."
  },
  {
    "objectID": "presentations/nlp/index.html#objetivos",
    "href": "presentations/nlp/index.html#objetivos",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Objetivos",
    "text": "Objetivos\n\nO objetivo deste trabalho é formular um agregador de processos de controle concentrado, que são processos que tratam da constitucionalidade de leis e atos normativos. Constituem o dito controle concentrado os processos do Supremo Tribunal Federal das seguintes classes:\n\n\nADI (Ação Direta de Inconstitucionalidade)\nADC (Ação Declaratória de Constitucionalidade)\nADPF (Arguição de Descumprimento de Preceito Fundamental)\nADO (Ação Direta de Inconstitucionalidade por Omissão)"
  },
  {
    "objectID": "presentations/nlp/index.html#objetivos-1",
    "href": "presentations/nlp/index.html#objetivos-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Objetivos",
    "text": "Objetivos\n\nO agrupador fornecerá subsídios aos responsáveis pelo encaminhamento dos processos que chegam ao STF, visando reduzir o trabalho mecânico humano.\nDos objetivos específicos, espera-se:\n\n\nProcessar os dados utilizando técnicas de Processamento de Linguagem Natural (PLN), transformando petições iniciais de processos em vetores numéricos;\nComparar técnicas de agrupamento;\nAvaliar a similaridade entre processos em recortes temporais distintos;\nEstudar técnicas de PLN, análise multivariada e visualização de dados."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia",
    "href": "presentations/nlp/index.html#metodologia",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\nTendo os dados e o modelo pré-treinado para vetorização, os códigos Python e R para a vetorização dos textos, e posterior análise, são da seguinte forma:\n\n\n\n# Módulos\nimport polars\nimport gensim\nfrom gensim.models.doc2vec import Doc2Vec\n\n# Função\ndef infer_vector(text):\n    return model.infer_vector(text.split())\n\n# Modelo pré-treinado para Embedding\nmodel = gensim.models.Doc2Vec.load(\"modelo.model\")\n\n# Dados\ndf = polars.read_csv(\"dados.csv\",columns=[1,3,4])\n\n# Saída: DataFrame com duas colunas: Texto original e vetor Embedding correspondente.\ndf = df.with_columns_seq(polars.col(\"texto\").apply(infer_vector).alias(\"vetor\"))\n\n\n\n# Pacote\nlibrary(reticulate)\n\n# Definindo o ambiente virtual python\nreticulate::use_condaenv(\"TCC\")\n\n# Executando o script python\nreticulate::source_python(\"script.py\")\n\n# Ajustando o dataframe trazido do python para formato R mais adequado\ndf &lt;- as.data.frame(do.call(rbind, lapply(a, function(x) c(x[[1]], x[[2]]))), stringsAsFactors = FALSE)"
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-1",
    "href": "presentations/nlp/index.html#metodologia-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\nPassos para a construção do agregador:\n\n\n\n\n\nObtenção dos dados:\n\n\nOs dados foram disponibilizados pelo STF (mas estão disponíveis publicamente no Portal do STF.).\n\n\nVetorização (incluindo ocerização e processamento do texto PDF):\n\n\nEste módulo foi fornecido pelo STF (dados em formato CSV)"
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-2",
    "href": "presentations/nlp/index.html#metodologia-2",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nDefinir recortes temporais para a agregação:\n\n\nPor conta da natureza cíclica dos processos que compõem o acervo do STF, é necessário um sistema de atualização constante para uma aplicação prática.\nSerá realizado o agrupamento com dados em recortes temporais distintos, e, em cada recorte, será avaliada a similaridade entre os processos em tramitação naquela data."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-3",
    "href": "presentations/nlp/index.html#metodologia-3",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nAplicação de medidas de distância para comparar a similaridade dos processos (distância euclidiana, distância do cosseno etc)."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-4",
    "href": "presentations/nlp/index.html#metodologia-4",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nPara a formação dos agrupamentos, serão utilizadas técnicas de agrupamento hierárquico e não-hierárquico baseadas nas distâncias calculadas.\n\n\nPara a visualização dos dados, serão estudadas técnicas como dendrogramas e t-SNE."
  },
  {
    "objectID": "presentations/nlp/index.html#cronograma",
    "href": "presentations/nlp/index.html#cronograma",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Cronograma",
    "text": "Cronograma\n\n\nCronograma TCC 1\n\n\nAtividades\nMar\nAbr\nMai\nJun\nJul\n\n\n\n\nEscolha do tema a ser abordado.\n\n\n\n\n\n\n\nLevantamento de bibliografia relacionada ao tema.\n\n\n\n\n\n\n\nDefinição do recorte temporal com a AAJ do STF.\n\n\n\n\n\n\n\nSolicitação dos dados para a STI do STF.\n\n\n\n\n\n\n\nSolicitação dos algoritmos à STI do STF.\n\n\n\n\n\n\n\nRevisão de literatura.\n\n\n\n\n\n\n\nDesenvolvimento da proposta de projeto.\n\n\n\n\n\n\n\nAnálise preliminar do banco de dados.\n\n\n\n\n\n\n\nEntrega da proposta do projeto.\n\n\n\n\n\n\n\nElaboração da apresentação da proposta.\n\n\n\n\n\n\n\nManipulação do banco de dados.\n\n\n\n\n\n\n\nAnálise do banco de dados.\n\n\n\n\n\n\n\nElaboração do relatório parcial.\n\n\n\n\n\n\n\nEntrega do relatório parcial a Professora Orientadora.\n\n\n\n\n\n\n\nCorreção do relatório parcial.\n\n\n\n\n\n\n\nEntrega do relatório parcial a banca."
  },
  {
    "objectID": "presentations/nlp/index.html#cronograma-1",
    "href": "presentations/nlp/index.html#cronograma-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Cronograma",
    "text": "Cronograma\n\n\nCronograma TCC 2\n\n\nAtividades\nAgo\nSet\nOut\nNov\nDez\n\n\n\n\nDesenvolvimento do modelo e da aplicação.\n\n\n\n\n\n\n\nElaboração do relatório final.\n\n\n\n\n\n\n\nEntrega do relatório final a Professora Orientadora.\n\n\n\n\n\n\n\nCorreção do relatório final.\n\n\n\n\n\n\n\nEntrega do relatório final para a banca."
  },
  {
    "objectID": "presentations/nlp/index.html#referências",
    "href": "presentations/nlp/index.html#referências",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Referências",
    "text": "Referências\n\nARTES, R.; BARROSO, L. P. Métodos multivariados de análise estatística. [S.l.]: São Paulo: Blucher, 2023.\nEVERITT, B.; SKRONDAL, A. The cambridge dictionary of statistics. [S.l.]: Cambridge University Press, 2010. v. 4.\nFREITAS, L. J. G. et al. Catboost algorithm application in legal texts and un 2030 agenda. Revista de Informatica Teórica e Aplicada - RITA - ISSN 2175-2745. Vol. 30, Num. 02 (2023) 51-58, 2023.\nFREITAS, L. J. G. et al. Text clustering applied to data augmentation in legal contexts. arXiv preprint arXiv:2404.08683, 2024.\nJOHNSON, R. A.; WICHERN, D. W. Applied Multivariate Statistical Analysis. [S.l.]: 6. ed.[S.l.]:Prentice Hall, 2007.\nKAUFMAN, L.; ROUSSEEUW, P. J. Finding groups in data: an introduction to cluster analysis. [S.l.]: John Wiley & Sons, 1990.\nLECUN, Y. et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, Ieee, v. 86, n. 11, p. 2278–2324, 1998.\nMAATEN, L. Van der; HINTON, G. Visualizing data using t-sne. Journal of machine learning research, v. 9, n. 11, 2008.\nMACQUEEN, J. et al. Some methods for classification and analysis of multivariate observations. [S.l.], 1967. v. 1. 281–297 p.\nMORETTIN, P. A.; SINGER, J. M. Estatística e Ciência de Dados. [S.l.]: LTC, 2021.\nRICARDO, B.-Y.; BERTHIER, R.-N. Modern information retrieval: the concepts and technology behind search. [S.l.]: New Jersey, USA: Addi-son-Wesley Professional, 2011.\nvon Borries, G.; WANG, H. Partition clustering of high dimensional low sample size data based on p-values. Computational statistics & data analysis, v. 53, n. 12, p. 3987-3998, 2009.\n\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/otim_mv/lista2.html",
    "href": "presentations/otim_mv/lista2.html",
    "title": "Lista 2 — Otimização e máxima verossimilhança",
    "section": "",
    "text": "Show the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\nSeja \\(\\mathbf{T}\\) uma variável aleatória seguindo uma distribuição Birnbaum-Saunders. Então, \\(\\mathbf{T}\\) é definido por \\[\\mathbf{T = \\beta  \\left[\\frac{\\alpha}{2}Z+\\sqrt{\\left[\\frac{\\alpha}{2}Z\\right]^2+1}\\right]^2},\\] em que \\(\\alpha&gt;0\\) e \\(\\beta&gt;0\\) são parâmetros de forma e escala, respectivamente, e \\(\\mathbf{Z}\\) é uma variável aleatória com distribuição normal padrão. Temos a notação \\(\\mathbf{T\\sim BS(\\alpha,\\beta)}\\). O parâmetro \\(\\beta\\) é também um parâmetro de localização, pois ele é a mediana da distribuição de \\(\\mathbf{T}\\).\nNote que se \\(\\mathbf{T\\sim BS(\\alpha,\\beta)}\\), então \\[\\mathbf{Z = \\frac{1}{\\alpha}\\left[\\sqrt{\\frac{T}{\\beta}}-\\sqrt{\\frac{\\beta}{T}}\\right]\\sim N(0,1).}\\]\nA função densidade de probabilidade de \\(\\mathbf{T}\\) é dada por \\[\\mathbf{f_T(t)=\\frac{1}{\\sqrt{2\\pi}}exp\\left(-\\frac{1}{2\\alpha^2}\\left[\\frac{t}{\\beta}+\\frac{\\beta}{t}-2\\right]\\right)\\frac{t^{-\\frac{3}{2}}[t+\\beta]}{2\\alpha\\sqrt{\\beta}};t&gt;0}.\\]\nUma forma alternativa de expressar a densidade dada na equação anterior é \\[\\mathbf{f_T(t)=\\phi(a_t)A_t;t&gt;0,\\alpha&gt;0,\\beta&gt;0},\\] em que \\(\\mathbf{A_t}\\) é a derivada de \\(\\mathbf{a_t}\\) com respeito a \\(\\mathbf{t}\\),\n\\[\\mathbf{a_t(\\alpha,\\beta)=a_t=\\frac{1}{\\alpha}\\left[\\sqrt{\\frac{t}{\\beta}}-\\sqrt{\\frac{\\beta}{t}}\\right]},\\]\nque é expresso como\n\\[\\mathbf{A_t=\\frac{d}{dt}a_t = \\frac{t^{-\\frac{3}{2}}[t+\\beta]}{2\\alpha\\sqrt{\\beta}}}.\\]\nSeja \\(\\mathbf{T_1,T_2,...,T_n}\\) uma amostra aleatória de tamanho \\(\\mathbf{n}\\) de \\(\\mathbf{T \\sim BS(\\alpha,\\beta)},\\) e seja \\(\\mathbf{t_1,t_2,...,t_n}\\) as correspondentes observações. A função de log-verossimilhança para \\(\\mathbf{\\theta=(\\alpha,\\beta)}^T\\) é dada por\n\\[\\mathbf{\\ell(\\theta)=c_1+\\frac{n}{\\alpha^2}-\\frac{1}{2\\alpha^2}\\sum_{i=1}^n\\left(\\frac{t_i}{\\beta}+\\frac{\\beta}{t_i}\\right)- n log(\\alpha)-\\frac{n}{2}log(\\beta)+\\sum_{i=1}^{n}log(t_i+\\beta)},\\] em que \\(\\mathbf{c_1}\\) é uma constante que não depende de \\(\\theta\\).\nPara maximizar a função de log-verossimilhança \\(\\mathbf{\\ell(\\theta)=\\ell(\\alpha,\\beta)},\\) precisamos das primeiras derivadas em relação a \\(\\mathbf{\\alpha}\\) e \\(\\mathbf{\\beta}\\) formando o vetor escore definido por \\(\\mathbf{\\dot{\\ell}=(\\dot{\\ell}_{\\alpha},\\dot{\\ell}_{\\beta})}^T,\\) cujos elementos são dados por\n\\[\n\\mathbf{\\dot{\\ell}_{\\alpha}=\\frac{\\partial\\ell(\\alpha,\\beta)}{\\partial\\alpha}  =-\\frac{2n}{\\alpha^3}+\\frac{1}{\\alpha^3}\\sum_{i=1}^n\\left(\\frac{t_i}{\\beta}+\\frac{\\beta}{t_i}\\right)-\\frac{n}{\\alpha},}\n\\]\n\\[\n\\mathbf{\\dot{\\ell}_{\\beta}=\\frac{\\partial\\ell(\\alpha,\\beta)}{\\partial\\beta}  =\\frac{1}{2\\alpha^2}\\sum_{i=1}^n\\left(\\frac{t_i}{\\beta^2}+\\frac{1}{t_i}\\right)-\\frac{n}{2\\beta}+\\sum_{i=1}^n\\frac{1}{t_i+\\beta}.}\n\\]\nA matriz Hessiana é dada por\n\\[\n\\mathbf{\\ddot{\\ell}=\\left(\\frac{\\partial^2\\ell(\\theta)}{\\partial\\theta_i\\partial\\theta_j}\\right) =\n\\left(\n\\begin{matrix}\n\\ddot{\\ell}_{\\alpha\\alpha} & \\ddot{\\ell}_{\\alpha\\beta} \\\\\n\\ddot{\\ell}_{\\beta\\alpha} & \\ddot{\\ell}_{\\beta\\beta}\n\\end{matrix}\n\\right), i,j = 1,2},\n\\]\nem que\n\\[\\begin{matrix}\n\\ddot{\\ell}_{\\alpha\\alpha} = \\frac{\\partial^2\\ell(\\alpha,\\beta)}{\\partial\\alpha^2} = \\frac{6n}{\\alpha^4}-\\frac{3}{\\alpha^4}\\sum_{i=1}^n\\left(\\frac{t_i}{\\beta}+\\frac{\\beta}{t_i}\\right)+\\frac{n}{\\alpha^2},\\\\\n\\ddot{\\ell}_{\\alpha\\beta} = \\frac{\\partial^2\\ell(\\alpha,\\beta)}{\\partial\\alpha\\partial\\beta} = \\ddot{\\ell}_{\\beta\\alpha} =  \\frac{1}{\\alpha^3}\\sum_{i=1}^n\\left(\\frac{1}{t_i}-\\frac{t_i}{\\beta^2}\\right),\\\\\n\\ddot{\\ell}_{\\beta\\beta} = \\frac{\\partial^2\\ell(\\alpha,\\beta)}{\\partial\\beta^2} = -\\frac{1}{\\alpha^2\\beta^3}\\sum_{i=1}^nt_i+\\frac{n}{2\\beta^2}-\\sum_{i=1}^n\\frac{1}{(t_i+\\beta)^2}.\n\\end{matrix}\\]\nGere uma amostra simulada de tamanho \\(n=100\\) da distribuição Birnbaum-Saunders com \\(\\mathbf{\\alpha=0.5}\\) e \\(\\mathbf{\\beta=2.0}\\). Estime os parâmetros \\(\\mathbf{\\alpha}\\) e \\(\\mathbf{\\beta}\\) através do método da máxima verossimilhança (usando o método de Newton) baseado na amostra simulada (pode usar como valores iniciais \\(\\mathbf{\\alpha_0=0.1}\\) e \\(\\mathbf{\\beta_0=1.0}\\))."
  },
  {
    "objectID": "presentations/otim_mv/lista2.html#gerando-valores-da-bs",
    "href": "presentations/otim_mv/lista2.html#gerando-valores-da-bs",
    "title": "Lista 2 — Otimização e máxima verossimilhança",
    "section": "Gerando valores da BS",
    "text": "Gerando valores da BS\nPara gerar valores da BS, podemos aproveitar da relação estocástica desta distribuição com a distribuição normal, cujo gerador de NPA’s já se encontra implementado no R.\n\n\nShow the code\nalpha = .5\nbeta = 2\nn = 100\n\nset.seed(251106723)\nZ = rnorm(100)\n\nt = beta*((alpha/2)*Z + sqrt((((alpha/2)*Z)^2)+1))^2\n\nhead(t);median(t)\n\n\n[1] 2.9541459 1.9799638 2.0596584 0.9425411 2.3689728 2.6796811\n\n\n[1] 1.860395"
  },
  {
    "objectID": "presentations/otim_mv/lista2.html#estimando-os-parâmetros",
    "href": "presentations/otim_mv/lista2.html#estimando-os-parâmetros",
    "title": "Lista 2 — Otimização e máxima verossimilhança",
    "section": "Estimando os parâmetros",
    "text": "Estimando os parâmetros\nPara estimar os parâmetros \\(\\alpha\\) e \\(\\beta\\), implementaremos o método da máxima verossimilhança, utilizando o método de Newton. É necessário informar um valor inicial para o algoritmo, cujo exercício sugere \\(\\mathbf{\\alpha_0=0.1}\\) e \\(\\mathbf{\\beta_0=1.0}\\).\n\n\nShow the code\nalpha0 = 0.1\nbeta0 = 1\ntheta = c(alpha0, beta0)\n\nbsest &lt;- function(t, theta, tol = 1e-6, maxit = 1000) {\n  alpha &lt;- theta[1]\n  beta &lt;- theta[2]\n  n &lt;- length(t)\n  for (i in 1:maxit) {\n  \n  la = -((2*n)/(alpha^3)) + (1/(alpha^3)) * sum((t/beta) + (beta/t)) - (n/alpha) \n  lb = (1/(2*alpha^2)) * sum((t/beta^2) - (1/t)) - (n/(2*beta)) + sum(1/(t+beta))\n  laa = ((6*n)/alpha^4) - (3/alpha^4) * sum((t/beta) + (beta/t)) + (n/alpha^2)\n  lab = (1/alpha^3) * sum((1/t) - (t/beta^2))\n  lbb = -(1/((alpha^2)*(beta^3))) * sum(t + (n/(2*beta^2))) - sum(1/((t+beta)^2))\n  \n  H &lt;- matrix(c(laa, lab, lab, lbb), nrow = 2)\n  grad &lt;- c(la, lb)\n  \n  delta &lt;- solve(H, -grad)\n  new_theta &lt;- theta + delta\n\n    if (max(abs(new_theta - theta)) &lt; tol) {\n      cat(\"Convergiu em\", i, \"iterações\\n\")\n      return(new_theta)\n    }\n    theta &lt;- new_theta\n    alpha &lt;- theta[1]\n    beta &lt;- theta[2]\n  }\n  stop(\"Não convergiu\")\n}\nresult &lt;- bsest(t, theta)\n\n\nConvergiu em 122 iterações\n\n\nShow the code\nresult\n\n\n[1] 0.4614008 1.8881997\n\n\nPodemos observar que a convergência foi muito boa. A menos de um \\(\\epsilon\\), os valores aproximados de \\(\\alpha\\) e \\(\\beta\\) a partir da amostra de fato convergem para os verdadeiros parâmetros."
  },
  {
    "objectID": "presentations/tcc2/slide.html#introdução",
    "href": "presentations/tcc2/slide.html#introdução",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Introdução",
    "text": "Introdução\n\n\n\n\n\nO Ministro Luís Roberto Barroso assumiu a presidência do Tribunal em 2023, e estipulou como uma de suas metas de gestão a diminuição do acervo de processos de controle concentrado em 20%.\nA tarefa de busca de processos similares no acervo de controle concentrado é feita hoje manualmente. Portanto, a identificação automatizada possibilita reduzir o tempo e o esforço manual na busca de processos similares.\nNesse trabalho, buscou-se aplicar técnicas de processamento de linguagem natural (NLP) para detecção de processos semelhantes no acervo de processos de controle concentrado do Tribunal.\n\n\n\n\n\n\n\n\n\ngraph TD\n    A[Recebimento] --&gt; B{Triagem Inicial};\n    B --&gt; C[Distribuição];\n    B --&gt; T1[...];\n    T1 --&gt; T2[Busca por similaridade com \\n outros processos do acervo];\n    T2 --&gt; T3[...];\n    T3 --&gt; B;\n    C --&gt; D[...];"
  },
  {
    "objectID": "presentations/tcc2/slide.html#conjunto-de-dados",
    "href": "presentations/tcc2/slide.html#conjunto-de-dados",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Conjunto de dados",
    "text": "Conjunto de dados\n\n\n\n\nDescriçãoComparaçãoQuantidade\n\n\n\nOs dados utilizados para esta análise são as petições iniciais dos processos de controle concentrado. Este é o primeiro documento que chega no Tribunal tratando de um processo.\nDesta forma, o desafio é encontrar similaridades entre processos na sua fase inicial de tramitação, tal que esta ferramenta seja desbravadora na busca de similaridades antes de outros encaminhamentos internos no Tribunal.\nAs petições iniciais são um dado público, e podem ser obtidas no Portal do STF.\n\n\n\n\nPara realizar a busca por similaridades, necessitamos fixar um processo, que chamaremos de paradigma.\nOs processos paradigmas selecionados foram aqueles que tiveram pauta ou decisão conjunta pelo Tribunal com outros processos.\nDesta forma, buscou-se encontrar técnicas que apontassem a similaridade desses processos já em suas petições iniciais.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExemplo de uma petição inicial — ADPF 857"
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-vetorização",
    "href": "presentations/tcc2/slide.html#referencial-teórico-vetorização",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — Vetorização",
    "text": "Referencial teórico — Vetorização\n\nPor se tratar de um dado textual, é necessário aplicar técnicas de vetorização ao texto.\n\n\n\n\nBag of WordsNota\n\n\n\n\n\n\nEsta é uma das formas mais simples de vetorizar um texto. Iremos simplesmente tabelar a frequência de utilização de cada um dos termos presentes no texto.\n\n\n\n\n\n\nDoc2vecNota\n\n\n\n\n\n\nEsta é uma das formas mais sofisticadas para vetorização de texto. Nela, iremos obter o vetor numérico que representa o texto pelo treinamento de uma rede neural. Os parâmetros deste modelo são atualizados buscando prever a próxima palavra dado o seu contexto e dado um vetor de palavras que formam coletivamente o texto (Freitas et al. ,2024)."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-métricas-de-similaridade",
    "href": "presentations/tcc2/slide.html#referencial-teórico-métricas-de-similaridade",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — Métricas de similaridade",
    "text": "Referencial teórico — Métricas de similaridade\n\nPara comparar os vetores criados, necessitamos de métricas de distância estatística\n\n\n\n\nDistância do CossenoNota\n\n\n\n\n\n\nPela Lei dos cossenos:\n\\[\\begin{equation}\n\\mathbf{A} \\cdot \\mathbf{B} = | \\mathbf{A}|| \\mathbf{B}|\\cos\\theta.\n\\end{equation}\\]\nPodemos utilizar, para avaliar o grau de similaridade entre os vetores \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\), a correlação entre eles. E para quantificar esta correlação, podemos utilizar o cosseno do ângulo entre estes vetores, tal que:\n\\[\\begin{equation}\\text{sim}(\\mathbf{A,B}) = \\cos\\theta = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{A}|| \\mathbf{B}|}.\n\\end{equation}\\]\nEsta é possivelmente a métrica mais utilizada no paradigma de NLP.\n\n\n\n\n\n\nDivergência de JensenNota\n\n\n\n\n\n\nUma outra métrica popular na área de Machine Learning, podemos calcular o complementar da divergência de Jensen-Shannon definida por:\n\\[\\begin{equation}\nJSD(P||Q) = \\\\\n\\frac{1}{2}(KL(P||R)+KL(Q||R)),\n\\end{equation}\\]\ncomo sendo uma medida de similaridade entre duas distribuições de probabilidade. No caso, tomamos os vetores como distribuições de probabilidade empírica. Esta é uma alternativa interessante a distância do cosseno, visto que esta métrica respeita a desigualdade triangular."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-impe",
    "href": "presentations/tcc2/slide.html#referencial-teórico-impe",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — IMPE",
    "text": "Referencial teórico — IMPE\n\nUma terceira métrica foi proposta para a medição da similaridade entre os processos, a qual se chamou Interseção mínima de probabilidade empírica (IMPE).\n\n\nDefiniçãoImplementação\n\n\n\n\nEsta métrica foi construída inspirada no índice de Tversky.\nNela, consideraremos os vetores dos textos como distribuições de probabilidade empírica. Destes, tomaremos um menos a soma das diferenças dos absolutos das frequência relativa de um termo entre dois textos, tal que \\[\\begin{equation}\n  \\text{IMPE}(\\textbf{A},\\textbf{B}) = 1 - \\sum_{w \\in A \\cap B} |f_A(w) - f_B(w)|.\n\\end{equation}\\]\nPor somar os menores valores, esta métrica foi construída para ser conservadora, apenas acusando similaridade entre textos muito parecidos.\n\\(IMPE(\\textbf{A},\\textbf{B}) \\in (0,1)\\), em que \\(IMPE(\\textbf{A},\\textbf{B}) = 1 \\rightarrow\\) textos idênticos.\n\n\n\n\n\n# Calcular interseção mínima de probabilidade empírica entre dois documentos ----\n# Entrada: 2 df n x 2, coluna 1 \"word\" coluna 2 \"freq\"\n\nIMPE &lt;- function(df1, df2) {\n  df1$relative_freq &lt;- df1$freq / sum(df1$freq)\n  df2$relative_freq &lt;- df2$freq / sum(df2$freq)\n  common_words &lt;- merge(df1, df2, by = \"word\", suffixes = c(\"_df1\", \"_df2\"))\n  common_words$dissimilaridade &lt;- abs(common_words$relative_freq_df1 - common_words$relative_freq_df2)\n  similaridade &lt;- 1 - sum(common_words$dissimilaridade)\n  return(similaridade)\n}"
  },
  {
    "objectID": "presentations/tcc2/slide.html#resultados",
    "href": "presentations/tcc2/slide.html#resultados",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Resultados",
    "text": "Resultados\n\nForam realizados estudos de casos, onde se comparou a combinação de cada técnica de vetorização com cada métrica de similaridade para casos onde se sabia os processos que deveriam ser identificados como mais parecidos. Estes foram os resultados:\n\n\nCaso 1Caso 2Caso 3Caso 4Caso 5\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI6931 e ADI6921.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n8º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n65º mais similar\n\n\nJensen-Shannon\n3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n7º mais similar\n\n\nIMPE\n4º mais similar\n\n\nJensen-Shannon\n72º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADPF857 e ADPF743, ADPF746.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 2º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n42º e 89º mais similar\n\n\nJensen-Shannon\n1º e 3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n8º e 9º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADO54 e ADPF760.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n4º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n36º mais similar\n\n\nJensen-Shannon\n3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n8º mais similar\n\n\nJensen-Shannon\n2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI7078 e ADI7066, ADI7070.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 6º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n437º e 475º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n3º e 4º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI3318 e ADI2943, ADI3309.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 5º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n3º e 4º mais similar\n\n\nIMPE\n358º e 360º mais similar\n\n\nJensen-Shannon\n2º e 3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n2º e 3º mais similar\n\n\nIMPE\n6º e 7º mais similar\n\n\nJensen-Shannon\n2º e 4º mais similar"
  },
  {
    "objectID": "presentations/tcc2/slide.html#resultados-t-sne",
    "href": "presentations/tcc2/slide.html#resultados-t-sne",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Resultados — t-SNE",
    "text": "Resultados — t-SNE\n\nTestou-se utilizar t-SNE para visualização do acervo, com objetivo de que as semelhanças entre processos fossem preservadas\n\n\n\nTeste 1\n\n\nTeste 2\n\n\nTeste 3\n\n\n\n\nComo os resultados não foram satisfatórios, esta técnica não foi utilizada na aplicação final."
  },
  {
    "objectID": "presentations/tcc2/slide.html#conclusão",
    "href": "presentations/tcc2/slide.html#conclusão",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Conclusão",
    "text": "Conclusão\n\n\nA combinação das técnicas mais simples de vetorização Bag of Words com a distância do cosseno produziu excelentes resultados na busca de similaridade de processos.\nEste resultado mostra que técnicas simples e parcimoniosas ainda são úteis para problemas práticos no paradigma da NLP.\nA implementação de um aplicativo Shiny para utilização deste modelo fornece um valioso insumo para os analistas do Tribunal para o cumprimento da meta de diminuição do acervo de processos de controle concentrado do Ministro Presidente do STF."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referências",
    "href": "presentations/tcc2/slide.html#referências",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referências",
    "text": "Referências\n\n\nFREITAS, L. J. G. Clusterização de textos aplicada ao tratamento de dados jurídicos desbalanceados. Tese (Mestrado em Estatística) – Departamento de estatística, Universidade de Brasília, 2023.\nFREITAS, L. J. G.; ALENCAR, E.; RODRIGUES, T. C. V. Rafa 2030-deep learning applied to brazilian supreme court legal documents and un 2030 agenda. Galoá, 2022.\nFREITAS, L. J. G. et al. Catboost algorithm application in legal texts and un 2030 agenda. Revista de Informatica Teórica e Aplicada - RITA - ISSN 2175-2745. Vol. 30, Num. 02 (2023) 51-58, 2023.\nFREITAS, L. J. G. et al. Text clustering applied to data augmentation in legal contexts. arXiv preprint arXiv:2404.08683, 2024.\nLU, J.; HENCHION, M.; NAMEE, B. M. Diverging divergences: Examining variants of jensen shannon divergence for corpus comparison tasks. 2021.\nMORETTIN, P. A.; SINGER, J. M. Estatística e Ciência de Dados. 1ª. ed. Rio de Janeiro: LTC, 2023.\nSOARES, A. Introdção à análise textual aplicada à sociologia. 2022. Disponível em: ⟨https://soaresalisson.github.io/analisetextual/⟩. Acesso em: 31 de dez. de 2024.\n\n\n\n\n\n\nDepartamento de estatística - UnB"
  }
]