[
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#introdução",
    "href": "presentations/xgboost_catboost/trabalho.html#introdução",
    "title": "XGBoost/Catboost",
    "section": "Introdução",
    "text": "Introdução\nXGBoost (eXtreme Gradient Boosting) (Tianqi Chen, 2016) é uma das implementações de three ensembling baseada em gradient boosting mais populares, por ser otimizado para desempenho e eficiência.\nA ideia do algoritmo gradient boosting é realizar ensemble de M classificadores fracos para obter um modelo robusto que faça boas previsões. Estes classificadores serão árvores de decisão. Porém, diferentemente do AdaBoost estudado anteriormente, a regularização é feita a partir da minimização de uma função de perda diferenciável, e não pela atribuição de maior peso a instâncias classificadas erroneamente."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#introdução-1",
    "href": "presentations/xgboost_catboost/trabalho.html#introdução-1",
    "title": "XGBoost/Catboost",
    "section": "Introdução",
    "text": "Introdução\nPor se tratar de um modelo sequencial, em que a árvore \\(i+2\\) necessitará da informação dos resíduos da árvore \\(i+1\\), também herda a restrição de paralelismo que outros modelos de three ensembling também contém. Porém, a implementação do algoritmo XGBoost busca paralelizar todas as rotinas necessárias para construção do modelo, tornando sequencial apenas o início da construção de cada árvore.\n\n\n\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#gradient-boosting",
    "href": "presentations/xgboost_catboost/trabalho.html#gradient-boosting",
    "title": "XGBoost/Catboost",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting\nO algoritmo base do XGBoost é o Gradient Boosting, no qual busca-se minimizar uma função de perda utilizando gradiente descendente. Esta otimização não será numérica, mas sim fornecida pela combinação das árvores geradas (boosting), que irão conduzir o gradiente para o mínimo global da função.\nCada um desses aprendizes fracos sequenciais irão focar nos resíduos da árvore anterior, buscando a divisão que minimize a função de perda definida (podendo ser default ou escolhida pelo usuário)."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#gradient-boosting-1",
    "href": "presentations/xgboost_catboost/trabalho.html#gradient-boosting-1",
    "title": "XGBoost/Catboost",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting\nSeja M o número de árvores fixadas para construção do modelo, o modelo final será dado pela fórmula \\[F_M(x) = F_0(x) +\\sum^M_{m=1}F_m(x)\\] Em que \\(F_M(x)\\) será o modelo final para predições, \\(F_0(x)\\) será um modelo inicial de árvore, \\(F_1(x)\\) será o modelo que minimiza os resíduos do modelo \\(F_0(x)\\); \\(F_2(x)\\) será o modelo que minimiza os resíduos do modelo \\(F_1(x)\\), e assim por diante até o modelo \\(F_{m}(x)\\). Portanto, a classificação da i-ésima instância será dada por:\\[\\hat{y_i} = F_M(x_i)\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#construção-das-árvores",
    "href": "presentations/xgboost_catboost/trabalho.html#construção-das-árvores",
    "title": "XGBoost/Catboost",
    "section": "Construção das árvores",
    "text": "Construção das árvores\nSalvo a primeira árvore, que será um chute inicial, iremos definir uma função objetivo com sendo\\[obj = \\sum^n_{i=1}l(y_i,\\hat{y}_i^{(t)})+\\sum^t_{i=1}\\Omega(f_i),\\] em que o termo \\(l(y_i,\\hat{y}_i^{(t)})\\) irá computar a perda relativa no t-ésimo passo, e \\(\\Omega(f_i)\\) será o t-ésimo termo de regularização, e \\(obj\\) será baseado nos resíduos da árvore anterior.\nO algoritmo escolhe a cada passo a árvore que minimiza \\(obj\\) com base nas features selecionadas automaticamente pelo modelo."
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#under-the-hood",
    "href": "presentations/xgboost_catboost/trabalho.html#under-the-hood",
    "title": "XGBoost/Catboost",
    "section": "Under the hood",
    "text": "Under the hood\nPara a seleção de features que irão minimizar a função objetivo, o algoritmo irá avaliar diversas possíveis divisões em diferentes features, construindo entre outras coisas histogramas para as features que ajudem a encontrar o melhor ponto de divisão. Para isso, o algoritmo utiliza do paralelismo, executando diversas divisões e encontrando a melhor possível de forma iterativa e rápida.\nDesta forma, apesar de se tratar de um modelo sequencial, o algoritmo implementado utiliza em todas as etapas que for possível da computação paralelizada, afim de aproveitar ao máximo o desempenho computacional e retornar um modelo que seja robusto e rápido\nPara maior aprofundamento teórico sobre o funcionamento do modelo, consulte XGBoost tutorial, seção 1.3"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo",
    "href": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo",
    "title": "XGBoost/Catboost",
    "section": "Flow do modelo",
    "text": "Flow do modelo\nUma forma simplificada de mostrar o funcionamento do XGBoost pode ser representada pelo seguinte grafo:\n\n\n\n\n\n\ngraph TD\n\n    A[Entrada de dados] --&gt; B\n    B[Preparação dos Dados] --&gt; B1 & B2 & B3\n    B1[Limpeza dos Dados]\n    B2[Normalização]\n    B3[Divisão Treino/Teste]\n    B1 & B2 & B3 --&gt; B_Final[Dados Preparados]\n\n    B_Final --&gt; C[Entrada de Dados no Modelo]\n\n    C --&gt; D[Construção da Primeira Árvore]\n    D --&gt; D1 & D2 & D3\n    D1[Avaliação das Divisões]\n    D2[Construção dos Nós]\n    D3[Combinação das Previsões]\n    D1 & D2 & D3 --&gt; D_Final[Árvore Construída]\n    \n    D_Final --&gt; E[Entrada de Dados Residual para a Segunda Árvore]\n\n    E --&gt; F[Construção da Segunda Árvore]\n    F --&gt; F1 & F2 & F3\n    F1[Avaliação das Divisões]\n    F2[Construção dos Nós]\n    F3[Combinação das Previsões]\n    F1 & F2 & F3 --&gt; F_Final[Segunda Árvore Construída]\n\n    F_Final --&gt; G[Repetir o processo anterior m vezes]\n\n    G --&gt; H[Avaliação do Modelo]\n    \n    H --&gt; I[Ensembling das Árvores com Pesos Relativos]\n\n    I --&gt; J[Modelo final]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#implementação-xgboost",
    "href": "presentations/xgboost_catboost/trabalho.html#implementação-xgboost",
    "title": "XGBoost/Catboost",
    "section": "Implementação XGBoost",
    "text": "Implementação XGBoost\nVamos à prática!"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#exemplo-prático-xgboost-catboost-outros",
    "href": "presentations/xgboost_catboost/trabalho.html#exemplo-prático-xgboost-catboost-outros",
    "title": "XGBoost/Catboost",
    "section": "Exemplo prático: XGBoost, CatBoost & outros",
    "text": "Exemplo prático: XGBoost, CatBoost & outros\nExemplo prático"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#categorical-boost---catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#categorical-boost---catboost",
    "title": "XGBoost/Catboost",
    "section": "Categorical Boost - CatBoost",
    "text": "Categorical Boost - CatBoost\n\nAlgoritmo de aprendizado de máquina\nModelo Ensemble\nÁrvores de decisão\nNão é necessário pré-processamento dos dados categóricos\nCada árvore reduz a perda comparada com a anterior\nRandom Permutations"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#target-encoding",
    "href": "presentations/xgboost_catboost/trabalho.html#target-encoding",
    "title": "XGBoost/Catboost",
    "section": "Target Encoding",
    "text": "Target Encoding\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {OptionCount+Prior}{n+1}\\]\nPrior: Palpite inicial arbitrário \nOption Count: número de vezes que alguém da mesma cor gostou de estatística anteriormente \nn: número de repetições da mesma cor já vistas"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {OptionCount+Prior}{n+1}\\]\nTarget Encoding \\[\\frac {0+0.05}{0+1} = 0.05\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-1",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-1",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {0+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {0.05}{2} = 0.025\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-2",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-2",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {1.05}{2} = 0.525\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-3",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-3",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{1+1}\\]\nTarget Encoding \\[\\frac {1.05}{2} = 0.525\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-4",
    "href": "presentations/xgboost_catboost/trabalho.html#processando-variáveis-categóricas-4",
    "title": "XGBoost/Catboost",
    "section": "Processando variáveis categóricas",
    "text": "Processando variáveis categóricas\n\n\n\n\n\n\n\n\nTarget Encoding \\[\\frac {1+0.05}{2+1}\\]\nTarget Encoding \\[\\frac {1.05}{3} = 0.35\\]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#árvores-de-decisão-simétricas",
    "href": "presentations/xgboost_catboost/trabalho.html#árvores-de-decisão-simétricas",
    "title": "XGBoost/Catboost",
    "section": "Árvores de decisão Simétricas",
    "text": "Árvores de decisão Simétricas\n\nYoutube: CatBoost Parte 2: Construindo e Usando Árvores\nGera modelos mais fracos, porém mais simples\nGera previsões mais rapidamente"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo-catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#flow-do-modelo-catboost",
    "title": "XGBoost/Catboost",
    "section": "Flow do modelo CatBoost",
    "text": "Flow do modelo CatBoost\nUma forma simplificada de mostrar o funcionamento do Catboost pode ser representada pelo seguinte grafo:\n\n\n\n\n\n\ngraph TD\n\n    A[Entrada de dados] --&gt; B\n    B[Preparação dos Dados] --&gt; B1 & B3\n    B1[Limpeza dos Dados]\n    B3[Divisão Treino/Teste]\n    B1 & B3 --&gt; B_Final[Dados Preparados]\n\n    B_Final --&gt; C[Entrada de Dados no Modelo]\n    \n    C[Entrada de Dados no Modelo] --&gt; K1[Random Permutations]\n    \n    K1[Random Permutations] --&gt; K2[Target Encoding]\n    \n    K2[Target Encoding] --&gt; D[Construção da Primeira Árvore]\n    D --&gt; D1 & D2 & D3\n    D1[Avaliação das Divisões]\n    D2[Construção dos Nós]\n    D3[Combinação das Previsões]\n    D1 & D2 & D3 --&gt; D_Final[Árvore Construída]\n    \n    D_Final --&gt; E[Entrada de Dados Residual para a Segunda Árvore]\n\n    E --&gt; F[Construção da Segunda Árvore]\n    F --&gt; F1 & F2 & F3\n    F1[Avaliação das Divisões]\n    F2[Construção dos Nós]\n    F3[Combinação das Previsões]\n    F1 & F2 & F3 --&gt; F_Final[Segunda Árvore Construída]\n\n    F_Final --&gt; G[Repetir o processo anterior m vezes]\n\n    G --&gt; H[Avaliação do Modelo]\n    \n    H --&gt; I[Ensembling das Árvores com Pesos Relativos]\n\n    I --&gt; J[Modelo final]"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#implementação-catboost",
    "href": "presentations/xgboost_catboost/trabalho.html#implementação-catboost",
    "title": "XGBoost/Catboost",
    "section": "Implementação CatBoost",
    "text": "Implementação CatBoost\nVamos à prática!"
  },
  {
    "objectID": "presentations/xgboost_catboost/trabalho.html#referências",
    "href": "presentations/xgboost_catboost/trabalho.html#referências",
    "title": "XGBoost/Catboost",
    "section": "Referências",
    "text": "Referências\nDocumentação XGBoost.\nPinheiro, João Manoel Herrera. Um estudo sobre Algoritmos de Boosting e a Otimização de Hiperparâmetros Utilizando Optuna. São Carlos, SP. 2023.\nDocumetação CatBoost\nChepenko, Introduction to gradient boosting on decision trees with Catboost\nCatBoost Parte 1: Codificação de destino ordenada\nCatBoost Parte 2: Construindo e Usando Árvores\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#introdução",
    "href": "presentations/random_forest/apresentacao.html#introdução",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Introdução",
    "text": "Introdução\n\nO que é floresta aleatória?\n\nFloresta aleatória é um algoritmo de Machine learning, utilizado para realizar predições, que combina o resultado de múltiplas árvores de decisão criadas aleatoriamente com o objetivo de diminuir a variância e o viés no contexto dos métodos de árvores."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#introdução-1",
    "href": "presentations/random_forest/apresentacao.html#introdução-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Introdução",
    "text": "Introdução\n\nÁrvore de decisão\n\nUma árvore de decisão é a forma mais básica entre os métodos baseados em árvore, muito utilizada em regressão e classificação, uma árvore de decisão consiste em segmentar o espaço composto pelas variáveis preditoras em regiões mais simples, nos quais a média (se a variável resposta for quantitativa) é utilizada como valor predito ou (caso categorizado) a classe da variável resposta com maior frequência, os modelos de árvores de decisão são atraentes pela simplicidade e fácil interpretação, contudo não possuem a precisão que outros métodos de classificação ou regressão alcançam."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens",
    "href": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Vantagens e desvantagens",
    "text": "Vantagens e desvantagens\n\nsobre árvore de decisão:\n\n\n\nVantagens\n\n\nSão facilmente explicaveis;\nPodem ser representadas graficamente com fácil interpretação;\nPodem manupular preditores qualitativos sem a necessidade de variáveis dummy.\n\n\n\nDesvantagens\n\n\nNão possuem o mesmo nível de precisão preditiva como outros modelos de regressão e classificação;\nNão são robustas, ou seja, uma pequena mudança nos dados pode gerar uma grande mudança na árvore de estimação final."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#motivação",
    "href": "presentations/random_forest/apresentacao.html#motivação",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Motivação",
    "text": "Motivação\nUma forma de superar a baixa precisão das árvores de decisão é a utilização de métodos agregadoes que ajustam modelos poderosos, como floresta aleatória, que consegue uma grande melhoria no poder de predição mesmo comparando com outros modelos de classificação."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#procedimento",
    "href": "presentations/random_forest/apresentacao.html#procedimento",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Procedimento",
    "text": "Procedimento\n\n\nComo na técnica bagging, construímos várias árvores a partir das amostras bootstrap do conjunto de testes.\nNa construção de cada árvore, cada vez que uma divisão (ou corte) é considerada para alguma árvore, uma seleção aleatória dos preditores é escolhida como candidatos dos cortes ao invés de todos os preditores como normalmente é feito.\nEssa abordagem tem como propósito reduzir a correlação entre as árvores, reduzindo a variância quando tiramos a média das árvores, já que a média tende a ser menor quando temos menor correlação entre as árvores.\nA escolha do número de preditores que serão selecionados para cada corte, é tipicamente escolhida pela raiz quadrada do número total de preditores.\nFlorestas aleatórias fornecem uma melhoria em relação ao método bagging, já que a correlação entre as árvores diminui."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#considerações",
    "href": "presentations/random_forest/apresentacao.html#considerações",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Considerações",
    "text": "Considerações\n\nDe forma geral, pode-se dizer que o procedimento introduz mais aleatoriedade e diversidade no processo de construção em relação ao método bagging.\nIntuitivamente, a utilização de florestas aleatórias para tomada de decisão corresponde à síntese da opinião de indivíduos com diferentes fontes de informação.\nEm geral, florestas aleatórias produzem resultados menos variáveis em relação ao método bagging, já que nesse método as árvores geradas podem ser muito semelhantes, dependendo de preditores fortes, o que não contribui para redução de variabilidade das predições, o que não acontece com florestas aleatórias."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#rotina",
    "href": "presentations/random_forest/apresentacao.html#rotina",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Rotina",
    "text": "Rotina\nUma rotina minimalista de classificação via Random Forest em R pode ser executada da seguinte forma:\n\n\n\nlibrary(randomForest)\n\ndata &lt;- iris\n\ndata$Species &lt;- as.factor(data$Species)\n\nset.seed(150167636)\nind &lt;- sample(2, nrow(data), replace = TRUE,\n              prob = c(0.7, 0.3))\ntrain &lt;- data[ind==1,]\ntest &lt;- data[ind==2,]\n\nrf &lt;- randomForest(Species~., data=train, proximity=TRUE)\n\n\n\n\n\nCall:\n randomForest(formula = Species ~ ., data = train, proximity = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6.36%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         33          0         0  0.00000000\nversicolor      0         36         3  0.07692308\nvirginica       0          4        34  0.10526316"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#resultados",
    "href": "presentations/random_forest/apresentacao.html#resultados",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Resultados",
    "text": "Resultados\nPodemos acessar os resultados do modelo no objeto rf\n\n\nTeste do modelo no conjunto de treino:\n\nlibrary(caret)\n\np1 &lt;- predict(rf, train)\nconfusionMatrix(p1, train$ Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         33          0         0\n  versicolor      0         39         0\n  virginica       0          0        38\n\nOverall Statistics\n                                    \n               Accuracy : 1         \n                 95% CI : (0.967, 1)\n    No Information Rate : 0.3545    \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                    1.0            1.0000           1.0000\nSpecificity                    1.0            1.0000           1.0000\nPos Pred Value                 1.0            1.0000           1.0000\nNeg Pred Value                 1.0            1.0000           1.0000\nPrevalence                     0.3            0.3545           0.3455\nDetection Rate                 0.3            0.3545           0.3455\nDetection Prevalence           0.3            0.3545           0.3455\nBalanced Accuracy              1.0            1.0000           1.0000\n\n\n\nValidação do modelo nos dados de teste:\n\np2 &lt;- predict(rf, test)\nconfusionMatrix(p2, test$ Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         2\n  virginica       0          1        10\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8854          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.8333\nSpecificity                  1.000            0.9310           0.9643\nPos Pred Value               1.000            0.8333           0.9091\nNeg Pred Value               1.000            0.9643           0.9310\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2500\nDetection Prevalence         0.425            0.3000           0.2750\nBalanced Accuracy            1.000            0.9201           0.8988"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#parâmetros",
    "href": "presentations/random_forest/apresentacao.html#parâmetros",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Parâmetros",
    "text": "Parâmetros\n\nA função randomForest do pacote homônimo tem uma série de parâmetros opcionais além do mínimo obrigatório, que seria o modelo e os dados. O mais importante destes parâmetros é o ntree, que por default é 500 e em geral deve-se utilizar o máximo possível tal que execute em um tempo aceitável. Em geral, o restante dos parâmetros deve ser deixado em default.\nNeste caso, o modelo foi extremamente eficiente mesmo na versão minimalista"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#fine-tuning",
    "href": "presentations/random_forest/apresentacao.html#fine-tuning",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nSe for o caso, também podemos fazer o fine-tuning dos parâmetros do modelo\n\n\n\n\nt &lt;- tuneRF(train[,-5], train[,5],\n       stepFactor = 0.5,\n       plot = TRUE,\n       ntreeTry = 150,\n       trace = TRUE,\n       improve = 0.05)\n\nmtry = 2  OOB error = 5.45% \nSearching left ...\nmtry = 4    OOB error = 5.45% \n0 0.05 \nSearching right ...\nmtry = 1    OOB error = 5.45% \n0 0.05 \n\n\n\n\n\n\n\n\n\n\n\nhist(treesize(rf),\n     main = \"No. of Nodes for the Trees\",\n     col = \"green\")"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#importâncias",
    "href": "presentations/random_forest/apresentacao.html#importâncias",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Importâncias",
    "text": "Importâncias\n\nPodemos verificar a importância de cada variável para o modelo.\n\n\n\n\nvarImpPlot(rf,\n           sort = T,\n           n.var = 10,\n           main = \"Top 10 - Variable Importance\")\n\n\n\n\n\n\n\nimportance(rf)\n\n             MeanDecreaseGini\nSepal.Length         7.299621\nSepal.Width          2.006132\nPetal.Length        35.799244\nPetal.Width         27.318058"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#implementações",
    "href": "presentations/random_forest/apresentacao.html#implementações",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Implementações",
    "text": "Implementações\n\nUma rotina de classificação via Random forest poderia ser executada de forma análoga em python da seguinte forma:\n\n\n\n\nlibrary(reticulate)\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\n\niris = datasets.load_iris()\ndados = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n\nX = dados\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\ny_pred = model.predict(X_test)\nconfusion_matrix(y_test, y_pred)\n\narray([[15,  0,  0],\n       [ 0, 14,  0],\n       [ 0,  3, 13]], dtype=int64)"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições",
    "href": "presentations/random_forest/apresentacao.html#definições",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\nAs árvores podem ser representadas como \\(h_1(\\boldsymbol x),\\;  h_2(\\boldsymbol x),\\; . . . ,\\; h_K( \\boldsymbol x)\\) na forma\n\\[ \\{ h \\left( \\boldsymbol x, \\Theta_k \\right), \\; \\; k = 1, \\dots \\}  \\]\nonde \\(\\Theta_k \\; \\text{i.i.d}\\) são vetores aleatórios representando a escolha de \\(p\\) entre os \\(m\\) atributos de \\(\\boldsymbol X\\).\nNormalmente, \\(p \\approx \\sqrt m\\)"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições-1",
    "href": "presentations/random_forest/apresentacao.html#definições-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\n1) A medida em que o número de árvores cresce, a média de acertos se estabiliza e a chance de cometer uma predição errada pode ser quantificada:\n\\[  P_{\\boldsymbol X, Y} \\left(   P_{\\Theta}( h(\\boldsymbol X, \\Theta) = Y)   - P_{\\Theta} (h(\\boldsymbol X, \\Theta)  \\neq Y) &lt; 0\\right) \\]\n\n\\(P_{\\Theta}( h(\\boldsymbol X, \\Theta) = Y)\\) representa a probabilidade de que uma árvore acerte a predição de Y"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#definições-2",
    "href": "presentations/random_forest/apresentacao.html#definições-2",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Definições",
    "text": "Definições\n2) A acurácia da random forest vai depender do “poder” de cada um dos classificadores individuais e da dependência entre eles.\nUm limite superior para o erro de generalização é dado por\n\\[PE^* \\le -\\bar \\rho(1 − s^2)/s^2 \\]\nonde:\n\n\\(\\boldsymbol s = E_{\\boldsymbol X, Y} mr({\\boldsymbol X, Y} )\\) é o “poder” das árvores \\(h(\\boldsymbol x, \\Theta)\\)\n\\(\\bar \\rho\\) pode ser entendido como a média entre as correlações das árvores."
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#exemplo-alzheimer",
    "href": "presentations/random_forest/apresentacao.html#exemplo-alzheimer",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Exemplo Alzheimer",
    "text": "Exemplo Alzheimer\nO dataset DARWIN (https://archive.ics.uci.edu/dataset/732/darwin) contém dados sobre a escrita a mão de pessoas afetadas pelo Alzheimer e de um grupo de controle, totalizando 174 observações. São 450 variáveis e o objetivo é distinguir pessoas afetadas (P) de pessoas saudáveis (H).\n\n\n\nset.seed(123)\n\nrf_model &lt;- randomForest(\n  class ~ ., data = train_data, \n  ntree = 500, \n  importance = TRUE\n  )\n\n\nNo. of variables tried at each split: 21\nOOB estimate of  error rate: 13.57%\n\nConfusion matrix:\n   H  P class.error\nH 58 10   0.1470588\nP  9 63   0.1250000\n\n\n   H  P class.error\nH 58 10   0.1470588\nP  8 64   0.1111111\n\n\n[1] \"Test error:8.82%\""
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#section",
    "href": "presentations/random_forest/apresentacao.html#section",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "",
    "text": "No gráfico abaixo é possível perceber como a escolha do número de variáveis em cada split faz diferença para o resultado final do modelo.\n\n\n\n\n\n\n\n\n\n\nAinda, podemos verficiar a acurácia do modelo pelo número de árvores:"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\nUma das vantagens das florestas aleatórias é sua robustez a pontos atípicos, ou outliers. O exemplo a seguir demonstra a robustez desses modelos a contaminações, além de compará-los a outros métodos de classificação:\n\npacman::p_load(randomForest)\npacman::p_load(caret,e1071,VGAM)\n\niris &lt;- iris %&gt;%\n  mutate(cor = ifelse(Species == \"setosa\",1,ifelse(Species == \"versicolor\",2,3)))\n\nset.seed(150167636)\nind &lt;- sample(2, nrow(iris), replace = TRUE,\n              prob = c(0.7, 0.3))\ntrain &lt;- iris[ind==1,]\ntest &lt;- iris[ind==2,]\n\n\n\ni=4 #Número de pontos contaminados\ndadosPoluidos1 &lt;- train[train$cor==1,]\ndadosPoluidos1 &lt;- dadosPoluidos1[sample(1:nrow(dadosPoluidos1),i,replace = F),]\n\ndadosPoluidos2 &lt;- train[train$cor==3,]\ndadosPoluidos2 &lt;- dadosPoluidos2[sample(1:nrow(dadosPoluidos2),i,replace = F),]\n\ndadosPoluidos1$Petal.Length &lt;- dadosPoluidos1$Petal.Length + 5\ndadosPoluidos1$Petal.Width &lt;- dadosPoluidos1$Petal.Width + 1.7\n\ndadosPoluidos2$Petal.Length &lt;- dadosPoluidos2$Petal.Length - 4\ndadosPoluidos2$Petal.Width &lt;- dadosPoluidos2$Petal.Width - 1.2\n\nDadosExempOutTreino &lt;- rbind(train,dadosPoluidos1,dadosPoluidos2)\n#DadosExempOutTreino &lt;- DadosExempOutTreino[,-6]"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-1",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-2",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-2",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n\nRandom Forest\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         2\n  virginica       0          1        10\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8854          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.8333\nSpecificity                  1.000            0.9310           0.9643\nPos Pred Value               1.000            0.8333           0.9091\nNeg Pred Value               1.000            0.9643           0.9310\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2500\nDetection Prevalence         0.425            0.3000           0.2750\nBalanced Accuracy            1.000            0.9201           0.8988\n\n\n\n\nRegressão logística\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0          6         1\n  virginica       0          5        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.85            \n                 95% CI : (0.7016, 0.9429)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.669e-08       \n                                          \n                  Kappa : 0.7697          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.5455           0.9167\nSpecificity                  1.000            0.9655           0.8214\nPos Pred Value               1.000            0.8571           0.6875\nNeg Pred Value               1.000            0.8485           0.9583\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.1500           0.2750\nDetection Prevalence         0.425            0.1750           0.4000\nBalanced Accuracy            1.000            0.7555           0.8690"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-3",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-3",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n\nSVM linear\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0          9         1\n  virginica       0          2        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.925           \n                 95% CI : (0.7961, 0.9843)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 3.546e-11       \n                                          \n                  Kappa : 0.8852          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.8182           0.9167\nSpecificity                  1.000            0.9655           0.9286\nPos Pred Value               1.000            0.9000           0.8462\nNeg Pred Value               1.000            0.9333           0.9630\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2250           0.2750\nDetection Prevalence         0.425            0.2500           0.3250\nBalanced Accuracy            1.000            0.8918           0.9226\n\n\n\n\nSVM radial\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         17          0         0\n  versicolor      0         10         1\n  virginica       0          1        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.95            \n                 95% CI : (0.8308, 0.9939)\n    No Information Rate : 0.425           \n    P-Value [Acc &gt; NIR] : 2.026e-12       \n                                          \n                  Kappa : 0.9235          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                  1.000            0.9091           0.9167\nSpecificity                  1.000            0.9655           0.9643\nPos Pred Value               1.000            0.9091           0.9167\nNeg Pred Value               1.000            0.9655           0.9643\nPrevalence                   0.425            0.2750           0.3000\nDetection Rate               0.425            0.2500           0.2750\nDetection Prevalence         0.425            0.2750           0.3000\nBalanced Accuracy            1.000            0.9373           0.9405"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-4",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-4",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\nComparando robustez de modelos com observações contaminadas\nSem contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-5",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-5",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n2 contaminações\n\n\n\n\n\n\n\n\n\n\n4 contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-6",
    "href": "presentations/random_forest/apresentacao.html#robustez-a-dados-contaminados-6",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Robustez a Dados Contaminados",
    "text": "Robustez a Dados Contaminados\n\n\n6 contaminações\n\n\n\n\n\n\n\n\n\n\n8 contaminações"
  },
  {
    "objectID": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens-1",
    "href": "presentations/random_forest/apresentacao.html#vantagens-e-desvantagens-1",
    "title": "Técnica Random Forest em árvores de decisão",
    "section": "Vantagens e desvantagens",
    "text": "Vantagens e desvantagens\n\nsobre classificação por florestas aleatórias:\n\n\n\nVantagens\n\n\nRobusto contra overfitting;\nTrabalha bem com dados de alta dimesão;\nConsegue captar relações não-lineares nos dados;\nFornece uma medida de importância;\nRobusto contra outliers e ruídos;\nConsegue lidar com dados faltantes.\n\n\n\nDesvantagens\n\n\nDificil interpretação;\nNão adequado para dados escassos;\nDemora para fazer predições;\nRequer ajuste de hiper-parâmetros."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html",
    "href": "presentations/estocasticos/estocasticos.html",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "",
    "text": "Modelagem de dados e previsão de eventos futuros tem sido o garantidor do mercado de trabalho em estatística estar aquecido. Este tipo de inteligência aplicada à negócios vem se tornando essencial em qualquer setor, inclusive no poder Judiciário. Apesar deste não depender de faturamento para sua existência por ser inalienável ao setor público, tem sido do interesse dos próprios tribunais saber indicadores e estatísticas relacionadas ao próprio tribunal. E a modelagem surge neste contexto para agregar valor às decisões estratégicas deste poder.\nBuscou-se trabalhos de temas semelhantes, que visam modelar a quantidade de decisões em processos em tribunais. Entretanto, não consegui localizar nenhum; motivo esse talvez por ser tema sensível, no sentido de poder levar indiretamente a comparação de rendimentos entre juízes em um tribunal, seja pela possível falta de pertinência do assunto, seja pela tentativa inédita de modelar algo do tipo.\nEntretando, buscou-se trabalhos que modelam outros tipos de contagem usando de processo de Poisson não homogêneo, como (DINCER;DEMIR;YALÇIN,2022).\nNa metodologia, com base no livro ROSS (2010), apresento toda a metodologia e suporte teórico adotado na seção de análises. Na seção aplicação computacional iremos trazer uma possível solução simulada do problema proposto, e na seção de resultados apresento as conclusões gerais."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#dia-útil",
    "href": "presentations/estocasticos/estocasticos.html#dia-útil",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Dia útil",
    "text": "Dia útil\nPor mais hiperbólico que pareça, não foi possível localizar uma definição fechada de dia útil. Portanto, consideraremos como dia útil, todos os dias do ano excetuado os feriados, finais de semana (sábado e domingo) e recessos coletivos. Pegando como base o ano de 2023, iríamos considerar como dia útil para o tribunal selecionado para a aplicação computacional todos os dias com excessão de:\n\nTodos os sábados e todos os domingos do ano;\nDias 01/01/2023 a 06/01/2023 - Recesso Forense;\nDias 20/02/2023 a 22/02/2023 - Carnaval & quarta-feira de cinzas;\nDias 5/04/2023 a 7/04/2023 - Quarta e quinta-feira Santa e Paixão de Cristo;\nDia 21/04/2023 - Tiradentes;\nDia 1/05/2023 - Dia do trabalhador;\nDia 08/06/2023 - Corpus Christi;\nDia 11/08/2023 - Dia do Advogado;\nDia 7/09/2023 - Independência;\nDia 12/10/2023 - Dia de Nossa Senhora Aparecida;\nDias 01/11/2023 e 02/11/2023 - Dia de Todos os Santos e Finados;\nDia 15/11/2023 - Proclamação da República;\nDia 08/12/2023 - Dia da Justiça;\nDias 20/12/2023 a 31/12/2023 - Recesso Forense.\n\nNote que feriados que se sobrepõe ou que são aos finais de semana, só foram anotados uma vez, para evitar confusão. Note ainda que é trivial inserir as férias individuais de juízes e ministros desta forma.\nPortanto, se fixado o ano, pode-se pensar no intervalo \\((0,t]\\) como sendo o primeiro instante do primeiro dia do ano até o último instante do último dia do ano, podemos ordenar os dias por índice, tal que o primeiro dia do ano (1º de janeiro) representa o dia 1, o dia 1º de Fevereiro representa o dia 32, até o dia 31 de dezembro, representando o dia 365. O restante dos dias são dias úteis.\nCom isso, teremos 134 dias que não são dias úteis no ano de 2023, sendo estes os dias de índice: 1, 2, 3, 4, 5, 6, 7, 8, 14, 15, 21, 22, 28, 29, 35, 36, 42, 43, 49, 50, 51, 52, 53, 56, 57, 63, 64, 70, 71, 77, 78, 84, 85, 91, 92, 95, 96, 97, 98, 99, 105, 106, 111, 112, 113, 119, 120, 121, 126, 127, 133, 134, 140, 141, 147, 148, 154, 155, 159, 161, 162, 168, 169, 175, 176, 182, 183, 189, 190, 196, 197, 203, 204, 210, 211, 217, 218, 223, 224, 225, 231, 232, 238, 239, 245, 246, 250, 252, 253, 259, 260, 266, 267, 273, 274, 280, 281, 285, 287, 288, 294, 295, 301, 302, 305, 306, 308, 309, 315, 316, 319, 322, 323, 329, 330, 336, 337, 342, 343, 344, 350, 351, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364 e 365. Iremos chamar o vetor que contém estes valores acima de \\(\\textbf{v}_{dnu}\\), sendo seu complementar \\(\\textbf{v}_{du}\\) o vetor que contém todos os dias de 1 a 365 não contidos no vetor acima, representando assim os dias úteis (\\(du\\))."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#processo-de-contagem",
    "href": "presentations/estocasticos/estocasticos.html#processo-de-contagem",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Processo de contagem",
    "text": "Processo de contagem\nUm processo de contagem, definido por DINCER;DEMIR;YALÇIN (2022 apud BAS,2019) , é um processo estocástico, em que \\(M(t),t \\geq 0\\) é não negativo, com suporte em \\(\\mathbb{Z}\\) não decrescente \\(\\forall t \\geq 0\\). \\(M(t)\\) é o número total de eventos que ocorrem até o tempo \\(t\\), e \\(M(t,t+h)=M(t+h)-M(t)\\) denota o número de eventos ocorridos no intervalo \\((t,t+h],h&gt;0\\). Além disso, um processo de contagem, em que o número de eventos ocorridos em intervalos de tempo disjuntos são independentes e tem incrementos independentes. Um processo de contagem tem incrementos estacionários se a distribuição de \\(M(t,t+h)\\) depende apenas do comprimento do intervalo de tempo."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#variável-aleatória-poisson",
    "href": "presentations/estocasticos/estocasticos.html#variável-aleatória-poisson",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Variável aleatória Poisson",
    "text": "Variável aleatória Poisson\nSe \\(M(t)\\) denota o número de eventos que ocorrem num intervalo de tempo específico, este é chamado variável aleatória Poisson. A variável aleatória Poisson possui função de massa de probabilidade:\n\\[\\begin{equation}\n    \\mathbb{P}(M(t)=k)=\\frac{e^{-\\lambda_t}\\lambda_t^k}{k!}\n\\end{equation}\\]\nonde \\(\\lambda_t\\) é o parâmetro da distribuição Poisson e representa a taxa de eventos que ocorrem no intervalo de tempo especificado, além de ser a variância e esperança de \\(M(t)\\) ser igual a \\(\\lambda_t\\)."
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#processo-de-poisson",
    "href": "presentations/estocasticos/estocasticos.html#processo-de-poisson",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Processo de Poisson",
    "text": "Processo de Poisson\nUm processo de Poisson é definido por ROSS(2010) como:\n\\[\\begin{equation} \\label{eq1}\nN(t)  \\stackrel{}{\\sim} Poisson(m_(t)), i=1,2,...,n.\n\\end{equation}\\]\nE:\n\\[\\begin{equation}\n    \\mathbb{P}\\{N(t) = x\\} = \\frac{[m(t)]^{x}}{x!}e^{-m(t)}\n\\end{equation}\\]\nOnde \\(\\frac{o(h)}{h} \\rightarrow 0, h \\rightarrow 0\\)\n\nProcesso de Poisson homogêneo\nSe o processo de Poisson \\(M(t)\\) têm função de intensidade constante \\(\\lambda_t=\\lambda \\ \\forall \\ (t,t+s]\\), então este é chamado um processo de Poisson homogêneo. (DINCER;DEMIR;YALÇIN,2022) \n\n\nProcesso de Poisson não homogêneo\nSe o processo de Poisson \\(M(t)\\) tem função de intensidade variando com o tempo, este é um Processo de Poisson não homogêneo. A função massa de probabilidade deste processo é definida como:\n\\[\\begin{equation}\n    \\mathbb{P}(M(t,t+h)=x)=\\frac{\\left( \\int_t^{t+h}\\lambda(z)dz \\right)^x}{x!}e^{-\\int_t^{t+h}\\lambda(z)dz}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juiz-em-um-mês",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juiz-em-um-mês",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados por um juiz em um mês?",
    "text": "Quantos processos são julgados por um juiz em um mês?\nTomando Janeiro de 2023 (dias 1 a 31) como exemplo, o número de processos julgados pelo i-ésimo juiz será uma variável aleatória Poisson, com média:\n\\[\\begin{equation} \\label{eq3}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=31}\\Lambda_i(s)ds \\ ; \\{s = 30\\} \\\\\n    = \\lambda_i\\int_{t=0}^{t=8}\\Lambda_{i(2)}ds \\\\ + \\ \\lambda_i\\int_{t=8}^{t=13}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=13}^{t=15}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=15}^{t=20}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=20}^{t=22}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=22}^{t=27}\\Lambda_{i(1)}ds\\\\+ \\ \\lambda_i\\int_{t=27}^{t=29}\\Lambda_{i(2)}ds\\\\+ \\ \\lambda_i\\int_{t=29}^{t=31}\\Lambda_{i(1)}ds\\\\\n\\end{split}\n\\end{equation}\\]\nNote que podemos chamar uma variável \\(z\\) para substituir \\(t\\) nos limites de integração, afim de não precisar respeitar a ordenação dos dias e considerar apenas a passagem de tempo, tal que:\n\\[\\begin{equation}\n\\begin{cases}\n    t = i \\rightarrow \\text{i-ésimo dia} \\\\\n    z = i \\rightarrow \\text{i-dias}\n\\end{cases}\n\\end{equation}\\]\nPortanto, teremos\n\\[\\begin{equation} \\label{eq4}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=31}\\lambda_i(s)ds &= \\lambda_i \\left( \\int_{z=0}^{z=17}\\Lambda_{i(1)}ds \\right) + \\lambda_i \\left( \\int_{z=17}^{z=31}\\Lambda_{i(2)}ds \\right)\\\\\n    &= \\lambda_i \\left[ \\left( \\int_{z=0}^{z=17}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=17}^{z=31}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juíz-em-um-ano",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-por-um-juíz-em-um-ano",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados por um juíz em um ano?",
    "text": "Quantos processos são julgados por um juíz em um ano?\nExpandindo o raciocínio empregado anteriormente, podemos dizer que a quantidade de processos julgados pelo i-ésimo juiz no ano de 2023, com tempo \\(t \\in \\textbf{v}_{du} \\cup \\textbf{v}_{dnu}\\) será Poisson com média:\n\\[\\begin{equation} \\label{eq5}\n\\begin{split}\n    \\lambda_i\\int_{t=0}^{t=365}\\lambda_i(s)ds\n    &= \\lambda_i \\left[ \\left( \\int_{z=0}^{z=231}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=231}^{z=365}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-em-um-tribunal-em-um-ano",
    "href": "presentations/estocasticos/estocasticos.html#quantos-processos-são-julgados-em-um-tribunal-em-um-ano",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Quantos processos são julgados em um tribunal em um ano?",
    "text": "Quantos processos são julgados em um tribunal em um ano?\nFinalmente, podemos responder nossa pergunta inicial, aproveitando dos resultados das equações anteriores, temos que a quantidade de processos julgados em um tribunal (N(t=365)) será Poisson com média:\n\\[\\begin{equation} \\label{eq6}\n\\begin{split}\n    N(t=365) = \\sum_{i=1}^{n} \\lambda_i \\left[ \\left( \\int_{z=0}^{z=231}\\Lambda_{i(1)}ds \\right)  + \\left( \\int_{z=231}^{z=365}\\Lambda_{i(2)}ds \\right) \\right]\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/estocasticos/estocasticos.html#referências",
    "href": "presentations/estocasticos/estocasticos.html#referências",
    "title": "Processos de Poisson não homogêneos de n tipos",
    "section": "Referências",
    "text": "Referências\n\nRoss, S. M. (2010). Introduction to Probability Models. Academic Press.\nDincer, E., Demir, S. M., & Yalçın, B. A. (2022)."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "",
    "text": "Show the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, readxl, janitor,xgboost,tidymodels,vip,rpart.plot,\n               conflicted,tictoc,finetune,doParallel,DataExplorer,future,glmnet,\n               data.table,summarytools,knitr,compareGroups,bonsai,discrim,\n               baguette,naivebayes)\n\nregisterDoParallel(cores = parallel::detectCores())\n\ntidymodels_prefer()\ndf = read_csv(\"data.csv\")"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelagem",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelagem",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelagem",
    "text": "Modelagem\nNum contexto em que temos tantas covariáveis, tantas observações e, apesar de alguns indicativos observados na análise exploratória, não é possível observar um padrão óbvio que indique o salário do indivíduo. Para isso, utilizei do recurso da modelagem com apoio computacional afim de tornar possível esta análise. Um diferencial deste trabalho é a utilização do framework tidymodels, que é bastante verborrágico e permite uma compreensão das etapas do modelo pela leitura do código, além de eficiência e praticidade."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#parâmetros-gerais",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#parâmetros-gerais",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Parâmetros gerais",
    "text": "Parâmetros gerais\nIrei testar diversos modelos e fazer comparação de resultados destes, mas utilizarei a mesma receita para todos.\n\n\nShow the code\nset.seed(150167636)\nsplit = initial_split(df, prop = .8, strata = salary)\ntrain = training(split)\ntest = testing(split)\n\ncv_folds &lt;- vfold_cv(train, \n                     v = 5, \n                     strata = salary)\n\nrecipe &lt;- recipe(salary ~ .,\n                 data = train) %&gt;% \n  update_role(fnlwgt, new_role = \"case_weight\") %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ starts_with(\"occupation\"):starts_with(\"race\") + \n                  starts_with(\"occupation\"):starts_with(\"sex\") +\n                  starts_with(\"hours_per_week\"):starts_with(\"sex\"))\n\ndados_preparados &lt;- recipe %&gt;% \n  prep() %&gt;% \n  juice()\nkable(head(dados_preparados))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation_num\ncapital_gain\ncapital_loss\nhours_per_week\nsalary\nworkclass_not_working\nworkclass_private\nworkclass_self_employed\nworkclass_unknown\neducation_low_education\noccupation_Others\noccupation_services\noccupation_white_collar\nrace_Asian.Pac.Islander\nrace_Black\nrace_Other\nrace_White\nsex_Male\ncountry_United.States\ncivil_status_Single\noccupation_Others_x_race_Asian.Pac.Islander\noccupation_Others_x_race_Black\noccupation_Others_x_race_Other\noccupation_Others_x_race_White\noccupation_services_x_race_Asian.Pac.Islander\noccupation_services_x_race_Black\noccupation_services_x_race_Other\noccupation_services_x_race_White\noccupation_white_collar_x_race_Asian.Pac.Islander\noccupation_white_collar_x_race_Black\noccupation_white_collar_x_race_Other\noccupation_white_collar_x_race_White\noccupation_Others_x_sex_Male\noccupation_services_x_sex_Male\noccupation_white_collar_x_sex_Male\nsex_Male_x_hours_per_week\n\n\n\n\n0.0314592\n1.25e-05\n1.1354862\n0.1510791\n-0.2161513\n-0.0353722\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n-0.0353722\n\n\n0.8376443\n1.35e-05\n1.1354862\n-0.1461666\n-0.2161513\n-2.2198774\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n-2.2198774\n\n\n-0.0418303\n3.49e-05\n-0.4209394\n-0.1461666\n-0.2161513\n-0.0353722\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.0353722\n\n\n-0.7747258\n5.48e-05\n1.1354862\n-0.1461666\n-0.2161513\n-0.0353722\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0.0000000\n\n\n0.7643547\n2.59e-05\n-1.9773649\n-0.1461666\n-0.2161513\n-1.9771546\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n\n\n-1.1411735\n1.98e-05\n1.1354862\n-0.1461666\n-0.2161513\n-0.8444482\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0.0000000\n\n\n\n\n\nPodemos ver um pedaço da forma geral dos dados preparados para serem inseridos no modelo."
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-1-regressão-logística",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-1-regressão-logística",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 1: Regressão logística",
    "text": "Modelo 1: Regressão logística\nComo a variável resposta é binária, o primeiro modelo que podemos tentar seria o logístico\n\n\nShow the code\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(glm_spec)\n\nglm_fit &lt;- glm_wf %&gt;%\n  fit(data = train)\n\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.7372041\n0.4866916\n-3.5694143\n0.0003578\n\n\nage\n0.4055276\n0.0227845\n17.7984123\n0.0000000\n\n\neducation_num\n0.8441476\n0.0438024\n19.2717200\n0.0000000\n\n\ncapital_gain\n2.2344546\n0.0800493\n27.9134654\n0.0000000\n\n\ncapital_loss\n0.2691885\n0.0165243\n16.2904180\n0.0000000\n\n\nhours_per_week\n0.3348932\n0.0455127\n7.3582312\n0.0000000\n\n\nworkclass_not_working\n-11.7789355\n117.4336842\n-0.1003029\n0.9201039\n\n\nworkclass_private\n-0.0699743\n0.0559738\n-1.2501244\n0.2112541\n\n\nworkclass_self_employed\n-0.3486299\n0.0730037\n-4.7755074\n0.0000018\n\n\nworkclass_unknown\n-0.5554042\n1.8010975\n-0.3083699\n0.7578009\n\n\neducation_low_education\n0.1502829\n0.0759029\n1.9799356\n0.0477108\n\n\noccupation_Others\n0.1946984\n2.0025527\n0.0972251\n0.9225476\n\n\noccupation_services\n0.8077583\n0.8054694\n1.0028417\n0.3159372\n\n\noccupation_white_collar\n0.4311601\n0.5736424\n0.7516183\n0.4522806\n\n\nrace_Asian.Pac.Islander\n0.7500891\n0.5035234\n1.4896806\n0.1363082\n\n\nrace_Black\n0.4686787\n0.4650463\n1.0078109\n0.3135452\n\n\nrace_Other\n0.1107415\n0.6625542\n0.1671433\n0.8672573\n\n\nrace_White\n0.5799804\n0.4454281\n1.3020741\n0.1928910\n\n\nsex_Male\n0.1722064\n0.1804855\n0.9541286\n0.3400185\n\n\ncountry_United.States\n0.2594948\n0.0762643\n3.4025743\n0.0006675\n\n\ncivil_status_Single\n-2.4131329\n0.0517315\n-46.6472524\n0.0000000\n\n\noccupation_Others_x_race_Asian.Pac.Islander\n-1.1841893\n1.4582220\n-0.8120775\n0.4167472\n\n\noccupation_Others_x_race_Black\n-0.4225307\n1.3041276\n-0.3239949\n0.7459419\n\n\noccupation_Others_x_race_Other\n1.4529448\n1.6820682\n0.8637847\n0.3877062\n\n\noccupation_Others_x_race_White\n0.0266894\n1.2087874\n0.0220795\n0.9823846\n\n\noccupation_services_x_race_Asian.Pac.Islander\n-1.2872096\n0.8711872\n-1.4775350\n0.1395323\n\n\noccupation_services_x_race_Black\n-1.1793043\n0.8168805\n-1.4436680\n0.1488324\n\n\noccupation_services_x_race_Other\n-1.0749159\n1.4074752\n-0.7637192\n0.4450346\n\n\noccupation_services_x_race_White\n-1.1017327\n0.7831182\n-1.4068537\n0.1594708\n\n\noccupation_white_collar_x_race_Asian.Pac.Islander\n-0.3004502\n0.6078681\n-0.4942687\n0.6211164\n\n\noccupation_white_collar_x_race_Black\n0.0860022\n0.5730514\n0.1500777\n0.8807033\n\n\noccupation_white_collar_x_race_Other\n-0.3731938\n0.8639278\n-0.4319734\n0.6657608\n\n\noccupation_white_collar_x_race_White\n0.1513841\n0.5481445\n0.2761755\n0.7824132\n\n\noccupation_Others_x_sex_Male\n-0.3821967\n0.3098797\n-1.2333712\n0.2174373\n\n\noccupation_services_x_sex_Male\n0.2564560\n0.2581336\n0.9935012\n0.3204658\n\n\noccupation_white_collar_x_sex_Male\n0.1581810\n0.1879843\n0.8414584\n0.4000912\n\n\nsex_Male_x_hours_per_week\n0.0586262\n0.0513488\n1.1417232\n0.2535691"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-logístico",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-logístico",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo logístico",
    "text": "Importâncias para o modelo logístico\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- glm_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4556  649\n         1  388  920\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_logit = augment(glm_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-2-regressão-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-2-regressão-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 2: Regressão Lasso",
    "text": "Modelo 2: Regressão Lasso\nComo observado na receita do modelo, existem 38 covariáveis nesta modelagem. Diversas abordagens podem ser utilizadas para selecionar as covariáveis de maior importância, sendo uma dessas a regressão lasso, que penaliza coeficientes e torna-os 0 em caso de insignificância.\nEste é um modelo que contém um hiperparâmetro, portanto iremos ajustar um grid para escolher o melhor possível.\n\n\nShow the code\nlasso_spec &lt;- logistic_reg(penalty = tune(),\n                           mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(lasso_spec)\n\ngrid &lt;- grid_regular(penalty(),\n                     levels = 100)\n\nplan(multisession)\nset.seed(150167636)\nlasso_res &lt;- lasso_wf %&gt;%\n  tune_grid(resamples = cv_folds,\n            grid = grid,\n            metrics = metric_set(roc_auc))\n\n\n\n\nShow the code\nlasso_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, penalty) %&gt;%\n  pivot_longer(penalty,\n               names_to = \"hiperparâmetro\",\n               values_to = \"valor\") %&gt;%\n  ggplot(aes(valor, mean)) +\n  geom_point()+\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 10),\n                          axis.text.y = element_text(size = 10),\n                          axis.title.x = element_text(size = 10),\n                          axis.title.y = element_text(size = 10),\n                          plot.title = element_text(size = 10, hjust = 0.5))+\n  labs(x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\nPodemos ver que um menor valor de penalização é benéfico ao modelo, visto a importância relativa das covariáveis serem altas neste caso"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Estimativa dos parâmetros para o modelo Lasso",
    "text": "Estimativa dos parâmetros para o modelo Lasso\n\n\nShow the code\nbest_params = lasso_res %&gt;%\n  select_best(metric = \"roc_auc\")\n\nbest_wf = finalize_workflow(lasso_wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n-1.1948793\n0.0007391\n\n\nage\n0.3943655\n0.0007391\n\n\neducation_num\n0.7870445\n0.0007391\n\n\ncapital_gain\n2.1066124\n0.0007391\n\n\ncapital_loss\n0.2622005\n0.0007391\n\n\nhours_per_week\n0.3315385\n0.0007391\n\n\nworkclass_not_working\n-1.9072610\n0.0007391\n\n\nworkclass_private\n-0.0252262\n0.0007391\n\n\nworkclass_self_employed\n-0.2793742\n0.0007391\n\n\nworkclass_unknown\n-0.1482158\n0.0007391\n\n\neducation_low_education\n0.0470209\n0.0007391\n\n\noccupation_Others\n-0.1167739\n0.0007391\n\n\noccupation_services\n-0.0488290\n0.0007391\n\n\noccupation_white_collar\n0.3271691\n0.0007391\n\n\nrace_Asian.Pac.Islander\n0.0246369\n0.0007391\n\n\nrace_Black\n0.0000000\n0.0007391\n\n\nrace_Other\n-0.2075199\n0.0007391\n\n\nrace_White\n0.0681928\n0.0007391\n\n\nsex_Male\n0.1888269\n0.0007391\n\n\ncountry_United.States\n0.2273567\n0.0007391\n\n\ncivil_status_Single\n-2.3832942\n0.0007391\n\n\noccupation_Others_x_race_Asian.Pac.Islander\n-0.5597923\n0.0007391\n\n\noccupation_Others_x_race_Black\n-0.1860392\n0.0007391\n\n\noccupation_Others_x_race_Other\n0.5816320\n0.0007391\n\n\noccupation_Others_x_race_White\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_Asian.Pac.Islander\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_Black\n-0.0567795\n0.0007391\n\n\noccupation_services_x_race_Other\n0.0000000\n0.0007391\n\n\noccupation_services_x_race_White\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_race_Asian.Pac.Islander\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_race_Black\n0.1298919\n0.0007391\n\n\noccupation_white_collar_x_race_Other\n-0.2649066\n0.0007391\n\n\noccupation_white_collar_x_race_White\n0.2695555\n0.0007391\n\n\noccupation_Others_x_sex_Male\n-0.3617209\n0.0007391\n\n\noccupation_services_x_sex_Male\n0.0000000\n0.0007391\n\n\noccupation_white_collar_x_sex_Male\n0.1420720\n0.0007391\n\n\nsex_Male_x_hours_per_week\n0.0511361\n0.0007391"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo Lasso",
    "text": "Importâncias para o modelo Lasso\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4571  652\n         1  373  917\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_lasso = augment(final_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-3-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#modelo-3-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 3: XGBoost",
    "text": "Modelo 3: XGBoost\nO XGBoost é um modelo de Gradient boosting baseado em árvores, que costuma performar bem em tarefas como esta, de classificação com diversas covariáveis\nEste é um modelo de Boosting, ou seja, o “encaixe” de diversos modelos fracos afim da construção de um modelo robusto a partir da combinação destes resultados.\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#ajuste-do-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#ajuste-do-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Ajuste do XGBoost",
    "text": "Ajuste do XGBoost\nTambém iremos realizar o fine tuning de alguns hiperparâmetros deste modelo, no caso o número de árvores e a profundidade destas, afim de obter o melhor modelo.\n\n\nShow the code\nmodel = boost_tree(mode = \"classification\",\n                   trees = tune(),\n                   tree_depth = tune()\n                   ) %&gt;%\n  set_engine(\"xgboost\")\n\nwf = workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(model)\n\ngrid = wf %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 3)\n\nplan(multisession)\nset.seed(150167636)\ntune_res = tune_grid( \n  wf,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(accuracy, roc_auc, sens,spec)\n  )\n\n\n\n\nShow the code\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, trees:tree_depth) %&gt;%\n  pivot_longer(trees:tree_depth,\n               names_to = \"hiperparâmetro\",\n               values_to = \"valor\") %&gt;%\n  ggplot(aes(valor, mean, color = hiperparâmetro)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~hiperparâmetro, scales = \"free_x\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 10),\n                          axis.text.y = element_text(size = 10),\n                          axis.title.x = element_text(size = 10),\n                          axis.title.y = element_text(size = 10),\n                          plot.title = element_text(size = 10, hjust = 0.5))+\n  labs(x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_params = tune_res %&gt;%\n  select_best(metric = \"roc_auc\")\nbest_params\n\n\n# A tibble: 1 × 3\n  trees tree_depth .config             \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1  2000          1 Preprocessor1_Model3\n\n\nVemos que a melhor combinação de hiperparâmetros encontrada é utilizando 2000 árvores de tamanho 1"
  },
  {
    "objectID": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-xgboost",
    "href": "presentations/classificacao_salario_relatorio/trabalho_bruno.html#importâncias-para-o-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o XGBoost",
    "text": "Importâncias para o XGBoost\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\nShow the code\nbest_wf = finalize_workflow(wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\n\n\n\nShow the code\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 34),\n                          axis.text.y = element_text(size = 20),\n                          axis.title.x = element_text(size = 16),\n                          axis.title.y = element_text(size = 16),\n                          plot.title = element_text(size = 18, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n\n          Truth\nPrediction    0    1\n         0 4636  584\n         1  308  985\n\n\n\n\nShow the code\nmetrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nresultados_xgboost = augment(final_fit, new_data = test) %&gt;%\n  metrics(truth = salary, estimate = .pred_class) %&gt;%\n  select(.metric,.estimate)"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#introdução",
    "href": "presentations/birnbaum/seminario.html#introdução",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Introdução",
    "text": "Introdução\n\n\n\n\nA distribuição Birnbaum-Saunders (Birnbaum and Saunders, 1969) é utilizada para descrever tempo de vida por fadiga (exposição cumulativa a danos, vibrações, etc) de materiais expostos à ciclos homogêneos. O processo de fadiga descrito pelos autores é:\n\nUm material sujeito a ciclo de cargas repetitivos, que produzem desgaste no material;\nA falha no material dar-se-á quando a magnitude do desgaste excede um limite \\(\\omega\\);\nOs ciclos são homogêneos (a sequência de estresse aplicada ao material é sempre a mesma);\nA extensão da fissura (\\(X_i\\)) causada pela carga \\(l_i\\) é uma variável aleatória com dependência nas cargas e fissuras acumuladas até o i-ésimo ciclo;\nO tamanho total da fissura causados no j-ésimo ciclo (\\(Y_j\\)) também é uma variável aleatória com média \\(\\mu\\) e variância \\(\\sigma^2 &lt; \\infty\\);\nO tamanho das fissuras \\(Y_j\\) em diferentes ciclos são independentes."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#a-distribuição-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#a-distribuição-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "A distribuição Birnbaum-Saunders",
    "text": "A distribuição Birnbaum-Saunders\nPara este problema, os autores definem que: Seja T o tempo total até a ocorrência da falha e cumprida as condições de regularidade, podemos dizer que T segue distribuição Birnbaum-Saunders (BS), tal que \\(T \\sim BS(\\alpha,\\beta)\\), e \\[T = \\beta\\biggl(\\frac{\\alpha}{2}Z+\\sqrt{(\\frac{\\alpha}{2}Z)^2+1} \\biggl),\\] onde \\(\\alpha\\) é um parâmetro de forma, \\(\\beta\\) é parâmetro de localização (mediana), e \\(Z\\) é uma variável aleatória com distribuição normal padrão.\nEm uma aplicação, os parâmetros \\(\\alpha\\) e \\(\\beta\\) devem ser estimados."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#reparametrização-da-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#reparametrização-da-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Reparametrização da Birnbaum-Saunders",
    "text": "Reparametrização da Birnbaum-Saunders\nEm geral, iremos trabalhar com uma reparametrização da distribuição Birnbaum-Saunders, especialmente para regressão. Rieck e Nedelman (1991) mostram a relação da distribuição Birnbaum-Saunders com a distribuição seno hiperbólico normal.\nSeja \\(T \\sim BS(\\alpha,\\beta)\\), então \\(Y \\sim SHN(\\alpha,log(\\beta),\\sigma=2)\\), onde \\(\\alpha\\) é parâmetro de forma, \\(\\beta\\) é parâmetro de localização, e \\(\\sigma\\) é parâmetro de de escala fixo.\nDesta forma, a distribuição seno hiperbólica normal pode também ser chamada de distribuição log-Birnbaum-Saunders (LBS)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#regressão-birnbaum-saunders",
    "href": "presentations/birnbaum/seminario.html#regressão-birnbaum-saunders",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Regressão Birnbaum-Saunders",
    "text": "Regressão Birnbaum-Saunders\nAssumindo \\(T_i\\) variável resposta, podemos construir um modelo de regressão Birbaum-Saunders, tal que: \\[T_i = \\beta_i\\varphi_i = exp(\\mu_i)\\varphi_i=exp(\\mathbf{x}_i^T\\mathbf{\\eta})\\varphi_i, i = 1,2,...,n.\\] onde \\(T_i\\) e \\(\\beta_i = exp(\\mu_i)\\) são a variável resposta e a mediana para a i-ésima observação; \\(\\mathbf{\\eta} = (\\eta_0,\\eta_1,...,\\eta_p)^T\\) é o vetor de parâmetros desconhecidos a serem estimados pela regressão, \\(\\mathbf{x}_i^T=(1,x_{i1},...,x_{ip})\\) são os valores das \\(p\\) variáveis explicativas, e \\(\\varphi_i \\sim BS(\\alpha,1)\\) é o erro do modelo, tal que \\(T_i \\sim BS(\\alpha,\\beta)\\)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#função-de-verossimilhança",
    "href": "presentations/birnbaum/seminario.html#função-de-verossimilhança",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Função de verossimilhança",
    "text": "Função de verossimilhança\nPara construir a função de verossimilhança, será necessário reparametrizar a distribuição \\(T_i \\sim BS(\\alpha,\\beta_i)\\) para \\(Y_i = log(T_i) \\sim LBS(\\alpha,log(\\beta_i))\\), tal que \\[log(T_i) = Y_i = \\mu_i + \\epsilon_i = \\mathbf{x}_i^T\\mathbf{\\eta}+\\epsilon_i, i=1,...,n.\\]\nDesta forma, a função de verossimilhança para \\(\\mathbf{\\theta}=(\\alpha,\\mathbf{\\eta}^T)^T\\) não terá solução analítica, sendo necessário estimar o vetor de parâmetros \\(\\mathbf{\\eta}\\) utilizando métodos iterativos de otimização não linear.\nDesta estimativa obtida, podemos obter a estimativa de máxima verossimilhança do parâmetro \\(\\alpha\\), tal que: \\[\\hat{\\alpha} = \\sqrt{\\frac{4}{n}\\sum_{i=1}^nsinh^2\\biggl(\\frac{y_i-\\mathbf{x}_i^T\\mathbf{\\hat{\\eta}}}{2}\\biggl)}.\\]"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#estimador-de-mínimos-quadrados",
    "href": "presentations/birnbaum/seminario.html#estimador-de-mínimos-quadrados",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Estimador de mínimos quadrados",
    "text": "Estimador de mínimos quadrados\nPodemos também utilizar o estimador de mínimos quadrados para \\(\\mathbf{\\eta}\\), da forma \\[\\hat{\\mathbf{\\eta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\\] onde \\(\\mathbf{y}\\) é o vetor de observações, e \\(\\mathbf{X}\\) é a matriz de desenho do modelo. Entretanto, este método é menos eficiente que o estimador de máxima verossimilhança, conforme observado por Rieck e Nedelman (1991)."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#distribuição-bs-reparametrizada-pela-média",
    "href": "presentations/birnbaum/seminario.html#distribuição-bs-reparametrizada-pela-média",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Distribuição BS reparametrizada pela média",
    "text": "Distribuição BS reparametrizada pela média\nSantos-Neto et al. (2012) propuseram uma reparametrização da distribuição Birnbaum-Saunders, sendo: \\(\\alpha=\\frac{2}{\\delta},\\beta=\\frac{\\delta\\mu}{\\delta+1},\\) tal que \\(\\delta=\\frac{2}{\\alpha^2},\\mu=\\beta\\Bigl(1+\\frac{\\alpha^2}{2}\\Bigl)\\), sendo \\(\\delta&gt;0,\\mu&gt;0\\) parâmetros de forma e média, respectivamente.\nNesta parametrização, a função de densidade de probabilidade fica definida por: \\[f_Y(y|\\mu,\\delta)=\\frac{exp(\\frac{\\delta}{2}\\sqrt{\\delta+1})}{\\sqrt{16\\pi\\mu}y^{\\frac{3}{2}}}\\Bigl(y+\\frac{\\delta\\mu}{\\delta+1}\\Bigl)exp\\Biggl(-\\frac{\\delta}{4}\\Biggl[\\frac{y(\\delta+1)}{\\delta\\mu}+\\frac{\\delta\\mu}{y(\\delta+1)}\\Biggl]\\Biggl)\\]"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#regressão-bs-reparametrizada-pela-média",
    "href": "presentations/birnbaum/seminario.html#regressão-bs-reparametrizada-pela-média",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Regressão BS reparametrizada pela média",
    "text": "Regressão BS reparametrizada pela média\nSendo \\(RBS(\\mu,\\delta)\\) a reparametrização pela média da distribuição Birnbaum-Saunders original, Leiva et al. (2014) propõe um modelo de regressão em que, seja \\(Y \\sim RBS(\\mu,\\delta)\\), a média de \\(Y_i\\) satisfaz a relação: \\[h(\\mu_i)=\\mathbf{x}_i^T\\mathbf{\\beta}, i=1,...,n,\\] em que \\(\\mathbf{\\beta}=(\\beta_1,...,\\beta_p)^T,p&lt;n\\) é vetor de coeficientes de regressão a serem estimados, e \\(\\mathbf{x}_i=(x_{i1},x_{i2},...,x_{ip})^T\\) são observações de p regressores, e \\(\\mu_i=h^{-1}(\\mathbf{x}_i^T\\mathbf{\\beta}),\\) com \\(h(.)\\) sendo uma função de ligação positiva e duas vezes diferenciável.\nSob a distribuição RBS, \\(\\mathbb{E}(Y)=\\mu\\) e \\(Var(Y) = \\mu^2 CV(Y)^2\\), onde \\(CV(Y) = \\frac{\\sqrt{2\\delta+5}}{(\\delta+1)}\\in (0,\\sqrt{5})\\) é o coeficiente de variação de Y.\nA variância de \\(Y_i\\) é função de \\(\\mu_i\\), logo estaremos modelando também a variância."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#vantagens-e-desvantagens-dos-modelos-bs",
    "href": "presentations/birnbaum/seminario.html#vantagens-e-desvantagens-dos-modelos-bs",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Vantagens e desvantagens dos modelos BS",
    "text": "Vantagens e desvantagens dos modelos BS\n\n\nVantagens\n\n\nMais uma família para obter um ajuste melhor aos dados;\nPode ser utilizada para dados censurados;\nResultados do modelo têm interpretação fácil;\nImplementado no R!\n\n\n\nDesvantagens\n\n\nDistribuição pouco conhecida;\nLimitações conceituais e práticas;\nDefinição matemática complexa;\nPouco material prático disponível de forma fácil."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#aplicação",
    "href": "presentations/birnbaum/seminario.html#aplicação",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Aplicação",
    "text": "Aplicação\nUtilizarei o conjunto de dados cpd do pacote faraway, referente a projeção de vendas e o verdadeiro número de vendas de 20 produtos\n\n\n\nlibrary(tidyverse)\nlibrary(gamlss)\ndata(cpd,package='faraway')\nattach(cpd)\np1 = cpd %&gt;% \n  gather() %&gt;%\n  ggplot(aes(value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~key, scales = 'free_x') +\n  labs(x = '', y = '', title = '') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nVemos que tanto a variável resposta, actual, quanto a covariável explicativa, projected, são obviamente positivas, e também assimétricas à direta."
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#primeiro-ajuste",
    "href": "presentations/birnbaum/seminario.html#primeiro-ajuste",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Primeiro ajuste",
    "text": "Primeiro ajuste\nVeremos qual distribuição o pacote gamlss recomenda para melhor ajuste aos dados\n\nmod = fitDist(actual, type=\"realplus\") \n\n\nmod$fits\n\n      IG     BCPE   IGAMMA   LOGNO2    LOGNO      GIG       GG    BCCGo \n319.7223 320.9136 320.9552 321.5667 321.5667 321.7220 322.8726 323.0453 \n    BCCG      EXP  PARETO2       GP PARETO2o      GB2      WEI     WEI2 \n323.0453 323.5004 324.1676 324.1676 324.1676 324.8727 324.9636 324.9636 \n    WEI3     BCTo       GA   exGAUS \n324.9636 325.0453 325.3356 352.5137 \n\n\nVemos que o pacote GAMLSS oferece diversas opções de ajuste para este conjunto de dados. Entretanto, estamos especialmente interessados em tentar ajustar os dados utilizando o modelo RBS"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#ajuste-rbs",
    "href": "presentations/birnbaum/seminario.html#ajuste-rbs",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Ajuste RBS",
    "text": "Ajuste RBS\nPrimeiramente, é necessário baixar e instalar o pacote RBS, que não está disponível no CRAN, utilizando o comando\n\ndevtools::install_github(\"santosneto/RBS\")\n\n\nlibrary(RBS)\n\nAgora, tentaremos o ajuste utilizando a distribuição RBS, e aproveitando da sintaxe e funcionalidades do pacote gamlss\n\n\nAjustando o modelo\n\nmodel0 = gamlss::gamlss(actual ~ projected, family=RBS(mu.link=\"identity\"),method=CG())\n\n\nsummary(model0)\n\n******************************************************************\nFamily:  c(\"RBS\", \"BirnbaumSaunders\") \n\nCall:  \ngamlss::gamlss(formula = actual ~ projected, family = RBS(mu.link = \"identity\"),  \n    method = CG()) \n\nFitting method: CG() \n\n------------------------------------------------------------------\nMu link function:  identity\nMu Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.80360   18.08128   0.155    0.879    \nprojected    1.08047    0.07339  14.722 4.16e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  log\nSigma Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.9020     0.3162   12.34 6.56e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  20 \nDegrees of Freedom for the fit:  3\n      Residual Deg. of Freedom:  17 \n                      at cycle:  8 \n \nGlobal Deviance:     247.755 \n            AIC:     253.755 \n            SBC:     256.7422 \n******************************************************************\n\n\n\nModelando também a variância\n\nmodel = gamlss::gamlss(actual ~ projected, sigma.formula = ~projected, family=RBS(mu.link=\"identity\"),method=CG())\n\n\nsummary(model)\n\n******************************************************************\nFamily:  c(\"RBS\", \"BirnbaumSaunders\") \n\nCall:  \ngamlss::gamlss(formula = actual ~ projected, sigma.formula = ~projected,  \n    family = RBS(mu.link = \"identity\"), method = CG()) \n\nFitting method: CG() \n\n------------------------------------------------------------------\nMu link function:  identity\nMu Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.81320   21.00592   1.657    0.117    \nprojected    0.95759    0.01903  50.318   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  log\nSigma Coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.1348243  0.4749559   6.600  6.1e-06 ***\nprojected   0.0010326  0.0003238   3.189   0.0057 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  20 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  16 \n                      at cycle:  20 \n \nGlobal Deviance:     241.0335 \n            AIC:     249.0335 \n            SBC:     253.0165 \n******************************************************************"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#diagnósticos",
    "href": "presentations/birnbaum/seminario.html#diagnósticos",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Diagnósticos",
    "text": "Diagnósticos\nPodemos realizar a rotina normal de análise de diagnósticos do modelo, utilizando as funcionalidades do pacote gamlss\n\n\n\nwp(model)\n\n\n\n\n\n\n\n\n\n\nplot(model)\n\n\n\n\n\n\n\n\n******************************************************************\n          Summary of the Quantile Residuals\n                           mean   =  0.2395729 \n                       variance   =  1.024173 \n               coef. of skewness  =  -0.1452511 \n               coef. of kurtosis  =  1.76144 \nFilliben correlation coefficient  =  0.9877058 \n******************************************************************"
  },
  {
    "objectID": "presentations/birnbaum/seminario.html#referências",
    "href": "presentations/birnbaum/seminario.html#referências",
    "title": "Modelo de regressão Birnbaum-Saunders reparametrizado pela média",
    "section": "Referências",
    "text": "Referências\nBirnbaum, Z. and Saunders, S. (1969). A new family of life distributions. Journal of Applied Probability, 6:319–327.\nSantos-Neto, M., Cysneiros, F., Leiva, V., and Ahmed, S. (2012). On new parameterizations of the Birnbaum-Saunders distribution. Pakistan Journal of Statistics, 28:1–26.\nSantos-Neto, M., Cysneiros, F., Leiva, V., and Barros, M. (2014). On new parameterizations of the Birnbaum-Saunders distribution and its moments, estimation and application. Revstat Statistical Journal, 12:247–272.\nLeiva, V., Santos-Neto, M., Cysneiros, F., and Barros, M. (2014). BirnbaumSaunders statistical modelling: a new approach. Statistical Modelling, 14:21–48.\nRieck, J. and Nedelman, J. (1991). A log-linear model for the Birnbaum-Saunders distribution. Technometrics, 3:51–60.\nNotas de aula do Prof. Helton Saulo. Disciplina Tópicos em Estatística 2 — Modelagem com apoio computacional. UnB, 2º/2024.\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bruno Gondim",
    "section": "",
    "text": "Estudante de graduação no bacharel de Estatística na Universidade de Brasília - UnB\nAqui estão aplicações pessoais de machine learning, ciência de dados e estatística. Me interesso também em programação, estrutura e modelagem de dados. Acredito em um mundo mais igualitário e justo."
  },
  {
    "objectID": "index.html#formação",
    "href": "index.html#formação",
    "title": "Bruno Gondim",
    "section": "Formação",
    "text": "Formação\nUniversidade de Brasília (UnB) | Distrito Federal, Brasil | Bacharel em Estatística | Ago 2015 - presente"
  },
  {
    "objectID": "index.html#experiência",
    "href": "index.html#experiência",
    "title": "Bruno Gondim",
    "section": "Experiência",
    "text": "Experiência\nGerente de projetos na ESTAT - Jan 2022 - Jan 2023\nEstagiário STF | Núcleo de análise de dados e estatística (NUADE) | Jul 2023 - presente"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#introdução",
    "href": "presentations/classificacao_salario/trabalho.html#introdução",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Introdução",
    "text": "Introdução\n\n\n\nNeste trabalho, buscou-se estudar um conjunto censitário sintético com diversas características demográficas, com objetivo fim de entender a relação entre covariáveis o salário — variável esta que se encontra binarizada no conjunto de dados, sendo\n\n0: Renda anual de até 50.000 dólares\n1: Renda anual acima de 50.000 dólares\n\nOs dados são públicos e podem ser acessados em Kaggle."
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#preparação",
    "href": "presentations/classificacao_salario/trabalho.html#preparação",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Preparação",
    "text": "Preparação\nOs dados disponíveis no Kaggle já haviam passado por algumas etapas de transformações, mas para o objetivo deste trabalho achei pertinente realizar mais algumas rotinas de transformações nas covariáveis\n\ndf$salary = factor(df$salary)\ndf = clean_names(df)\n\ndf$fnlwgt = df$fnlwgt/sum(df$fnlwgt)\n\ndf = df %&gt;%\n  mutate(marital_status = factor(marital_status),\n         relationship = factor(relationship),\n         race = factor(race),\n         sex = factor(sex),\n         country = factor(ifelse(country == 'United-States',country,'Others')),\n         education = case_when(\n           education %in% c(\"Bachelors\",\"Masters\",\"Assoc-acdm\",\n                            \"Assoc-voc\",\"Doctorate\",\"Prof-school\") ~ \"high_education\",\n           .default = \"low_education\"),\n         education = factor(education),\n         occupation = case_when(\n           occupation %in% c(\"Adm-clerical\", \"Exec-managerial\", \"Prof-specialty\", \"Tech-support\", \"Sales\") ~ \"white_collar\", # Ocupações que geralmente envolvem trabalho em escritório ou administrativo.\n           occupation %in% c(\"Craft-repair\", \"Farming-fishing\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Transport-moving\") ~ \"blue_collar\", # Ocupações que envolvem trabalho manual ou técnico.\n           occupation %in% c(\"Other-service\", \"Priv-house-serv\", \"Protective-serv\") ~ \"services\", # Ocupações no setor de serviços.\n           occupation %in% c(\"Armed-Forces\",\"?\") ~ \"Others\"),\n         occupation = factor(occupation),\n         civil_status = ifelse(marital_status %in% c('Never-married','Divorced','Separated', 'Widowed') | relationship %in% c('Not-in-family','Unmarried'),\"Single\",\"Non-single\"),\n         civil_status = factor(civil_status),\n         workclass = case_when(\n           workclass %in% c(\"Federal-gov\", \"Local-gov\", \"State-gov\") ~ \"government\",\n           workclass == \"Private\" ~ \"private\",\n           workclass %in% c(\"Self-emp-inc\", \"Self-emp-not-inc\") ~ \"self_employed\",\n           workclass %in% c(\"Never-worked\", \"Without-pay\") ~ \"not_working\",\n           workclass == \"?\" ~ \"unknown\"),\n         workclass = factor(workclass)\n  ) %&gt;%\n  select(-marital_status,-relationship)"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#section",
    "href": "presentations/classificacao_salario/trabalho.html#section",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "",
    "text": "Análise exploratória\n\nApós as transformações, observamos em nossos dados 12 covariáveis de aspectos demográficos dos grupos populacionais bastante heterogêneos e de tamanhos desiguais, quando separado nos grupos de salário até 50 mil dólares anuais e acima de 50 mil dólares anuais.\n\n\n\n\n\n\n\n\n&lt;50k\n&gt;50k\np.overall\n\n\n\n\n\nN=24720\nN=7841\n\n\n\nage\n34.0 [25.0;46.0]\n44.0 [36.0;51.0]\n0.000\n\n\nworkclass:\n\n\n&lt;0.001\n\n\ngovernment\n3010 (12.2%)\n1341 (17.1%)\n\n\n\nnot_working\n21 (0.08%)\n0 (0.00%)\n\n\n\nprivate\n17733 (71.7%)\n4963 (63.3%)\n\n\n\nself_employed\n2311 (9.35%)\n1346 (17.2%)\n\n\n\nunknown\n1645 (6.65%)\n191 (2.44%)\n\n\n\neducation:\n\n\n0.000\n\n\nhigh_education\n5981 (24.2%)\n4535 (57.8%)\n\n\n\nlow_education\n18739 (75.8%)\n3306 (42.2%)\n\n\n\neducation_num\n9.00 [9.00;10.0]\n12.0 [10.0;13.0]\n0.000\n\n\noccupation:\n\n\n0.000\n\n\nblue_collar\n8362 (33.8%)\n1700 (21.7%)\n\n\n\nOthers\n1660 (6.72%)\n192 (2.45%)\n\n\n\nservices\n3744 (15.1%)\n349 (4.45%)\n\n\n\nwhite_collar\n10954 (44.3%)\n5600 (71.4%)\n\n\n\nrace:\n\n\n&lt;0.001\n\n\nAmer-Indian-Eskimo\n275 (1.11%)\n36 (0.46%)\n\n\n\nAsian-Pac-Islander\n763 (3.09%)\n276 (3.52%)\n\n\n\nBlack\n2737 (11.1%)\n387 (4.94%)\n\n\n\nOther\n246 (1.00%)\n25 (0.32%)\n\n\n\nWhite\n20699 (83.7%)\n7117 (90.8%)\n\n\n\nsex:\n\n\n0.000\n\n\nFemale\n9592 (38.8%)\n1179 (15.0%)\n\n\n\nMale\n15128 (61.2%)\n6662 (85.0%)\n\n\n\ncapital_gain\n0.00 [0.00;0.00]\n0.00 [0.00;0.00]\n0.000\n\n\ncapital_loss\n0.00 [0.00;0.00]\n0.00 [0.00;0.00]\n&lt;0.001\n\n\nhours_per_week\n40.0 [35.0;40.0]\n40.0 [40.0;50.0]\n0.000\n\n\ncountry:\n\n\n&lt;0.001\n\n\nOthers\n2721 (11.0%)\n670 (8.54%)\n\n\n\nUnited-States\n21999 (89.0%)\n7171 (91.5%)\n\n\n\ncivil_status:\n\n\n0.000\n\n\nNon-single\n8357 (33.8%)\n6702 (85.5%)\n\n\n\nSingle\n16363 (66.2%)\n1139 (14.5%)"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#covariáveis-numéricas",
    "href": "presentations/classificacao_salario/trabalho.html#covariáveis-numéricas",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Covariáveis numéricas",
    "text": "Covariáveis numéricas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\neducation_num\ncapital_gain\ncapital_loss\nhours_per_week\n\n\n\n\n\nMin. :17.00\nMin. : 1.00\nMin. : 0\nMin. : 0.0\nMin. : 1.00\n\n\n\n1st Qu.:28.00\n1st Qu.: 9.00\n1st Qu.: 0\n1st Qu.: 0.0\n1st Qu.:40.00\n\n\n\nMedian :37.00\nMedian :10.00\nMedian : 0\nMedian : 0.0\nMedian :40.00\n\n\n\nMean :38.58\nMean :10.08\nMean : 1078\nMean : 87.3\nMean :40.44\n\n\n\n3rd Qu.:48.00\n3rd Qu.:12.00\n3rd Qu.: 0\n3rd Qu.: 0.0\n3rd Qu.:45.00\n\n\n\nMax. :90.00\nMax. :16.00\nMax. :99999\nMax. :4356.0\nMax. :99.00"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#covariáveis-categóricas",
    "href": "presentations/classificacao_salario/trabalho.html#covariáveis-categóricas",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Covariáveis categóricas",
    "text": "Covariáveis categóricas"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelagem",
    "href": "presentations/classificacao_salario/trabalho.html#modelagem",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelagem",
    "text": "Modelagem\nNum contexto em que temos tantas covariáveis, tantas observações e, apesar de alguns indicativos observados na análise exploratória, não é possível observar um padrão óbvio que indique o salário do indivíduo. Para isso, utilizei do recurso da modelagem com apoio computacional afim de tornar possível esta análise. Um diferencial deste trabalho é a utilização do framework tidymodels, que é bastante verborrágico e permite uma compreensão das etapas do modelo pela leitura do código, além de eficiência e praticidade"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#parâmetros-gerais",
    "href": "presentations/classificacao_salario/trabalho.html#parâmetros-gerais",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Parâmetros gerais",
    "text": "Parâmetros gerais\nIrei testar diversos modelos e fazer comparação de resultados destes, mas utilizarei a mesma “receita” para todos\n\nset.seed(150167636)\nsplit = initial_split(df, prop = .8, strata = salary)\ntrain = training(split)\ntest = testing(split)\n\ncv_folds &lt;- vfold_cv(train, \n                     v = 5, \n                     strata = salary)\n\nrecipe &lt;- recipe(salary ~ .,\n                 data = train) %&gt;% \n  update_role(fnlwgt, new_role = \"case_weight\") %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ starts_with(\"occupation\"):starts_with(\"race\") + \n                  starts_with(\"occupation\"):starts_with(\"sex\") +\n                  starts_with(\"hours_per_week\"):starts_with(\"sex\"))\n\ndados_preparados &lt;- recipe %&gt;% \n  prep() %&gt;% \n  juice()\nhead(dados_preparados)\n\n# A tibble: 6 × 38\n      age   fnlwgt education_num capital_gain capital_loss hours_per_week salary\n    &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt; \n1  0.0315  1.25e-5         1.14         0.151       -0.216        -0.0354 0     \n2  0.838   1.35e-5         1.14        -0.146       -0.216        -2.22   0     \n3 -0.0418  3.49e-5        -0.421       -0.146       -0.216        -0.0354 0     \n4 -0.775   5.48e-5         1.14        -0.146       -0.216        -0.0354 0     \n5  0.764   2.59e-5        -1.98        -0.146       -0.216        -1.98   0     \n6 -1.14    1.98e-5         1.14        -0.146       -0.216        -0.844  0     \n# ℹ 31 more variables: workclass_not_working &lt;dbl&gt;, workclass_private &lt;dbl&gt;,\n#   workclass_self_employed &lt;dbl&gt;, workclass_unknown &lt;dbl&gt;,\n#   education_low_education &lt;dbl&gt;, occupation_Others &lt;dbl&gt;,\n#   occupation_services &lt;dbl&gt;, occupation_white_collar &lt;dbl&gt;,\n#   race_Asian.Pac.Islander &lt;dbl&gt;, race_Black &lt;dbl&gt;, race_Other &lt;dbl&gt;,\n#   race_White &lt;dbl&gt;, sex_Male &lt;dbl&gt;, country_United.States &lt;dbl&gt;,\n#   civil_status_Single &lt;dbl&gt;, …"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-1-regressão-logística",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-1-regressão-logística",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 1: Regressão logística",
    "text": "Modelo 1: Regressão logística\nComo a variável resposta é binária, o primeiro modelo que podemos tentar seria o logístico\n\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(glm_spec)\n\nglm_fit &lt;- glm_wf %&gt;%\n  fit(data = train)\n\nglm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  print(n=Inf)\n\n# A tibble: 37 × 5\n   term                                   estimate std.error statistic   p.value\n   &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                             -1.74      0.487    -3.57   3.58e-  4\n 2 age                                      0.406     0.0228   17.8    7.27e- 71\n 3 education_num                            0.844     0.0438   19.3    9.28e- 83\n 4 capital_gain                             2.23      0.0800   27.9    1.83e-171\n 5 capital_loss                             0.269     0.0165   16.3    1.15e- 59\n 6 hours_per_week                           0.335     0.0455    7.36   1.86e- 13\n 7 workclass_not_working                  -11.8     117.       -0.100  9.20e-  1\n 8 workclass_private                       -0.0700    0.0560   -1.25   2.11e-  1\n 9 workclass_self_employed                 -0.349     0.0730   -4.78   1.79e-  6\n10 workclass_unknown                       -0.555     1.80     -0.308  7.58e-  1\n11 education_low_education                  0.150     0.0759    1.98   4.77e-  2\n12 occupation_Others                        0.195     2.00      0.0972 9.23e-  1\n13 occupation_services                      0.808     0.805     1.00   3.16e-  1\n14 occupation_white_collar                  0.431     0.574     0.752  4.52e-  1\n15 race_Asian.Pac.Islander                  0.750     0.504     1.49   1.36e-  1\n16 race_Black                               0.469     0.465     1.01   3.14e-  1\n17 race_Other                               0.111     0.663     0.167  8.67e-  1\n18 race_White                               0.580     0.445     1.30   1.93e-  1\n19 sex_Male                                 0.172     0.180     0.954  3.40e-  1\n20 country_United.States                    0.259     0.0763    3.40   6.68e-  4\n21 civil_status_Single                     -2.41      0.0517  -46.6    0        \n22 occupation_Others_x_race_Asian.Pac.Is…  -1.18      1.46     -0.812  4.17e-  1\n23 occupation_Others_x_race_Black          -0.423     1.30     -0.324  7.46e-  1\n24 occupation_Others_x_race_Other           1.45      1.68      0.864  3.88e-  1\n25 occupation_Others_x_race_White           0.0267    1.21      0.0221 9.82e-  1\n26 occupation_services_x_race_Asian.Pac.…  -1.29      0.871    -1.48   1.40e-  1\n27 occupation_services_x_race_Black        -1.18      0.817    -1.44   1.49e-  1\n28 occupation_services_x_race_Other        -1.07      1.41     -0.764  4.45e-  1\n29 occupation_services_x_race_White        -1.10      0.783    -1.41   1.59e-  1\n30 occupation_white_collar_x_race_Asian.…  -0.300     0.608    -0.494  6.21e-  1\n31 occupation_white_collar_x_race_Black     0.0860    0.573     0.150  8.81e-  1\n32 occupation_white_collar_x_race_Other    -0.373     0.864    -0.432  6.66e-  1\n33 occupation_white_collar_x_race_White     0.151     0.548     0.276  7.82e-  1\n34 occupation_Others_x_sex_Male            -0.382     0.310    -1.23   2.17e-  1\n35 occupation_services_x_sex_Male           0.256     0.258     0.994  3.20e-  1\n36 occupation_white_collar_x_sex_Male       0.158     0.188     0.841  4.00e-  1\n37 sex_Male_x_hours_per_week                0.0586    0.0513    1.14   2.54e-  1"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-logístico",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-logístico",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo logístico",
    "text": "Importâncias para o modelo logístico\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- glm_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4556  649\n         1  388  920"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-2-regressão-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-2-regressão-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 2: Regressão Lasso",
    "text": "Modelo 2: Regressão Lasso\nComo observado na receita do modelo, existem 38 covariáveis nesta modelagem. Diversas abordagens podem ser utilizadas para selecionar as covariáveis de maior importância, sendo uma dessas a regressão lasso, que penaliza coeficientes e torna-os 0 em caso de insignificância.\nEste é um modelo que contém um hiperparâmetro, portanto iremos ajustar um grid para escolher o melhor possível.\n\n\n\nlasso_spec &lt;- logistic_reg(penalty = tune(),\n                           mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(lasso_spec)\n\ngrid &lt;- grid_regular(penalty(),\n                     levels = 100)\n\nplan(multisession)\nset.seed(150167636)\nlasso_res &lt;- lasso_wf %&gt;%\n  tune_grid(resamples = cv_folds,\n            grid = grid,\n            metrics = metric_set(roc_auc))\n\n\n\n\n\n\n\n\n\n\n\nPodemos ver que um menor valor de penalização é benéfico ao modelo, visto a importância relativa das covariáveis serem altas neste caso"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#estimativa-dos-parâmetros-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Estimativa dos parâmetros para o modelo Lasso",
    "text": "Estimativa dos parâmetros para o modelo Lasso\n\nbest_params = lasso_res %&gt;%\n  select_best(metric = \"roc_auc\")\n\nbest_wf = finalize_workflow(lasso_wf, best_params)\n\nfinal_fit &lt;- best_wf %&gt;%\n  fit(data = train)\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  print(n=Inf)\n\n# A tibble: 37 × 3\n   term                                              estimate  penalty\n   &lt;chr&gt;                                                &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                                        -1.19   0.000739\n 2 age                                                 0.394  0.000739\n 3 education_num                                       0.787  0.000739\n 4 capital_gain                                        2.11   0.000739\n 5 capital_loss                                        0.262  0.000739\n 6 hours_per_week                                      0.332  0.000739\n 7 workclass_not_working                              -1.91   0.000739\n 8 workclass_private                                  -0.0252 0.000739\n 9 workclass_self_employed                            -0.279  0.000739\n10 workclass_unknown                                  -0.148  0.000739\n11 education_low_education                             0.0470 0.000739\n12 occupation_Others                                  -0.117  0.000739\n13 occupation_services                                -0.0488 0.000739\n14 occupation_white_collar                             0.327  0.000739\n15 race_Asian.Pac.Islander                             0.0246 0.000739\n16 race_Black                                          0      0.000739\n17 race_Other                                         -0.208  0.000739\n18 race_White                                          0.0682 0.000739\n19 sex_Male                                            0.189  0.000739\n20 country_United.States                               0.227  0.000739\n21 civil_status_Single                                -2.38   0.000739\n22 occupation_Others_x_race_Asian.Pac.Islander        -0.560  0.000739\n23 occupation_Others_x_race_Black                     -0.186  0.000739\n24 occupation_Others_x_race_Other                      0.582  0.000739\n25 occupation_Others_x_race_White                      0      0.000739\n26 occupation_services_x_race_Asian.Pac.Islander       0      0.000739\n27 occupation_services_x_race_Black                   -0.0568 0.000739\n28 occupation_services_x_race_Other                    0      0.000739\n29 occupation_services_x_race_White                    0      0.000739\n30 occupation_white_collar_x_race_Asian.Pac.Islander   0      0.000739\n31 occupation_white_collar_x_race_Black                0.130  0.000739\n32 occupation_white_collar_x_race_Other               -0.265  0.000739\n33 occupation_white_collar_x_race_White                0.270  0.000739\n34 occupation_Others_x_sex_Male                       -0.362  0.000739\n35 occupation_services_x_sex_Male                      0      0.000739\n36 occupation_white_collar_x_sex_Male                  0.142  0.000739\n37 sex_Male_x_hours_per_week                           0.0511 0.000739"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-lasso",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-modelo-lasso",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o modelo Lasso",
    "text": "Importâncias para o modelo Lasso\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4571  652\n         1  373  917"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#modelo-3-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#modelo-3-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Modelo 3: XGBoost",
    "text": "Modelo 3: XGBoost\nO XGBoost é um modelo de Gradient boosting baseado em árvores, que costuma performar bem em tarefas como esta, de classificação com diversas covariáveis\nEste é um modelo de Boosting, ou seja, o “encaixe” de diversos modelos fracos afim da construção de um modelo robusto a partir da combinação destes resultados.\n\n\n\n\n\n\ngraph LR\n    A[Dados] --&gt; B[Modelo 1];\n    B --&gt; C[Modelo 2];\n    C --&gt; D[...];\n    D --&gt; E[Modelo m];\n    E --&gt; F[Ensembling dos modelos];\n    F --&gt; G[Modelo final];"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#ajuste-do-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#ajuste-do-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Ajuste do XGBoost",
    "text": "Ajuste do XGBoost\nTambém iremos realizar o fine tuning de alguns hiperparâmetros deste modelo, no caso o número de árvores e a profundidade destas, afim de obter o melhor modelo.\n\n\n\nmodel = boost_tree(mode = \"classification\",\n                   trees = tune(),\n                   tree_depth = tune()\n                   ) %&gt;%\n  set_engine(\"xgboost\")\n\nwf = workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(model)\n\ngrid = wf %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 3)\n\nplan(multisession)\nset.seed(150167636)\ntune_res = tune_grid( \n  wf,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(accuracy, roc_auc, sens,spec)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  trees tree_depth .config             \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1  2000          1 Preprocessor1_Model3\n\n\nVemos que a melhor combinação de hiperparâmetros encontrada é utilizando 2000 árvores de tamanho 1"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-xgboost",
    "href": "presentations/classificacao_salario/trabalho.html#importâncias-para-o-xgboost",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Importâncias para o XGBoost",
    "text": "Importâncias para o XGBoost\nPodemos observar as covariáveis de maior importância para este modelo, assim como sua matriz de confusão\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictions &lt;- final_fit %&gt;%\n  predict(new_data = test)\n\nresults &lt;- bind_cols(test, predictions)\n\nresults %&gt;%\n  conf_mat(salary,.pred_class)\n\n          Truth\nPrediction    0    1\n         0 4636  584\n         1  308  985"
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#comparando-desempenho-dos-modelos",
    "href": "presentations/classificacao_salario/trabalho.html#comparando-desempenho-dos-modelos",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Comparando desempenho dos modelos",
    "text": "Comparando desempenho dos modelos\n\n\nModelo logístico\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8407800\n\n\nspecificity\n0.5863607\n\n\nsensitivity\n0.9215210\n\n\n\n\n\n\nLasso\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8426224\n\n\nspecificity\n0.5844487\n\n\nsensitivity\n0.9245550\n\n\n\n\n\n\nXGBoost\n\n\n\n\n\n.metric\n.estimate\n\n\n\n\naccuracy\n0.8630431\n\n\nspecificity\n0.6277884\n\n\nsensitivity\n0.9377023\n\n\n\n\n\n\n\nO maior desafio para estes dados era capturar a especificidade (salário &gt;50k). Vemos que todos os modelos tiveram dificuldade com esta métrica, porém houve um ganho sensível do modelo de árvore em relação aos modelos baseados em regressão."
  },
  {
    "objectID": "presentations/classificacao_salario/trabalho.html#referências",
    "href": "presentations/classificacao_salario/trabalho.html#referências",
    "title": "Classificação salarial via algorítmos de Machine Learning",
    "section": "Referências",
    "text": "Referências\nFonte dos dados.\nhttps://www.tidymodels.org\nDocumentação XGBoost.\nPinheiro, João Manoel Herrera. Um estudo sobre Algoritmos de Boosting e a Otimização de Hiperparâmetros Utilizando Optuna. São Carlos, SP. 2023.\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html",
    "href": "presentations/classificação binária de decisões/trabalho_final.html",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "",
    "text": "Mostrar o código\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(MASS,tidyverse, readxl, janitor,xgboost,tidymodels,vip,rpart.plot,\n               conflicted,tictoc,finetune,doParallel,DataExplorer,future,\n               gridExtra,compareGroups,DT,naivebayes,discrim,baguette,progressr,\n               corrplot,skimr,GGally,glmnet,class,themis,kknn,gt,ranger,\n               kernlab,qgraph)\n\nconflicts_prefer(dplyr::filter)\ntidymodels_prefer()\n\nknitr::opts_chunk$set(echo = TRUE)\nMostrar o código\ndf &lt;- read_excel(\"../dados/decisoes_24.xlsx\", na = c(\"-\",NA))\nPrimeiramente, os dados necessitam de algumas etapas de engenharia de features, afim de obter variáveis mais adequadas para modelagem, eliminando também fatores redundantes, codificando de forma melhorada para visualização gráfica e binarização do tipo de decisão.\nMostrar o código\ndf = clean_names(df)\n\ndf &lt;- df %&gt;%\n  mutate(favoravel_reu = case_when(\n    andamento_decisao %in% c(\"Agravo de instrumento provido\", \"Agravo provido e desde logo provido o RE\", \n                             \"Agravo provido e desde logo provido parcialmente o RE\", \"Agravo provido e determinada a devolução pelo regime da repercussão geral\", \n                             \"Concedida a ordem\", \"Concedida a ordem de ofício\", \"Concedida a segurança\", \"Concedida a suspensão\", \n                             \"Concedida em parte a ordem\", \"Concedida em parte a segurança\", \"Conhecido e provido\", \n                             \"Conhecido e provido em parte\", \"Homologado o acordo\", \"Procedente\", \"Procedente em parte\", \n                             \"Provido\", \"Provido em parte\", \"Revogada a prisão\") ~ '1',\n    andamento_decisao %in% c(\"Agravo não provido\", \"Agravo provido e desde logo negado seguimento ao RE\", \n                             \"Conhecido e negado provimento\", \"Conhecido em parte e nessa parte negado provimento\", \n                             \"Denegada a ordem\", \"Denegada a segurança\", \"Denegada a suspensão\", \n                             \"Determinado arquivamento\", \"Extinto o processo\", \"Improcedente\", \n                             \"Não provido\", \"Negado seguimento\", \"Negado seguimento por ausência de preliminar, art. 327 do RISTF\", \n                             \"Rejeitada a denúncia\", \"Rejeitada a queixa\", \"Rejeitados\") ~ '0',\n    TRUE ~ \"outros\"\n  )) %&gt;%\n  dplyr::filter(favoravel_reu %in% c('0','1')) %&gt;%\n  mutate(ramo_direito = str_extract(ramo_direito, \"^[^|]+\") %&gt;% str_trim()) %&gt;%\n  select(classe,nome_ministro_a,indicador_virtual,origem_da_decisao,ramo_direito,favoravel_reu)%&gt;%\n  mutate(across(everything(), as.factor)) %&gt;%\n  mutate(indicador_virtual = factor(ifelse(indicador_virtual == \"MONOCRÁTICA\", \"Monocrática\", \"Não monocrática\"))) %&gt;%\n  select(-origem_da_decisao) %&gt;%\n  rename(tipo = indicador_virtual,\n         ministro = nome_ministro_a)\n\n# Análise descritiva ----\ndata = df %&gt;% mutate(favoravel_reu = factor(ifelse(favoravel_reu == 0,\"Decisão desfavorável ao réu\",\"Decisão favorável ao réu\"))) %&gt;% as.data.frame()\n\nlevels(data$ramo_direito) &lt;- c(\"Educação\", # \"DIREITO À EDUCAÇÃO\"  \n                               \"Adm Público\", # \"DIREITO ADMINISTRATIVO E OUTRAS MATÉRIAS DE DIREITO PÚBLICO\"\n                               \"Ambiental\", # \"DIREITO AMBIENTAL\"\n                               \"Assistencial\", # \"DIREITO ASSISTENCIAL\" \n                               \"Civil\", # \"DIREITO CIVIL\"\n                               \"Criança e Adolescente\", # \"DIREITO DA CRIANÇA E DO ADOLESCENTE\"\n                               \"Saúde\", # \"DIREITO DA SAÚDE\"\n                               \"Consumidor\", # \"DIREITO DO CONSUMIDOR\"\n                               \"Trabalho\", # \"DIREITO DO TRABALHO\"\n                               \"Eleitoral\", # \"DIREITO ELEITORAL\"\n                               \"Eleitoral STF\", # \"DIREITO ELEITORAL E PROCESSO ELEITORAL DO STF\"\n                               \"Internacional\", # \"DIREITO INTERNACIONAL\"\n                               \"Marítimo\", # \"DIREITO MARÍTIMO\"\n                               \"Penal\", # \"DIREITO PENAL\"\n                               \"Penal Militar\", #\"DIREITO PENAL MILITAR\"\n                               \"Previdenciário\", # \"DIREITO PREVIDENCIÁRIO\"\n                               \"Processual Civil e Trabalho\", # \"DIREITO PROCESSUAL CIVIL E DO TRABALHO\"\n                               \"Processual Penal\", # \"DIREITO PROCESSUAL PENAL\"\n                               \"Processual Penal Militar\", # \"DIREITO PROCESSUAL PENAL MILITAR\"\n                               \"Tributário\", # \"DIREITO TRIBUTÁRIO\"\n                               \"Alta Complexidade\", # \"QUESTÕES DE ALTA COMPLEXIDADE, GRANDE IMPACTO E REPERCUSSÃO\"\n                               \"Registros Públicos\" # \"REGISTROS PÚBLICOS\"\n                               )\n\nlevels(data$ministro) &lt;- c(\"Min. Alexandre de Moraes\",\n                                  \"Min. André Mendonça\",\n                                  \"Min. Cármen Lúcia\",\n                                  \"Min. Cristiano Zanin\",\n                                  \"Min. Dias Toffoli\",\n                                  \"Min. Edson Fachin\",\n                                  \"Min. Flávio Dino\",\n                                  \"Min. Gilmar Mendes\",\n                                  \"Min. Luiz Fux\",\n                                  \"Min. Nunes Marques\")\n\nrm(df)\nPodemos construir gráficos, afim de avaliar se existe variância nas covariáveis — pré requisito fundamental para uma modelagem classificatória.\nMostrar o código\np1 = ggplot(data, aes(x = classe, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np2 = ggplot(data, aes(x = ministro, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np3 = ggplot(data, aes(x = tipo, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\np4 = ggplot(data, aes(x = ramo_direito, fill = favoravel_reu)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"\",x = \"\", y = \"Prop\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-classe",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-classe",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Decisão por classe",
    "text": "Decisão por classe\n\n\nMostrar o código\np1 \n\n\n\n\n\n\n\n\n\nNotamos que é bastante heterogêneo o tipo de decisão de acordo com a classe processual, o que indica que esta variável possivelmente será importante na modelagem."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#ministro",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#ministro",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Ministro",
    "text": "Ministro\n\n\nMostrar o código\np2\n\n\n\n\n\n\n\n\n\nQuanto ao ministro que proferiu a decisão, existe uma distribuição mais uniforme do que em relação à classe. Ainda assim, é possível ver diferenças em alguns casos, e também a combinação desta covariável com outras pode ser bastante heterogênea."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#tipo-de-decisão",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#tipo-de-decisão",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Tipo de decisão",
    "text": "Tipo de decisão\n\n\nMostrar o código\np3\n\n\n\n\n\n\n\n\n\nAqui vemos uma grande diferença, onde decisões não monocráticas são fortemente mais inclinadas a serem favoráveis ao réu ante as monocráticas. Ainda assim, é importante notar que as decisões monocráticas representam mais de 90% do conjunto de dados."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-ramo-do-direito",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#decisão-por-ramo-do-direito",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Decisão por ramo do direito",
    "text": "Decisão por ramo do direito\n\n\nMostrar o código\np4\n\n\n\n\n\n\n\n\n\nAssim como na classe do processo, o ramo do direito também é bastante heterogêneo, e possivelmente significativo na modelagem.\nPodemos produzir uma tabela para entender melhor estas proporções para cada um dos dois grupos\n\n\nMostrar o código\ncomp = compareGroups(favoravel_reu ~ .,\n                     data=data, method = 4, max.ylev=100, max.xlev=100)\ntable = createTable(comp)\nexport2md(table,\n          strip=TRUE,\n          first.strip=TRUE,\n          format='html',\n          size = 10,\n          caption = \"\")\n\n\n\n\n\n\n\nDecisão desfavorável ao réu\nDecisão favorável ao réu\np.overall\n\n\n\n\n\nN=30256\nN=8554\n\n\n\nclasse:\n\n\n.\n\n\nAC\n1 (0.00%)\n0 (0.00%)\n\n\n\nACO\n25 (0.08%)\n40 (0.47%)\n\n\n\nADC\n3 (0.01%)\n3 (0.04%)\n\n\n\nADI\n99 (0.33%)\n136 (1.59%)\n\n\n\nADO\n2 (0.01%)\n3 (0.04%)\n\n\n\nADPF\n26 (0.09%)\n35 (0.41%)\n\n\n\nAI\n26 (0.09%)\n5 (0.06%)\n\n\n\nAO\n89 (0.29%)\n10 (0.12%)\n\n\n\nAP\n9 (0.03%)\n343 (4.01%)\n\n\n\nAR\n65 (0.21%)\n8 (0.09%)\n\n\n\nARE\n7779 (25.7%)\n1042 (12.2%)\n\n\n\nCC\n8 (0.03%)\n48 (0.56%)\n\n\n\nEP\n2 (0.01%)\n0 (0.00%)\n\n\n\nExt\n43 (0.14%)\n29 (0.34%)\n\n\n\nHC\n10429 (34.5%)\n551 (6.44%)\n\n\n\nHD\n4 (0.01%)\n0 (0.00%)\n\n\n\nInq\n14 (0.05%)\n0 (0.00%)\n\n\n\nMI\n20 (0.07%)\n0 (0.00%)\n\n\n\nMS\n286 (0.95%)\n55 (0.64%)\n\n\n\nPet\n295 (0.98%)\n116 (1.36%)\n\n\n\nPPE\n20 (0.07%)\n1 (0.01%)\n\n\n\nRcl\n5877 (19.4%)\n4316 (50.5%)\n\n\n\nRE\n3133 (10.4%)\n1699 (19.9%)\n\n\n\nRHC\n1878 (6.21%)\n106 (1.24%)\n\n\n\nRMS\n104 (0.34%)\n8 (0.09%)\n\n\n\nRvC\n15 (0.05%)\n0 (0.00%)\n\n\n\nTPA\n4 (0.01%)\n0 (0.00%)\n\n\n\nministro:\n\n\n&lt;0.001\n\n\nMin. Alexandre de Moraes\n2759 (9.12%)\n1207 (14.1%)\n\n\n\nMin. André Mendonça\n3168 (10.5%)\n1007 (11.8%)\n\n\n\nMin. Cármen Lúcia\n2592 (8.57%)\n726 (8.49%)\n\n\n\nMin. Cristiano Zanin\n3472 (11.5%)\n860 (10.1%)\n\n\n\nMin. Dias Toffoli\n2950 (9.75%)\n919 (10.7%)\n\n\n\nMin. Edson Fachin\n2884 (9.53%)\n597 (6.98%)\n\n\n\nMin. Flávio Dino\n3948 (13.0%)\n595 (6.96%)\n\n\n\nMin. Gilmar Mendes\n2874 (9.50%)\n1119 (13.1%)\n\n\n\nMin. Luiz Fux\n2830 (9.35%)\n711 (8.31%)\n\n\n\nMin. Nunes Marques\n2779 (9.18%)\n813 (9.50%)\n\n\n\ntipo:\n\n\n0.000\n\n\nMonocrática\n30118 (99.5%)\n7941 (92.8%)\n\n\n\nNão monocrática\n138 (0.46%)\n613 (7.17%)\n\n\n\nramo_direito:\n\n\n.\n\n\nEducação\n72 (0.24%)\n36 (0.42%)\n\n\n\nAdm Público\n5665 (18.7%)\n1946 (22.7%)\n\n\n\nAmbiental\n175 (0.58%)\n29 (0.34%)\n\n\n\nAssistencial\n11 (0.04%)\n0 (0.00%)\n\n\n\nCivil\n770 (2.54%)\n127 (1.48%)\n\n\n\nCriança e Adolescente\n101 (0.33%)\n5 (0.06%)\n\n\n\nSaúde\n134 (0.44%)\n74 (0.87%)\n\n\n\nConsumidor\n209 (0.69%)\n21 (0.25%)\n\n\n\nTrabalho\n1597 (5.28%)\n2218 (25.9%)\n\n\n\nEleitoral\n191 (0.63%)\n13 (0.15%)\n\n\n\nEleitoral STF\n19 (0.06%)\n1 (0.01%)\n\n\n\nInternacional\n83 (0.27%)\n43 (0.50%)\n\n\n\nMarítimo\n1 (0.00%)\n0 (0.00%)\n\n\n\nPenal\n4816 (15.9%)\n377 (4.41%)\n\n\n\nPenal Militar\n79 (0.26%)\n10 (0.12%)\n\n\n\nPrevidenciário\n530 (1.75%)\n95 (1.11%)\n\n\n\nProcessual Civil e Trabalho\n2664 (8.80%)\n1502 (17.6%)\n\n\n\nProcessual Penal\n10414 (34.4%)\n1375 (16.1%)\n\n\n\nProcessual Penal Militar\n92 (0.30%)\n12 (0.14%)\n\n\n\nTributário\n2607 (8.62%)\n666 (7.79%)\n\n\n\nAlta Complexidade\n22 (0.07%)\n4 (0.05%)\n\n\n\nRegistros Públicos\n4 (0.01%)\n0 (0.00%)\n\n\n\n\n\n\n\n\n\nO teste qui-quadrado que acompanha a tabela na ultima coluna mostra diferença entre as categorias para todos os casos."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#divisão-treino-teste",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#divisão-treino-teste",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Divisão treino-teste",
    "text": "Divisão treino-teste\n\n\nMostrar o código\ndata = data %&gt;%\n  mutate(favoravel_reu = factor(ifelse(favoravel_reu == \"Decisão desfavorável ao réu\",0,1)))\n\nset.seed(150167636)\nsplit &lt;- initial_split(data, prop = 0.80, strata = favoravel_reu)\ntreino &lt;- training(split)\nteste &lt;- testing(split)\n\n\n\n\nMostrar o código\nset.seed(150167636)\ncv_folds &lt;- vfold_cv(treino, \n                     v = 3, \n                     strata = favoravel_reu)\n\n\nFaremos uma divisão clássica de treino e teste, com proporção 80-20. Estratificamos estes conjuntos pela variável explicativa, e utilizaremos validação cruzada para validação de hiperparâmetros dos modelos.\nFizemos já também a re-condificação da variável resposta, sendo 0: decisão desfavorável ao réu, e 1: decisão favorável ao réu."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#receita",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#receita",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Receita",
    "text": "Receita\n\n\nMostrar o código\nreceita &lt;- recipe(favoravel_reu ~ ., data = data) %&gt;%\n  # step_interact(terms = ~ all_factor_predictors():all_factor_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_numeric_predictors())\n\n\nNeste caso, utilizaremos uma receita bem simples, simplesmente criando dummys para todas as covariáveis, e removendo alguma possível coluna de apenas zeros."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#visualizando-dados-após-aplicar-receita",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#visualizando-dados-após-aplicar-receita",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Visualizando dados após aplicar receita",
    "text": "Visualizando dados após aplicar receita\n\n\nMostrar o código\nreceita %&gt;%\n  prep() %&gt;%\n  juice() %&gt;%\n  head(10) %&gt;%\n  datatable()\n\n\n\n\n\n\nPodemos ver agora que nossos dados são simplesmente colunas de zeros e uns."
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#workflow",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#workflow",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Workflow",
    "text": "Workflow\nO nosso objetivo neste trabalho não será somente tentar modelar estes dados, mas testar diversos modelos entre os vistos (e também alguns não vistos) em aula, afim de fazer comparação de eficiência entre os modelos para este conjunto de dados. O {framework} do {tidymodels} favorece este tipo de aplicação, sendo possível trabalhar com diversos modelos existentes no pacote, seja para regressão, seja para classificação (nesta caso, estaremos fazendo classificação), e depois aproveitar de funções do tidymodels para compará-los todos de uma vez.\nSelecionou-se o Knn, Naive Bayes, discriminantes linear e quadrático, regressão logística, árvore de decisão, floresta de decisão de floresta de decisão com bagging para esta aplicação. Foram testados preliminarmente modelos de SVM e XGBoost também, porém estes demoravam demais (mais de um dia executando), portanto não se mostraram nada parcimoniosos, o que inviabilizaram a sua utilização neste relatório. Havia também a intenção de utilização do Catboost, que possivelmente performaria bem sobre este conjunto de dados, que é totalmente categórico. Infelizmente, o suporte para R foi depreciado, e até tentamos utilizá-lo em sua forma legada, mas não funcionou apesar de diversas tentativas. Desta forma, ficamos com 8 modelos.\n\n\nMostrar o código\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\nnbayes_spec &lt;- naive_Bayes(smoothness = tune(), Laplace = tune()) %&gt;%\n  set_engine(\"klaR\") %&gt;%\n  set_mode(\"classification\")\n\nlda_spec &lt;- discrim_linear() %&gt;%\n  set_engine(\"MASS\") %&gt;%\n  set_mode(\"classification\")\n\nqda_spec &lt;- discrim_quad() %&gt;%\n  set_engine(\"MASS\") %&gt;%\n  set_mode(\"classification\")\n\nreg_log_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(engine = \"glmnet\", standardize = FALSE) %&gt;%\n  set_mode(\"classification\")\n\ndec_tree &lt;- decision_tree(cost_complexity = tune(),\n                          min_n = tune(),\n                          tree_depth = tune()) %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nbagg_tree &lt;- bag_tree(cost_complexity = tune(),\n                      min_n = tune(),\n                      tree_depth = tune()) %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nrandom_forestst &lt;- rand_forest(min_n = tune(),\n                             trees = tune()) %&gt;%\n  set_engine(engine = \"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\nMostrar o código\nwf = workflow_set(\n  preproc = list(receita),\n  models = list(\n    KNN = knn_spec,\n    Naive_Bayes = nbayes_spec,\n    LDA = lda_spec,\n    QDA = qda_spec,\n    Reg_log = reg_log_spec,\n    decision = dec_tree,\n    bag_tree = bagg_tree,\n    random_forest = random_forestst\n  )\n) %&gt;%\n  mutate(wflow_id = gsub(\"(recipe_)\", \"\", wflow_id))\n\n\nDestes modelos, iremos fazer diversos ajustes de hiperparâmetros, afim de selecionar não só o melhor modelo, mas também a melhor combinação de hiperparâmetros. construiu-se portanto com grid de 20 combinações de hiperparâmetros para cada modelo, utilizando um hipercubo latino. Estes serão avaliados por validação cruzada. Fizemos a divisão treino-teste 80%/20%, e do conjunto de treino selecionamos a técnica v-folds cross validation, com \\(v=3\\). Salvo os modelos de discriminante linear e quadrático que não realizamos ajuste de hiperparâmetros — serão treinados portanto \\(3 \\times 2 = 6\\) modelos deste tipo — os demais modelos serão treinados \\(20 \\times 3 = 60\\) vezes cada, totalizando \\(20 \\times 3 \\times 6 + 2 \\times 6 = 366\\) modelos ao todo que serão ajustados, para um conjunto de dados relativamente grande. Portanto, este ajuste demorou cerca de 20 minutos para rodar, o que é relativamente aceitável dado o tamanho do conjunto de dados e a quantidade de modelos que estamos ajustando.\n\n\nMostrar o código\nplan(multisession)\n\ngrid_ctrl = control_grid(\n  save_pred = TRUE,\n  parallel_over = \"resamples\",\n  save_workflow = TRUE\n)\n\ngrid_results = wf %&gt;%\n  workflow_map(\n    seed = 150167636,\n    resamples = cv_folds,\n    grid = 20,\n    control = grid_ctrl\n  )"
  },
  {
    "objectID": "presentations/classificação binária de decisões/trabalho_final.html#comparando-modelos",
    "href": "presentations/classificação binária de decisões/trabalho_final.html#comparando-modelos",
    "title": "Classificação binária aplicada a dados do Supremo Tribunal Federal",
    "section": "Comparando modelos",
    "text": "Comparando modelos\n\n\nMostrar o código\nautoplot(grid_results)\n\n\n\n\n\n\n\n\n\nPelo gráfico comparando os modelos ajustados, notamos que o que teve melhor ajuste aparenta ter sido o modelo de florestas aleatórias, obtendo a maior acurácia e maior proporção de área sobre a curva ROC. Árvores com bagging e árvores de decisão também performaram bem, indicando que neste caso os modelos de árvores aparentam ter sido os melhores.\n\n\nMostrar o código\nautoplot(grid_results, select_best = TRUE, metric = \"roc_auc\")\n\n\n\n\n\n\n\n\n\nAnalisando a performance da melhor combinação de hiperparâmetros para cada modelo testado, notamos novamente que os três melhores modelos são os baseados em árvores. O pior algoritmo foi o KNN, seguido do naive bayes.\n\n\nMostrar o código\nbest_set_linear = grid_results %&gt;% \n  extract_workflow_set_result(\"Reg_log\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_knn = grid_results %&gt;% \n  extract_workflow_set_result(\"KNN\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_nbayes = grid_results %&gt;%\n  extract_workflow_set_result(\"Naive_Bayes\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_lda = grid_results %&gt;% \n  extract_workflow_set_result(\"LDA\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_qda = grid_results %&gt;% \n  extract_workflow_set_result(\"QDA\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_rand_fore = grid_results %&gt;% \n  extract_workflow_set_result(\"random_forest\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_decision = grid_results %&gt;% \n  extract_workflow_set_result(\"decision\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nbest_set_bag = grid_results %&gt;% \n  extract_workflow_set_result(\"bag_tree\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nresultado_teste &lt;- function(rc_rslts, fit_obj, par_set, split_obj) {\n  res &lt;- rc_rslts %&gt;%\n    extract_workflow(fit_obj) %&gt;%\n    finalize_workflow(par_set) %&gt;%\n    last_fit(split = split_obj,\n             metrics = metric_set(\n              accuracy,roc_auc,\n              f_meas,precision,\n              recall,spec,kap))\n  res\n}\n\nresultado_teste_reg_log &lt;- resultado_teste(grid_results, \"Reg_log\", best_set_linear, split)\nresultado_teste_knn &lt;- resultado_teste(grid_results, \"KNN\", best_set_knn, split)\nresultado_teste_lda &lt;- resultado_teste(grid_results, \"LDA\", best_set_lda, split)\nresultado_teste_qda &lt;- resultado_teste(grid_results, \"QDA\", best_set_qda, split)\nresultado_teste_naive &lt;- resultado_teste(grid_results, \"Naive_Bayes\", best_set_nbayes, split)\nresultado_teste_decision &lt;- resultado_teste(grid_results, \"decision\", best_set_decision, split)\nresultado_teste_bag &lt;- resultado_teste(grid_results, \"bag_tree\", best_set_bag, split)\nresultado_teste_random_forest &lt;- resultado_teste(grid_results, \"random_forest\", best_set_rand_fore, split)\n\nmetrics_table &lt;- rbind(collect_metrics(resultado_teste_reg_log)$.estimate, \n                       collect_metrics(resultado_teste_knn)$.estimate, \n                       collect_metrics(resultado_teste_lda)$.estimate, \n                       collect_metrics(resultado_teste_qda)$.estimate, \n                       collect_metrics(resultado_teste_naive)$.estimate, \n                       collect_metrics(resultado_teste_decision)$.estimate, \n                       collect_metrics(resultado_teste_bag)$.estimate,\n                       collect_metrics(resultado_teste_random_forest)$.estimate)\n\nmetrics_table &lt;- round(metrics_table, 4)\n\nrow_names &lt;- c(\"Regressão Logística\", \"KNN\", \"Discriminante Linear\", \"Discriminante Quadrático\", \"Naive Bayes\", \"Árvore de Decisão\", \"Bagged Tree\", \"Floresta Aleatória\")\n\nmetrics_table &lt;- cbind(row_names, metrics_table)\n\nmetrics_table &lt;- metrics_table %&gt;% \n  as_tibble()\n\ncolnames(metrics_table) &lt;- c(\"Método\", \"Acurácia\", \"Curva Roc\", \"f_means\", \"Precisão\", \"Recall\", \"Especificidade\", \"Kappa\")\n\nmetrics_table &lt;- metrics_table %&gt;% \n  mutate(Acurácia = as.numeric(Acurácia), \n         `Curva Roc` = as.numeric(`Curva Roc`), \n         f_means = as.numeric(f_means), \n         Precisão = as.numeric(Precisão), \n         Recall = as.numeric(Recall), \n         Especificidade = as.numeric(Especificidade), \n         Kappa = as.numeric(Kappa)) %&gt;% \n  arrange(desc(Acurácia), desc(`Curva Roc`), desc(f_means), desc(Kappa)) %&gt;%\n  select(Método,Acurácia,`Curva Roc`,Especificidade)\n  \nmetrics_table %&gt;%  \n  gt::gt() %&gt;% \n  gt::tab_header(\n    title = gt::html(\"&lt;b&gt; Resultado dos modelos nos dados de teste&lt;/b&gt;\"), \n    subtitle = glue::glue(\"De acordo com algumas métricas\")) %&gt;% \n  gt::data_color(\n    columns = Acurácia, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 10, palette = \"Green\"), \n      domain = c(min(metrics_table$Acurácia), max(metrics_table$Acurácia)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n  gt::data_color(\n    columns = `Curva Roc`, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 10, palette = \"Green\"), \n      domain = c(min(metrics_table$`Curva Roc`), max(metrics_table$`Curva Roc`)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n    gt::data_color(\n    columns = Especificidade, \n    colors = scales::col_numeric(\n      palette = colorspace::sequential_hcl(n = 8, palette = \"Red\"), \n      domain = c(max(metrics_table$Especificidade),min(metrics_table$Especificidade)),\n      reverse = TRUE\n    )\n  ) %&gt;% \n  cols_align(align = \"center\", columns = everything()) %&gt;% \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ), \n    locations = cells_body(columns = c(Acurácia, `Curva Roc`))\n  )\n\n\n\n\n\n\n\n\n\nResultado dos modelos nos dados de teste\n\n\nDe acordo com algumas métricas\n\n\nMétodo\nAcurácia\nCurva Roc\nEspecificidade\n\n\n\n\nFloresta Aleatória\n0.8256\n0.8961\n0.3699\n\n\nÁrvore de Decisão\n0.8244\n0.8951\n0.3714\n\n\nBagged Tree\n0.8230\n0.8941\n0.3694\n\n\nRegressão Logística\n0.8122\n0.8872\n0.3407\n\n\nDiscriminante Linear\n0.8046\n0.8815\n0.3344\n\n\nNaive Bayes\n0.7796\n0.8761\n0.0000\n\n\nKNN\n0.7795\n0.8760\n0.0004\n\n\nDiscriminante Quadrático\n0.7306\n0.8088\n0.3705\n\n\n\n\n\n\n\n\nDesta tabela, vemos a real situação do ajuste. Apesar de bons valores de acurácia e roc auc, a realidade é que todos os modelos performaram mal quando analisada a métrica especificidade. Ou seja, alguns foram até eficientes em predizer verdadeiros zeros, mas todos foram ruins em predizer verdadeiros um (alguns piores que outros). Desta forma, todos os modelos apresentam uma alta taxa de erro tipo 1. Na realidade, estes dados eram bastante desbalanceados, com em torno de 30mil zeros e 8mil uns. Desta forma, é razoável dizer que nenhum modelo pode ser escolhido para esta análise.\n\n\nMostrar o código\npredictions &lt;- resultado_teste_random_forest %&gt;%\n  collect_predictions()\n\nconfusion_matrix &lt;- predictions %&gt;%\n  conf_mat(truth = favoravel_reu, estimate = .pred_class)\n\nautoplot(confusion_matrix, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\nVisualizando a matriz de confusão do melhor modelo (floresta aleatória), notamos o observado acima. A classificação de verdadeiros 1 (decisão favorável ao réu) é extremamente ineficaz.\nDesta forma, podemos concluir que este conjunto de dados é de separação difícil, sendo necessário um estudo muito mais aprofundado das covariáveis — é interessante notar que estes dados são públicos, e foram obtidos de Corte aberta. No entanto, nem todas as covariáveis do processo estão disponíveis de forma pública e direta, como por exemplo o número de embargos que o processo sofreu, algumas tipificações extras, o mérito do processo, etc. Portanto, para tentar realizar uma análise classificatória, é necessário um conhecimento e um conjunto de dados muito mais holístico a fim de obter um ajuste mais preciso."
  },
  {
    "objectID": "presentations/nlp/index.html#introdução",
    "href": "presentations/nlp/index.html#introdução",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Introdução",
    "text": "Introdução\n\n\nNo campo do direito, a aplicação de técnicas estatísticas vem sendo testada em diversos âmbitos, inclusive no Supremo Tribunal Federal do Brasil.\nÉ de interesse do tribunal a aplicação destas técnicas para agrupamento de processos. Um agrupador poderia ajudar a identificar processos semelhantes, trabalho este feito manualmente.\nEste trabalho busca estudar e aplicar algumas destas técnicas para o desenvolvimento de uma aplicação prática no STF, com objetivo de agrupar processos de controle concentrado."
  },
  {
    "objectID": "presentations/nlp/index.html#objetivos",
    "href": "presentations/nlp/index.html#objetivos",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Objetivos",
    "text": "Objetivos\n\nO objetivo deste trabalho é formular um agregador de processos de controle concentrado, que são processos que tratam da constitucionalidade de leis e atos normativos. Constituem o dito controle concentrado os processos do Supremo Tribunal Federal das seguintes classes:\n\n\nADI (Ação Direta de Inconstitucionalidade)\nADC (Ação Declaratória de Constitucionalidade)\nADPF (Arguição de Descumprimento de Preceito Fundamental)\nADO (Ação Direta de Inconstitucionalidade por Omissão)"
  },
  {
    "objectID": "presentations/nlp/index.html#objetivos-1",
    "href": "presentations/nlp/index.html#objetivos-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Objetivos",
    "text": "Objetivos\n\nO agrupador fornecerá subsídios aos responsáveis pelo encaminhamento dos processos que chegam ao STF, visando reduzir o trabalho mecânico humano.\nDos objetivos específicos, espera-se:\n\n\nProcessar os dados utilizando técnicas de Processamento de Linguagem Natural (PLN), transformando petições iniciais de processos em vetores numéricos;\nComparar técnicas de agrupamento;\nAvaliar a similaridade entre processos em recortes temporais distintos;\nEstudar técnicas de PLN, análise multivariada e visualização de dados."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia",
    "href": "presentations/nlp/index.html#metodologia",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\nTendo os dados e o modelo pré-treinado para vetorização, os códigos Python e R para a vetorização dos textos, e posterior análise, são da seguinte forma:\n\n\n\n# Módulos\nimport polars\nimport gensim\nfrom gensim.models.doc2vec import Doc2Vec\n\n# Função\ndef infer_vector(text):\n    return model.infer_vector(text.split())\n\n# Modelo pré-treinado para Embedding\nmodel = gensim.models.Doc2Vec.load(\"modelo.model\")\n\n# Dados\ndf = polars.read_csv(\"dados.csv\",columns=[1,3,4])\n\n# Saída: DataFrame com duas colunas: Texto original e vetor Embedding correspondente.\ndf = df.with_columns_seq(polars.col(\"texto\").apply(infer_vector).alias(\"vetor\"))\n\n\n\n# Pacote\nlibrary(reticulate)\n\n# Definindo o ambiente virtual python\nreticulate::use_condaenv(\"TCC\")\n\n# Executando o script python\nreticulate::source_python(\"script.py\")\n\n# Ajustando o dataframe trazido do python para formato R mais adequado\ndf &lt;- as.data.frame(do.call(rbind, lapply(a, function(x) c(x[[1]], x[[2]]))), stringsAsFactors = FALSE)"
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-1",
    "href": "presentations/nlp/index.html#metodologia-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\nPassos para a construção do agregador:\n\n\n\n\n\nObtenção dos dados:\n\n\nOs dados foram disponibilizados pelo STF (mas estão disponíveis publicamente no Portal do STF.).\n\n\nVetorização (incluindo ocerização e processamento do texto PDF):\n\n\nEste módulo foi fornecido pelo STF (dados em formato CSV)"
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-2",
    "href": "presentations/nlp/index.html#metodologia-2",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nDefinir recortes temporais para a agregação:\n\n\nPor conta da natureza cíclica dos processos que compõem o acervo do STF, é necessário um sistema de atualização constante para uma aplicação prática.\nSerá realizado o agrupamento com dados em recortes temporais distintos, e, em cada recorte, será avaliada a similaridade entre os processos em tramitação naquela data."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-3",
    "href": "presentations/nlp/index.html#metodologia-3",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nAplicação de medidas de distância para comparar a similaridade dos processos (distância euclidiana, distância do cosseno etc)."
  },
  {
    "objectID": "presentations/nlp/index.html#metodologia-4",
    "href": "presentations/nlp/index.html#metodologia-4",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Metodologia",
    "text": "Metodologia\n\n\n\n\nPara a formação dos agrupamentos, serão utilizadas técnicas de agrupamento hierárquico e não-hierárquico baseadas nas distâncias calculadas.\n\n\nPara a visualização dos dados, serão estudadas técnicas como dendrogramas e t-SNE."
  },
  {
    "objectID": "presentations/nlp/index.html#cronograma",
    "href": "presentations/nlp/index.html#cronograma",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Cronograma",
    "text": "Cronograma\n\n\nCronograma TCC 1\n\n\nAtividades\nMar\nAbr\nMai\nJun\nJul\n\n\n\n\nEscolha do tema a ser abordado.\n\n\n\n\n\n\n\nLevantamento de bibliografia relacionada ao tema.\n\n\n\n\n\n\n\nDefinição do recorte temporal com a AAJ do STF.\n\n\n\n\n\n\n\nSolicitação dos dados para a STI do STF.\n\n\n\n\n\n\n\nSolicitação dos algoritmos à STI do STF.\n\n\n\n\n\n\n\nRevisão de literatura.\n\n\n\n\n\n\n\nDesenvolvimento da proposta de projeto.\n\n\n\n\n\n\n\nAnálise preliminar do banco de dados.\n\n\n\n\n\n\n\nEntrega da proposta do projeto.\n\n\n\n\n\n\n\nElaboração da apresentação da proposta.\n\n\n\n\n\n\n\nManipulação do banco de dados.\n\n\n\n\n\n\n\nAnálise do banco de dados.\n\n\n\n\n\n\n\nElaboração do relatório parcial.\n\n\n\n\n\n\n\nEntrega do relatório parcial a Professora Orientadora.\n\n\n\n\n\n\n\nCorreção do relatório parcial.\n\n\n\n\n\n\n\nEntrega do relatório parcial a banca."
  },
  {
    "objectID": "presentations/nlp/index.html#cronograma-1",
    "href": "presentations/nlp/index.html#cronograma-1",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Cronograma",
    "text": "Cronograma\n\n\nCronograma TCC 2\n\n\nAtividades\nAgo\nSet\nOut\nNov\nDez\n\n\n\n\nDesenvolvimento do modelo e da aplicação.\n\n\n\n\n\n\n\nElaboração do relatório final.\n\n\n\n\n\n\n\nEntrega do relatório final a Professora Orientadora.\n\n\n\n\n\n\n\nCorreção do relatório final.\n\n\n\n\n\n\n\nEntrega do relatório final para a banca."
  },
  {
    "objectID": "presentations/nlp/index.html#referências",
    "href": "presentations/nlp/index.html#referências",
    "title": "Agrupador de processos de controle concentrado",
    "section": "Referências",
    "text": "Referências\n\nARTES, R.; BARROSO, L. P. Métodos multivariados de análise estatística. [S.l.]: São Paulo: Blucher, 2023.\nEVERITT, B.; SKRONDAL, A. The cambridge dictionary of statistics. [S.l.]: Cambridge University Press, 2010. v. 4.\nFREITAS, L. J. G. et al. Catboost algorithm application in legal texts and un 2030 agenda. Revista de Informatica Teórica e Aplicada - RITA - ISSN 2175-2745. Vol. 30, Num. 02 (2023) 51-58, 2023.\nFREITAS, L. J. G. et al. Text clustering applied to data augmentation in legal contexts. arXiv preprint arXiv:2404.08683, 2024.\nJOHNSON, R. A.; WICHERN, D. W. Applied Multivariate Statistical Analysis. [S.l.]: 6. ed.[S.l.]:Prentice Hall, 2007.\nKAUFMAN, L.; ROUSSEEUW, P. J. Finding groups in data: an introduction to cluster analysis. [S.l.]: John Wiley & Sons, 1990.\nLECUN, Y. et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, Ieee, v. 86, n. 11, p. 2278–2324, 1998.\nMAATEN, L. Van der; HINTON, G. Visualizing data using t-sne. Journal of machine learning research, v. 9, n. 11, 2008.\nMACQUEEN, J. et al. Some methods for classification and analysis of multivariate observations. [S.l.], 1967. v. 1. 281–297 p.\nMORETTIN, P. A.; SINGER, J. M. Estatística e Ciência de Dados. [S.l.]: LTC, 2021.\nRICARDO, B.-Y.; BERTHIER, R.-N. Modern information retrieval: the concepts and technology behind search. [S.l.]: New Jersey, USA: Addi-son-Wesley Professional, 2011.\nvon Borries, G.; WANG, H. Partition clustering of high dimensional low sample size data based on p-values. Computational statistics & data analysis, v. 53, n. 12, p. 3987-3998, 2009.\n\n\n\n\n\nDepartamento de estatística - UnB"
  },
  {
    "objectID": "presentations/tcc2/slide.html#introdução",
    "href": "presentations/tcc2/slide.html#introdução",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Introdução",
    "text": "Introdução\n\n\n\n\n\nO Ministro Luís Roberto Barroso assumiu a presidência do Tribunal em 2023, e estipulou como uma de suas metas de gestão a diminuição do acervo de processos de controle concentrado em 20%.\nA tarefa de busca de processos similares no acervo de controle concentrado é feita hoje manualmente. Portanto, a identificação automatizada possibilita reduzir o tempo e o esforço manual na busca de processos similares.\nNesse trabalho, buscou-se aplicar técnicas de processamento de linguagem natural (NLP) para detecção de processos semelhantes no acervo de processos de controle concentrado do Tribunal.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Recebimento] --&gt; B{Triagem Inicial};\n    B --&gt; C[Distribuição];\n    B --&gt; T1[...];\n    T1 --&gt; T2[Busca por similaridade com \\n outros processos do acervo];\n    T2 --&gt; T3[...];\n    T3 --&gt; B;\n    C --&gt; D[...];"
  },
  {
    "objectID": "presentations/tcc2/slide.html#conjunto-de-dados",
    "href": "presentations/tcc2/slide.html#conjunto-de-dados",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Conjunto de dados",
    "text": "Conjunto de dados\n\n\n\n\n\nDescriçãoComparaçãoQuantidade\n\n\n\n\nOs dados utilizados para esta análise são as petições iniciais dos processos de controle concentrado. Este é o primeiro documento que chega no Tribunal tratando de um processo.\nDesta forma, o desafio é encontrar similaridades entre processos na sua fase inicial de tramitação, tal que esta ferramenta seja desbravadora na busca de similaridades antes de outros encaminhamentos internos no Tribunal.\nAs petições iniciais são um dado público, e podem ser obtidas no Portal do STF.\n\n\n\n\n\n\nPara realizar a busca por similaridades, necessitamos fixar um processo, que chamaremos de paradigma.\nOs processos paradigmas selecionados foram aqueles que tiveram pauta ou decisão conjunta pelo Tribunal com outros processos.\nDesta forma, buscou-se encontrar técnicas que apontassem a similaridade desses processos já em suas petições iniciais.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExemplo de uma petição inicial — ADPF 857"
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-vetorização",
    "href": "presentations/tcc2/slide.html#referencial-teórico-vetorização",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — Vetorização",
    "text": "Referencial teórico — Vetorização\n\nPor se tratar de um dado textual, é necessário aplicar técnicas de vetorização ao texto.\n\n\n\n\nBag of WordsNota\n\n\n\n\n\n\nEsta é uma das formas mais simples de vetorizar um texto. Iremos simplesmente tabelar a frequência de utilização de cada um dos termos presentes no texto.\n\n\n\n\n\n\nDoc2vecNota\n\n\n\n\n\n\nEsta é uma das formas mais sofisticadas para vetorização de texto. Nela, iremos obter o vetor numérico que representa o texto pelo treinamento de uma rede neural. Os parâmetros deste modelo são atualizados buscando prever a próxima palavra dado o seu contexto e dado um vetor de palavras que formam coletivamente o texto (Freitas et al. ,2024)."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-métricas-de-similaridade",
    "href": "presentations/tcc2/slide.html#referencial-teórico-métricas-de-similaridade",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — Métricas de similaridade",
    "text": "Referencial teórico — Métricas de similaridade\n\nPara comparar os vetores criados, necessitamos de métricas de distância estatística\n\n\n\n\nDistância do CossenoNota\n\n\n\n\n\n\nPela Lei dos cossenos:\n\\[\\begin{equation}\n\\mathbf{A} \\cdot \\mathbf{B} = | \\mathbf{A}|| \\mathbf{B}|\\cos\\theta.\n\\end{equation}\\]\nPodemos utilizar, para avaliar o grau de similaridade entre os vetores \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\), a correlação entre eles. E para quantificar esta correlação, podemos utilizar o cosseno do ângulo entre estes vetores, tal que:\n\\[\\begin{equation}\\text{sim}(\\mathbf{A,B}) = \\cos\\theta = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{A}|| \\mathbf{B}|}.\n\\end{equation}\\]\nEsta é possivelmente a métrica mais utilizada no paradigma de NLP.\n\n\n\n\n\n\nDivergência de JensenNota\n\n\n\n\n\n\nUma outra métrica popular na área de Machine Learning, podemos calcular o complementar da divergência de Jensen-Shannon definida por:\n\\[\\begin{equation}\nJSD(P||Q) = \\\\\n\\frac{1}{2}(KL(P||R)+KL(Q||R)),\n\\end{equation}\\]\ncomo sendo uma medida de similaridade entre duas distribuições de probabilidade. No caso, tomamos os vetores como distribuições de probabilidade empírica. Esta é uma alternativa interessante a distância do cosseno, visto que esta métrica respeita a desigualdade triangular."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referencial-teórico-impe",
    "href": "presentations/tcc2/slide.html#referencial-teórico-impe",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referencial teórico — IMPE",
    "text": "Referencial teórico — IMPE\n\nUma terceira métrica foi proposta para a medição da similaridade entre os processos, a qual se chamou Interseção mínima de probabilidade empírica (IMPE).\n\n\nDefiniçãoImplementação\n\n\n\n\nEsta métrica foi construída inspirada no índice de Tversky.\nNela, consideraremos os vetores dos textos como distribuições de probabilidade empírica. Destes, tomaremos um menos a soma das diferenças dos absolutos das frequência relativa de um termo entre dois textos, tal que \\[\\begin{equation}\n  \\text{IMPE}(\\textbf{A},\\textbf{B}) = 1 - \\sum_{w \\in A \\cap B} |f_A(w) - f_B(w)|.\n\\end{equation}\\]\nPor somar os menores valores, esta métrica foi construída para ser conservadora, apenas acusando similaridade entre textos muito parecidos.\n\\(IMPE(\\textbf{A},\\textbf{B}) \\in (0,1)\\), em que \\(IMPE(\\textbf{A},\\textbf{B}) = 1 \\rightarrow\\) textos idênticos.\n\n\n\n\n\n# Calcular interseção mínima de probabilidade empírica entre dois documentos ----\n# Entrada: 2 df n x 2, coluna 1 \"word\" coluna 2 \"freq\"\n\nIMPE &lt;- function(df1, df2) {\n  df1$relative_freq &lt;- df1$freq / sum(df1$freq)\n  df2$relative_freq &lt;- df2$freq / sum(df2$freq)\n  common_words &lt;- merge(df1, df2, by = \"word\", suffixes = c(\"_df1\", \"_df2\"))\n  common_words$dissimilaridade &lt;- abs(common_words$relative_freq_df1 - common_words$relative_freq_df2)\n  similaridade &lt;- 1 - sum(common_words$dissimilaridade)\n  return(similaridade)\n}"
  },
  {
    "objectID": "presentations/tcc2/slide.html#resultados",
    "href": "presentations/tcc2/slide.html#resultados",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Resultados",
    "text": "Resultados\n\nForam realizados estudos de casos, onde se comparou a combinação de cada técnica de vetorização com cada métrica de similaridade para casos onde se sabia os processos que deveriam ser identificados como mais parecidos. Estes foram os resultados:\n\n\nCaso 1Caso 2Caso 3Caso 4Caso 5\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI6931 e ADI6921.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n8º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n65º mais similar\n\n\nJensen-Shannon\n3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n7º mais similar\n\n\nIMPE\n4º mais similar\n\n\nJensen-Shannon\n72º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADPF857 e ADPF743, ADPF746.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 2º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n42º e 89º mais similar\n\n\nJensen-Shannon\n1º e 3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n8º e 9º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADO54 e ADPF760.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n4º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n36º mais similar\n\n\nJensen-Shannon\n3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º mais similar\n\n\nIMPE\n8º mais similar\n\n\nJensen-Shannon\n2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI7078 e ADI7066, ADI7070.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 6º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n437º e 475º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n1º e 2º mais similar\n\n\nIMPE\n3º e 4º mais similar\n\n\nJensen-Shannon\n1º e 2º mais similar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    Tabela de Similaridades\n    \n\n\n\n\n\nSimilaridades entre ADI3318 e ADI2943, ADI3309.\n\n\nVetorização\nMétrica de similaridade\nProximidade\n\n\n\n\ndoc2vec\nDistância do cosseno\n1º e 5º mais similar\n\n\nBag of Words completo\nDistância do cosseno\n3º e 4º mais similar\n\n\nIMPE\n358º e 360º mais similar\n\n\nJensen-Shannon\n2º e 3º mais similar\n\n\nBag of Words reduzido\nDistância do cosseno\n2º e 3º mais similar\n\n\nIMPE\n6º e 7º mais similar\n\n\nJensen-Shannon\n2º e 4º mais similar"
  },
  {
    "objectID": "presentations/tcc2/slide.html#resultados-t-sne",
    "href": "presentations/tcc2/slide.html#resultados-t-sne",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Resultados — t-SNE",
    "text": "Resultados — t-SNE\n\nTestou-se utilizar t-SNE para visualização do acervo, com objetivo de que as semelhanças entre processos fossem preservadas\n\n\n\nTeste 1\n\n\nTeste 2\n\n\nTeste 3\n\n\n\n\nComo os resultados não foram satisfatórios, esta técnica não foi utilizada na aplicação final."
  },
  {
    "objectID": "presentations/tcc2/slide.html#conclusão",
    "href": "presentations/tcc2/slide.html#conclusão",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Conclusão",
    "text": "Conclusão\n\n\nA combinação das técnicas mais simples de vetorização Bag of Words com a distância do cosseno produziu excelentes resultados na busca de similaridade de processos.\nEste resultado mostra que técnicas simples e parcimoniosas ainda são úteis para problemas práticos no paradigma da NLP.\nA implementação de um aplicativo Shiny para utilização deste modelo fornece um valioso insumo para os analistas do Tribunal para o cumprimento da meta de diminuição do acervo de processos de controle concentrado do Ministro Presidente do STF."
  },
  {
    "objectID": "presentations/tcc2/slide.html#referências",
    "href": "presentations/tcc2/slide.html#referências",
    "title": "Similaridade de Processos Judiciários Utilizando Processamento de Linguagem Natural",
    "section": "Referências",
    "text": "Referências\n\n\nFREITAS, L. J. G. Clusterização de textos aplicada ao tratamento de dados jurídicos desbalanceados. Tese (Mestrado em Estatística) – Departamento de estatística, Universidade de Brasília, 2023.\nFREITAS, L. J. G.; ALENCAR, E.; RODRIGUES, T. C. V. Rafa 2030-deep learning applied to brazilian supreme court legal documents and un 2030 agenda. Galoá, 2022.\nFREITAS, L. J. G. et al. Catboost algorithm application in legal texts and un 2030 agenda. Revista de Informatica Teórica e Aplicada - RITA - ISSN 2175-2745. Vol. 30, Num. 02 (2023) 51-58, 2023.\nFREITAS, L. J. G. et al. Text clustering applied to data augmentation in legal contexts. arXiv preprint arXiv:2404.08683, 2024.\nLU, J.; HENCHION, M.; NAMEE, B. M. Diverging divergences: Examining variants of jensen shannon divergence for corpus comparison tasks. 2021.\nMORETTIN, P. A.; SINGER, J. M. Estatística e Ciência de Dados. 1ª. ed. Rio de Janeiro: LTC, 2023.\nSOARES, A. Introdção à análise textual aplicada à sociologia. 2022. Disponível em: ⟨https://soaresalisson.github.io/analisetextual/⟩. Acesso em: 31 de dez. de 2024.\n\n\n\n\n\n\nDepartamento de estatística - UnB"
  }
]