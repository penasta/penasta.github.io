{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGKMewnRQ1n6"
      },
      "source": [
        "Instalar optuna no Colab (único módulo dos que não usarei que não está pré instalado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUk8t1mcom6p",
        "outputId": "addb0267-3c19-4587-d084-488b9d58561a"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFGtVQfQ76M"
      },
      "source": [
        "Importando módulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Stf_WDlTU0L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBjcTK6uQ-D3"
      },
      "source": [
        "Criando conexão com o meu drive, onde subi as imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMRUWlx-0Vxy",
        "outputId": "0fc1a212-5b13-442f-8a7e-e3d34ac336c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XjCm5xXRBVw"
      },
      "source": [
        "Definindo o caminho (remoto) dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hJMHw7fDnm2W"
      },
      "outputs": [],
      "source": [
        "ROOT = '/content/drive/MyDrive/colab_data/rna1/lista5/Treino'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvTtH8uuQSW6"
      },
      "source": [
        "Definindo questões operacionais: Modo fast debug para rodar rapidinho e verificar erros e problemas, desligar para rodar de verdade\n",
        "\n",
        "fixando o tamanho da imagem para upscale - como estou usando modelos pré treinados para 224x224, a recomendação da bibliografia é (neste caso) aumentar o tamanho das imagens de 96x103 para 224x224, para garantir o correto funcionamento dos filtros kernel de imagem da forma que foram concebidos nos modelos originais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Xp4K4ScoBa5"
      },
      "outputs": [],
      "source": [
        "FAST_DEBUG = False\n",
        "if FAST_DEBUG:\n",
        "    N_EPOCHS = 2\n",
        "    N_TRIALS = 2\n",
        "    BATCH_SIZE = 16\n",
        "else:\n",
        "    N_EPOCHS = 20\n",
        "    N_TRIALS = 30\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_WIDTH  = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It94Q524QuOK"
      },
      "source": [
        "Garantindo possibilidades: rodar na GPU se possível, no processador, caso não tenha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW2wB7jkoEyA",
        "outputId": "6c44d0fb-a4ec-4d49-f16d-30b287de07b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n",
            "FAST_DEBUG: False\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", device)\n",
        "print(\"FAST_DEBUG:\", FAST_DEBUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x1w6X3dBa1h"
      },
      "source": [
        "Trazer os dados para o python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vFdzyKPfoJ2A"
      },
      "outputs": [],
      "source": [
        "def collect_image_files(root):\n",
        "    exts = (\"*.bmp\", \"*.BMP\")\n",
        "    files = []\n",
        "    for e in exts:\n",
        "        files.extend(glob.glob(os.path.join(root, e)))\n",
        "    files = sorted(files)\n",
        "    return files\n",
        "\n",
        "all_files = collect_image_files(ROOT)\n",
        "if len(all_files) == 0:\n",
        "    raise RuntimeError(f\"Nenhuma imagem encontrada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZGt0udiBWSZ"
      },
      "source": [
        "Preparando os dados pré-separação em treino e validação\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3z1-65p8oMf8"
      },
      "outputs": [],
      "source": [
        "class FingerprintDataset(Dataset):\n",
        "    def __init__(self, files_list, transform=None):\n",
        "        self.files = list(files_list)\n",
        "        self.transform = transform\n",
        "        self.labels = []\n",
        "        for f in self.files:\n",
        "            name = os.path.basename(f)\n",
        "            first = name[0].upper() if len(name) > 0 else \"\"\n",
        "            if first == \"F\":\n",
        "                self.labels.append(1)\n",
        "            elif first == \"M\":\n",
        "                self.labels.append(0)\n",
        "            else:\n",
        "                base = name.split(\"_\")[0].upper() if \"_\" in name else first\n",
        "                if base == \"F\":\n",
        "                    self.labels.append(1)\n",
        "                elif base == \"M\":\n",
        "                    self.labels.append(0)\n",
        "                else:\n",
        "                    raise ValueError(f\"Nome de arquivo não tem F/M na frente: {name}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        img = Image.open(path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fJHEiYBPG2"
      },
      "source": [
        "Criando os tensores - data augmentation sendo feito nesta etapa\n",
        "\n",
        "obs: a normalização está sendo feita com os pesos dos modelos pré treinados, em detrimento das estatísticas dos meus dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ie30LEYAoN20"
      },
      "outputs": [],
      "source": [
        "train_tf = T.Compose([\n",
        "\n",
        "    T.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    T.Grayscale(num_output_channels=3),\n",
        "\n",
        "    T.RandomApply([T.ColorJitter(brightness=0.1, contrast=0.1)], p=0.2),\n",
        "    T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))], p=0.1),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=(-3, 3)),\n",
        "    T.RandomResizedCrop(size=(IMAGE_HEIGHT, IMAGE_WIDTH), scale=(0.95, 1.0)),\n",
        "    T.RandomAffine(degrees=0, translate=(0.03, 0.03)),\n",
        "\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "    ])\n",
        "\n",
        "val_tf = T.Compose([\n",
        "\n",
        "    T.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    T.Grayscale(num_output_channels=3),\n",
        "\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-T3AMkOBK-9"
      },
      "source": [
        "Split treino/validação (80/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZH5fLQSqoO_e"
      },
      "outputs": [],
      "source": [
        "labels_all = []\n",
        "for f in all_files:\n",
        "    name = os.path.basename(f)\n",
        "    first = name[0].upper() if len(name) > 0 else \"\"\n",
        "    if first == \"F\":\n",
        "        labels_all.append(1)\n",
        "    elif first == \"M\":\n",
        "        labels_all.append(0)\n",
        "    else:\n",
        "        base = name.split(\"_\")[0].upper() if \"_\" in name else first\n",
        "        labels_all.append(1 if base == \"F\" else 0)\n",
        "\n",
        "labels_all = np.array(labels_all)\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(all_files)),\n",
        "    test_size=0.2,\n",
        "    stratify=labels_all,\n",
        "    random_state=42\n",
        ")\n",
        "train_files = [all_files[i] for i in train_idx]\n",
        "val_files   = [all_files[i] for i in val_idx]\n",
        "\n",
        "train_dataset = FingerprintDataset(train_files, transform=train_tf)\n",
        "val_dataset   = FingerprintDataset(val_files, transform=val_tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrNEnAzFBp9q"
      },
      "source": [
        "Aplicando pesos para a classe com menor frequência (impressões digitais de mulheres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DWRGXw1AoTbG"
      },
      "outputs": [],
      "source": [
        "train_labels = np.array(train_dataset.labels)\n",
        "class_counts = np.bincount(train_labels)\n",
        "\n",
        "class_weights = 1.0 / (class_counts)\n",
        "sample_weights = class_weights[train_labels]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights.tolist(),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0, pin_memory=False)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp0-aqoHB_cd"
      },
      "source": [
        "Definindo modelos: Ao invés de montar uma arquitetura própria, trouxe algumas sugestões de arquiteturas pré-treinadas para classificação de imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G61JVGIpoT8o"
      },
      "outputs": [],
      "source": [
        "def create_model(trial):\n",
        "    model_name = trial.suggest_categorical(\n",
        "        \"model_type\",\n",
        "        [\"resnet18\", \"resnet50\", \"mobilenet_v2\", \"efficientnet_b0\", \"densenet121\"]\n",
        "    )\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        model = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
        "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "\n",
        "    elif model_name == \"mobilenet_v2\":\n",
        "        model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
        "\n",
        "    elif model_name == \"efficientnet_b0\":\n",
        "        model = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
        "\n",
        "    elif model_name == \"densenet121\":\n",
        "        model = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
        "\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSoJkfknHHKM"
      },
      "source": [
        "Treino e avaliação dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SKAgYicvoVlt"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, trial=None, save_path=None):\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True) if trial is not None else 1e-4\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_state = None\n",
        "    patience = 3\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs).squeeze(1)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        trues = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                logits = model(imgs).squeeze(1)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                pred_bin = (probs > 0.5).long().cpu().numpy()\n",
        "                preds.extend(pred_bin.tolist())\n",
        "                trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "        f1 = f1_score(trues, preds, zero_division=0)\n",
        "        prec = precision_score(trues, preds, zero_division=0)\n",
        "        rec  = recall_score(trues, preds, zero_division=0)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{N_EPOCHS} - loss: {running_loss/len(train_dataset):.4f} - F1: {f1:.4f} - P: {prec:.4f} - R: {rec:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            no_improve = 0\n",
        "            best_state = model.state_dict()\n",
        "            if save_path is not None:\n",
        "                torch.save(best_state, save_path)\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "        if trial is not None:\n",
        "            trial.report(best_f1, epoch)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_f1, best_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcC9JSOrHZHj"
      },
      "source": [
        "Função objetivo do optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ilypJoTXoW6K"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    f1, _ = train_and_evaluate(model, trial=trial, save_path=None)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDbJRorxHa8j"
      },
      "source": [
        "Rodar optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe15586b73af44d2ae6f1be6f6aa8ad0",
            "be6a08c8226f43818c3276f10e0bd1cb",
            "215a67a21fcc465f98b7d634be55482a",
            "2f82bbd7b34d46fa8f51daa2c11fd98d",
            "f0a86b63dd9f4730b9ab01e46812ba4e",
            "200fb41b436e4c62be467a0d2875809e",
            "bf6a786f41a04617b0709fc979ee11d3",
            "45d93c49be104ec5b635cb25f1901980",
            "9e0fb6fce209468292ab2c18641ee628",
            "84f981d526c546a2a4c2a9d720590c43",
            "25a7d59052554b4eb002fb9f42464c92"
          ]
        },
        "id": "U3T7-ZcmoX_o",
        "outputId": "f1ee5f5e-f225-4220-fb47-3fe864afdf59"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, n_jobs=-1, show_progress_bar=True)\n",
        "\n",
        "print(\"Melhor trial:\", study.best_trial.params)\n",
        "print(\"Melhor F1 obtido (val):\", study.best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJp11H2ESJps"
      },
      "source": [
        "Coletando melhor modelo do estudo de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGPbIxh9oag5",
        "outputId": "ced5d65c-6500-4d81-a7d0-bcc372f71dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - loss: 0.5652 - F1: 0.4317 - P: 0.3053 - R: 0.7365\n",
            "Epoch 2/20 - loss: 0.4471 - F1: 0.5226 - P: 0.4029 - R: 0.7432\n",
            "Epoch 3/20 - loss: 0.3927 - F1: 0.4959 - P: 0.4155 - R: 0.6149\n",
            "Epoch 4/20 - loss: 0.3358 - F1: 0.4978 - P: 0.3709 - R: 0.7568\n",
            "Epoch 5/20 - loss: 0.2652 - F1: 0.5101 - P: 0.5067 - R: 0.5135\n",
            "Early stopping.\n",
            "Final F1 (val): 0.5225653206650831\n",
            "Best model saved to: best_model_final.pth\n"
          ]
        }
      ],
      "source": [
        "best_params = study.best_trial.params\n",
        "class Dummy:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "    def suggest_float(self, *args, **kwargs):\n",
        "        return self.params.get(\"lr\", 1e-4)\n",
        "    def suggest_categorical(self, name, choices):\n",
        "        return self.params.get(\"model_type\", choices[0])\n",
        "\n",
        "dummy_trial = Dummy(best_params)\n",
        "final_model = create_model(dummy_trial)\n",
        "best_model_path = \"best_model_final.pth\"\n",
        "best_f1, best_state = train_and_evaluate(final_model, trial=None, save_path=best_model_path)\n",
        "print(\"Final F1 (val):\", best_f1)\n",
        "print(\"Best model saved to:\", best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUtaQgGBHgXY"
      },
      "source": [
        "Avaliação \"final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPdPbilpobFJ",
        "outputId": "a082bc79-76f0-402d-d1c7-d78622c949d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.5066666666666667\n",
            "Recall: 0.5135135135135135\n",
            "F1 Score: 0.5100671140939598\n",
            "Confusion Matrix:\n",
            " [[578  74]\n",
            " [ 72  76]]\n"
          ]
        }
      ],
      "source": [
        "if best_state is not None:\n",
        "    final_model.load_state_dict(best_state)\n",
        "\n",
        "final_model.eval()\n",
        "preds = []\n",
        "trues = []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = final_model(imgs).squeeze(1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        pred_bin = (probs > 0.5).long().cpu().numpy()\n",
        "        preds.extend(pred_bin.tolist())\n",
        "        trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "print(\"Precision:\", precision_score(trues, preds, zero_division=0))\n",
        "print(\"Recall:\", recall_score(trues, preds, zero_division=0))\n",
        "print(\"F1 Score:\", f1_score(trues, preds, zero_division=0))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(trues, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizando agora o modelo selecionado para classificar as imagens no conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPfjvRJXZTdy",
        "outputId": "4eef4440-4a26-40b4-8527-59e7ca28098a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Classificando imagens de teste: 100%|██████████| 63/63 [00:39<00:00,  1.59it/s]\n"
          ]
        }
      ],
      "source": [
        "TEST_ROOT = '/content/drive/MyDrive/colab_data/rna1/lista5/Teste'\n",
        "\n",
        "def collect_image_files(root):\n",
        "    exts = (\"*.bmp\", \"*.BMP\")\n",
        "    files = []\n",
        "    for e in exts:\n",
        "        files.extend(glob.glob(os.path.join(root, e)))\n",
        "    files = sorted(files)\n",
        "    return files\n",
        "\n",
        "test_files = collect_image_files(TEST_ROOT)\n",
        "\n",
        "if best_state is None and os.path.exists(\"best_model_final.pth\"):\n",
        "    final_model.load_state_dict(torch.load(\"best_model_final.pth\", map_location=device))\n",
        "elif best_state is not None:\n",
        "    final_model.load_state_dict(best_state)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, files_list, transform=None):\n",
        "        self.files = files_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        img = Image.open(path)\n",
        "        file_id = os.path.basename(path).split('.')[0]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, file_id\n",
        "\n",
        "test_dataset = TestDataset(test_files, transform=val_tf)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "final_model.eval()\n",
        "final_model.to(device)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, file_ids in tqdm(test_loader, desc=\"Classificando imagens de teste\"):\n",
        "        imgs = imgs.to(device)\n",
        "        logits = final_model(imgs).squeeze(1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        pred_bin = (probs > 0.5).long().cpu().numpy()\n",
        "\n",
        "        for file_id, pred in zip(file_ids, pred_bin):\n",
        "            label_str = \"M\" if pred == 0 else \"F\"\n",
        "            predictions.append([file_id, label_str])\n",
        "\n",
        "df_predictions = pd.DataFrame(predictions, columns=[\"ID\", \"PREDIT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Salvando os resultados em CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uow1NrGiZ_IM"
      },
      "outputs": [],
      "source": [
        "output_csv_path = \"/content/drive/MyDrive/colab_data/rna1/lista5/test_predictions.csv\"\n",
        "df_predictions.to_csv(output_csv_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "200fb41b436e4c62be467a0d2875809e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215a67a21fcc465f98b7d634be55482a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d93c49be104ec5b635cb25f1901980",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e0fb6fce209468292ab2c18641ee628",
            "value": 30
          }
        },
        "25a7d59052554b4eb002fb9f42464c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f82bbd7b34d46fa8f51daa2c11fd98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f981d526c546a2a4c2a9d720590c43",
            "placeholder": "​",
            "style": "IPY_MODEL_25a7d59052554b4eb002fb9f42464c92",
            "value": " 30/30 [47:32&lt;00:00, 45.46s/it]"
          }
        },
        "45d93c49be104ec5b635cb25f1901980": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f981d526c546a2a4c2a9d720590c43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0fb6fce209468292ab2c18641ee628": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be6a08c8226f43818c3276f10e0bd1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_200fb41b436e4c62be467a0d2875809e",
            "placeholder": "​",
            "style": "IPY_MODEL_bf6a786f41a04617b0709fc979ee11d3",
            "value": "Best trial: 0. Best value: 0.576577: 100%"
          }
        },
        "bf6a786f41a04617b0709fc979ee11d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a86b63dd9f4730b9ab01e46812ba4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe15586b73af44d2ae6f1be6f6aa8ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be6a08c8226f43818c3276f10e0bd1cb",
              "IPY_MODEL_215a67a21fcc465f98b7d634be55482a",
              "IPY_MODEL_2f82bbd7b34d46fa8f51daa2c11fd98d"
            ],
            "layout": "IPY_MODEL_f0a86b63dd9f4730b9ab01e46812ba4e"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
