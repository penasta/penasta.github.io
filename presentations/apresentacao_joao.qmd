---
title: "Técnica Random Forest em árvores de decisão"
subtitle: "Grupo 4"
author:
  - name: Bruno Gondim Toledo
    url: https://github.com/penasta
  - name: Stefan Zurman Gonçalves
  - name: João Pedro Almeida Santos
  - name: João Alberto de Rezende Alvares
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: auto
    scrollable: true
    logo: as_vert_cor.jpg
    theme: serif
    width: 1500
    css: styles.css
    footer: Departamento de estatística - UnB
resources:
  - demo.pdf
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk:
    dev: png
    dev.args:
      bg: transparent
---

```{r}
#| echo: false
#| eval: false
options(repos='http://cran.rstudio.org')
have.packages <- installed.packages()
cran.packages <- c('devtools','plotrix','randomForest','tree')
to.install <- setdiff(cran.packages, have.packages[,1])
if(length(to.install)>0) install.packages(to.install)

library(devtools)
if(!('reprtree' %in% installed.packages())){
   install_github('munoztd0/reprtree')
}
for(p in c(cran.packages, 'reprtree')) eval(substitute(library(pkg), list(pkg=p)))
```

```{r, include=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load("tidyverse", "randomForest","randomForest","reprtree",
               "reshape2","latex2exp")
```

## Motivação: Podemos melhorar o bagging?

Suponha que entre as variáveis do modelo, há uma com forte poder preditivo e outras com poder moderado. Da forma que é construído, o algoritmo Bagging vai quase sempre dispor o preditor mais poderoso no topo de cada árvore. 

Dessa forma, as árvores construídas com o *bagging* vão ser muito parecidas umas com as outras. 

Ou seja, serão **altamente correlacionadas**.

```{r}
#| echo: true
#| eval: false
par(mfrow = c(2, 2))

set.seed(123)
model <- randomForest(Species ~ ., data=iris, importance=TRUE, ntree=500, mtry = 4, do.trace=100)

reprtree:::plot.getTree(model, k = 1)
reprtree:::plot.getTree(model, k = 2)
reprtree:::plot.getTree(model, k = 3)
reprtree:::plot.getTree(model, k = 4)
```

---

<!-- A agregação de quantidades correlacionadas não costuma levar a uma redução significativa da variância quando comparadas com quantidades não correlacionadas. -->

<!-- Como pode ser visto no exemplo utilizando o banco IRIS, as árvores tem estruturas muito parecidas -->

```{r, results='hide'}
par(mfrow = c(2, 2))



set.seed(123)
model <- randomForest(Species ~ ., data=iris, importance=TRUE, ntree=500, mtry = 4, do.trace=100)

reprtree:::plot.getTree(model, k = 1)
reprtree:::plot.getTree(model, k = 2)
reprtree:::plot.getTree(model, k = 3)
reprtree:::plot.getTree(model, k = 4)
```


## Definições

Breiman, L. Random Forests. Machine Learning **45**, 5–32 (2001)

As árvores podem ser representadas como $h_1(\boldsymbol x),\;  h_2(\boldsymbol x),\; . . . ,\; h_K( \boldsymbol x)$ na forma

$$ \{ h \left( \boldsymbol x, \Theta_k \right), \; \; k = 1, \dots \}  $$

onde $\Theta_k \; \text{i.i.d}$ são vetores aleatórios representando a escolha de $ p$ entre os $m$ atributos de $\boldsymbol X$. 

Normalmente, $p \approx \sqrt m$

---
<!-- $$mg(\boldsymbol X, Y) = av_k I (h_k(\boldsymbol X) = Y) - \underset{j \neq Y}{\text{max}} av_k I(h_k (\boldsymbol X) = j)$$. -->

<!-- A margem é a diferença entre a média do número de votos para a classe correta e máximo da média de votos para a classe incorreta. -->

<!-- O erro de generalização, a probabilidade de que a função de margem seja menor que zero, é dado por: -->

<!-- $$ PE^{*} = P_{\boldsymbol X, Y} (mg(\boldsymbol X, Y) < 0$$ -->

**1)** A medida em que o número de árvores cresce, a média de acertos se estabiliza e a chance de cometer uma predição errada pode ser quantificada:

$$  P_{\boldsymbol X, Y} \left(   P_{\Theta}( h(\boldsymbol X, \Theta) = Y)   - P_{\Theta} (h(\boldsymbol X, \Theta)  \neq Y) < 0\right) $$

- $P_{\Theta}( h(\boldsymbol X, \Theta) = Y)$ representa a probabilidade de que uma árvore acerte a predição de Y

<!-- A diferença dos dois termos representa a margem (o quão mais provável é prever a classe correta do que a incorreta). -->

<!-- A probabilidade $P_{\boldsymbol X, Y}$ mede a frequência em que a margem é negativa, indicando um erro. -->

<!-- Basicamente, a medida em que o número de árvores cresce, o comportamento da random forest se torna mais previsível, de forma que o erro de generalização se estabiliza e converge para um valor fixo, e assim é possível estimar a taxa de erro da random forest. -->


**2)** A acurácia da random forest vai depender do "poder" de cada um dos classificadores individuais e da dependência entre eles.

Um limite superior para o erro de generalização é dado por 

$$PE^* \le -\bar \rho(1 − s^2)/s^2 $$

onde:

- $\boldsymbol s = E_{\boldsymbol X, Y} mr({\boldsymbol X, Y} )$ é o "poder" das árvores $h(\boldsymbol x, \Theta)$

<!-- É  a média do nível de acerto das classes corretas. -->

- $\bar \rho$ pode ser entendido como a média entre as correlações das árvores.

## Exemplo Alzheimer

O dataset DARWIN (https://archive.ics.uci.edu/dataset/732/darwin) contém dados sobre a escrita a mão de pessoas afetadas pelo Alzheimer e de um grupo de controle, totalizando 174 observações. São 450 variáveis e o objetivo é distinguir pessoas afetadas (P) de pessoas saudáveis (H).

```{r}
data = read.csv('data.csv', header = TRUE)
```

```{r}
#| echo: false
library('caret')
library('dplyr')

set.seed(123)  # Set seed for reproducibility
splitIndex <- createDataPartition(data$class, p = 0.8, list = FALSE)

train_data <- data[splitIndex, ]  %>% dplyr::select(-`ID`)
test_data <- data[-splitIndex, ]  %>% dplyr::select(-`ID`)
```

```{r}
#| echo: false
train_data$class <- as.factor(train_data$class)
test_data$class <- as.factor(test_data$class)
```


:::: {.columns}

::: {.column width="50%"}
```{r, results='hide'}
#| echo: true
set.seed(123)

rf_model <- randomForest(
  class ~ ., data = train_data, 
  ntree = 500, 
  importance = TRUE
  )
```


```{}
No. of variables tried at each split: 21
OOB estimate of  error rate: 13.57%

Confusion matrix:
   H  P class.error
H 58 10   0.1470588
P  9 63   0.1250000
```



```{r}
#| echo: false
#| eval: false

# Printa a matriz de confusão
print(rf_model$confusion)


# Prever com o modelo
predictions <- predict(rf_model, test_data)

# Avaliar a precisão
accuracy <- mean(predictions == test_data$class)
print(paste0("Test error:", round((1-accuracy)*100,2),"%"))
```
:::


```{r}
#| echo: false
#| eval: false
# Extrair a importância das variáveis baseadas no índice Gini
importance <- importance(rf_model, type = 2)

# Criar um dataframe com as variáveis e sua importância
importance_df <- data.frame(variable = rownames(importance), importance = importance[, "MeanDecreaseGini"])
importance_df <- importance_df[order(importance_df$importance, decreasing = TRUE), ]

# Plotar a importância das variáveis
ggplot(importance_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Importância das Variáveis - Índice Gini",
       x = "Variáveis",
       y = "Importância") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```


```{r}
#| echo: false
#| eval: false
ggsave('presentations/importance.png', width = 158, height = 93, units = "mm")
```

::: {.column width="50%"}
![](importance.png){width=80%}
:::
::::

<!--Pelo gráfico, percebe-se o destaque de algumas varíaveis quanto ao nível de importância.-->


No gráfico abaixo é possível perceber como a escolha do número de variáveis em cada *split* faz diferença para o resultado final do modelo. 

```{r}
#| echo: false
#| eval: false
# Definir os valores de mtry que queremos avaliar
mtry_values <- seq(1, ncol(train_data) - 1)  # de 1 até o número de variáveis - 1

# Vetor para armazenar as acurácias
accuracy <- numeric(length(mtry_values))

# Loop para ajustar o modelo e calcular a acurácia para cada valor de mtry
for (i in seq_along(mtry_values)) {
  rf_model <- randomForest(class ~ ., data = train_data, mtry = mtry_values[i], ntree = 500, importance = TRUE)
  predictions <- predict(rf_model, test_data)
  accuracy[i] <- mean(predictions == test_data$class)
}


# Criar um dataframe com os resultados
results <- data.frame(mtry = mtry_values, accuracy = accuracy)
```

```{r}
#| echo: false
#| eval: false

# Identificar o índice do ponto com maior acurácia
best_index <- which.max(results$accuracy)
max_accuracy <- max(results$accuracy)
sqrt_accuracy <- results[round(sqrt(450)),]$accuracy
sqrt_index <- results[round(sqrt(450)),]$mtry

# Plotar a acurácia em função de mtry
ggplot(results, aes(x = mtry, y = accuracy)) +
  geom_smooth() +
  geom_point(alpha = 0.1) +
  geom_point(data = results[best_index, ], aes(x = mtry, y = accuracy), color = "red", size = 3) +
    geom_point(data = results[sqrt_index, ], aes(x = mtry, y = accuracy), color = "blue", size = 3) +
  annotate("text", x = best_index+100 , y = max_accuracy+0.03, label = "Maior Acurácia", color = "red", hjust = 1.5) +
    annotate("text", x = sqrt_index+180 , y = sqrt_accuracy+0.02, label = TeX("Acurácia com  $p = \\sqrt{m}$"), color = "blue", hjust = 1.5) +
  labs(title = "Acurácia no banco de teste em função de p",
       x = "Número de Variáveis (p) em cada divisão",
       y = "Acurácia") +
  ylim(.7,1) +
  theme_minimal()
```


```{r}
#| echo: false
#| eval: false
ggsave('presentations/mtry_accuracy.png', width = 158, height  = 93, units = "mm")
```


![](mtry_accuracy.png){width=60%}

Ainda, podemos verficiar a acurácia do modelo pelo número de árvores:

```{r}
#| echo: false
#| eval: false
# Definir os valores de ntree que queremos avaliar
ntree_values <- c(1, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)

# Vetor para armazenar as acurácias
accuracy <- numeric(length(ntree_values))

# Loop para ajustar o modelo e calcular a acurácia para cada valor de ntree
for (i in seq_along(ntree_values)) {
  rf_model <- randomForest(class ~ ., data = train_data, ntree = ntree_values[i])
  predictions <- predict(rf_model, test_data)
  accuracy[i] <- mean(predictions == test_data$class)
}

# Criar um dataframe com os resultados
results <- data.frame(ntree = ntree_values, accuracy = accuracy)

# Plotar a acurácia em função de ntree
ggplot(results, aes(x = ntree, y = accuracy)) +
  geom_line() +
  geom_point() +
  labs(title = "Acurácia do Random Forest em função de ntree",
       x = "Número de Árvores (ntree)",
       y = "Acurácia") +
  theme_minimal() +
  ylim(0,1)
```

```{r}
#| echo: false
#| eval: false
ggsave('presentations/ntree_accuracy.png', width = 158, height  = 93, units = "mm")
```

![](ntree_accuracy.png){width=60%}


# Referências:

https://www.r-bloggers.com/2021/04/random-forest-in-r/
